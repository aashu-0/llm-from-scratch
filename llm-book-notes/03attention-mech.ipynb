{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causal Attention**: each word(or tokens) only see the past and current, mask is used to block information from future words.\n",
    "\n",
    "**Self Attention**: looks at all the words, past and future, and calculate scores(attention weights) to decides which are the most important one for understanding the context\n",
    "\n",
    "**MultiHead Attention**: multiple attention mechanism at a same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shortcomings of RNNs**\n",
    "- vanishing gardient problem\n",
    "- RNN can’t directly access earlier hidden states from the encoder during the decoding phase.\n",
    "- short term memory: rnns don't work well for long sequences.\n",
    "\n",
    "* RNNs have to remember the entire encoded input into a fixed-size hidden state before passing it to the decoder (*Information Bottleneck*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To addresss this: attention mechanism is introduced\n",
    "\n",
    "- *Bahdanau attention* mechanism -> additive Attention\n",
    "\n",
    "    ↳ Bahdanau attention lets the decoder focus on the different parts of input at each time step.\n",
    "\n",
    "    ↳ It assigns different weights (called attention scores) to each encoder state, deciding how important the input token is for the respective output token.\n",
    "\n",
    "- *Luong attention* mechanism -> multiplicative attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self Attention**\n",
    "- self in self-attention means -> assesses and learns the relationships & dependencies b/w various parts of the input itself\n",
    "\n",
    "- the goal of self attention is to have a *context vector(z)* for each element(x) in the input sequence\n",
    "\n",
    "e.g->  z(2), context vector is an embedding that contains information about x(2) and all other x(i)'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SIMPLIFIED SELF ATTENTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7120, 7002, 4940, 351, 530, 2239]\n",
      "50257\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "text = 'Your journey starts with one step'\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4521,  1.0357,  0.1780],\n",
       "        [ 1.4295,  0.9807, -0.9021],\n",
       "        [ 1.5082,  0.8670, -0.1339],\n",
       "        [ 0.2479,  2.3997,  0.7642],\n",
       "        [-0.6501,  0.3397,  0.8724],\n",
       "        [-0.0332, -0.2958,  0.1109]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dim = 3 #d\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(12)\n",
    "embedding_layer = nn.Embedding(tokenizer.n_vocab, 3)\n",
    "embedding_layer(torch.tensor(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], #Your\n",
    "     [0.55, 0.87, 0.66], #journey\n",
    "     [0.57, 0.85, 0.64], #starts\n",
    "     [0.22, 0.58, 0.33], #with\n",
    "     [0.77, 0.25, 0.10], #one\n",
    "     [0.05, 0.80, 0.55]] #step\n",
    ")\n",
    "\n",
    "query = inputs[1]   # second token as query\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])  #initialize\n",
    "\n",
    "# taking dot product of query with each token\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0.55*0.43) + (0.87*0.15) + (0.89*0.66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dot-product** is used as a measure of similarity because it quantifies how closely two vectors are aligned.\n",
    "\n",
    "here, the magnitude of attention scores (dot product of query with input tensor) depicts the extent to which query(`input[1]`) **\"attends to\"** other input elements\n",
    "\n",
    "higher the `attn_score`, higher the similarity b/w two inputs elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# normalised attn scores (also called attention weights)\n",
    "attn_weights_2_tmp= attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(f'Attention Weights:\\n{attn_weights_2_tmp}')\n",
    "print(f'Sum: {attn_weights_2_tmp.sum():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "# using softmax for normalisationm\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(f'Attention Weights:\\n{attn_weights_2}')\n",
    "print(f'Sum: {attn_weights_2.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context Vector**\n",
    "\n",
    "context vector, here(`z²`) = embedded input tokens * corresponding attention weights and then summing the resultant vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# calculating the context vector(z²)\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manual calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(0.1385*0.43, 0.1385*0.15, 0.1385*0.89)\n",
    "# print(0.2379*0.55, 0.2379*0.87, 0.2379*0.66)\n",
    "# print(0.2333*0.57, 0.2333*0.58, 0.2333*0.64)\n",
    "# print(0.1240*0.22, 0.1240*0.58, 0.1240*0.33)\n",
    "# print(0.1082*0.77, 0.1082*0.25, 0.1082*0.10)\n",
    "# print(0.1581*0.05, 0.1581*0.80, 0.1581*0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(0.059555000000000004+0.13084500000000002+0.132981+0.02728+0.083314+0.007905)\n",
    "# print(0.020775000000000002+0.206973+0.135314+0.07192+0.02705+0.12648)\n",
    "# print(0.12326500000000001+0.15701400000000001+0.149312+0.040920000000000005+0.010820000000000001+0.086955)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing all context vectors**\n",
    "\n",
    "`attention_scores` ->\n",
    "`attention_weights` -> \n",
    "`context_vectors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# compute attention scores for all input elements\n",
    "\n",
    "attn_scores = torch.empty(6,6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i,j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# attention scores using matmul(or @) as for loop is slow\n",
    "\n",
    "# attn_scores = torch.matmul(inputs, inputs.T)\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# attn weights\n",
    "attn_weights = torch.softmax(attn_scores, dim =-1) # dim =-1 means normalise across columns\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# context vectors\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **IMPLEMENTING SELF-ATTENTION WITH TRAINABLE WEIGHTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have to introduce three trainable weight matrices Wq, Wk, Wv.\n",
    "these matrices are used to project the embedded inputs tokens x(i) into `query`, `key` and `value` vectors\n",
    "\n",
    "\\ Wq, Wk, Wv -> these matrices are updated during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]),\n",
       " torch.Size([6, 3]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usually `input_embedding` = `output_embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing only one context vector (z²)\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]  # input embedding dim\n",
    "d_out = 2 #output embedding dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize three weight matrices(query, key and value)\n",
    "torch.manual_seed(123)\n",
    "W_q = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_k = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_v = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)  #should use requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vector corresponding to input token x(2):\n",
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "#query, key and value vectors (vector as d_out = 2)\n",
    "query_2 = x_2 @ W_q\n",
    "key_2 = x_2 @ W_k\n",
    "value_2 = x_2 @ W_v\n",
    "\n",
    "print(f'Query vector corresponding to input token x(2):\\n{query_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys shape: torch.Size([6, 2])\n",
      "Values shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# to calculate the context vector (z2), we need to compute all key and value vectors\n",
    "# all keys, values \n",
    "\n",
    "keys = inputs @ W_k\n",
    "values = inputs @ W_v\n",
    "print(f'Keys shape: {keys.shape}')\n",
    "print(f'Values shape: {values.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys, values, query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8524)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention scores = query * keys\n",
    "# attention score for x(2)\n",
    "keys_2 = keys[1]\n",
    "\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "attn_score_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all attention scores corresponding to x(2)\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention weights = scaled attention scores\n",
    "import math\n",
    "\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2/ math.sqrt(d_k), dim=-1)\n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context vector -> weighted sum over the value vectors\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "context_vec_2  # single context vector for token 2 in the input sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a compact self-attention Python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))  #trainable weights matrices\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self,x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in=3, d_out=2)\n",
    "\n",
    "context_vector = sa_v1(inputs)\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why `nn.Linear`?\n",
    "- internally initializes weights and bias\n",
    "- automatically registers gradient updates\n",
    "- applies linear transformation to the incoming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a better way to use nn.Linear\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias= qkv_bias)  # stores W_query as (d_out, d_in)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias= qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)   # internally does x @ W_key.T\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        # print(f'Atteention Weights: {attn_weights}')\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5337, -0.1051],\n",
       "        [-0.5323, -0.1080],\n",
       "        [-0.5323, -0.1079],\n",
       "        [-0.5297, -0.1076],\n",
       "        [-0.5311, -0.1066],\n",
       "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "context_vector = sa_v2(inputs)\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CASUAL ATTENTION**(or masked attention)\n",
    "**standard self-attention**: access to the entire input sequence at once.\n",
    "\n",
    "**casual attention**: only previous and current inputs in a sequence, masking future tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way to implement attention mask\n",
    "1. normalise attention scores using softmax to get attention weights\n",
    "2. mask with 0's above diagonal to obtain masked attention scores\n",
    "3. normalise rows to get masked attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
       "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
       "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
       "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
       "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to apply casual mask -> mask elements above the diagonal with 0's.\n",
    "\n",
    "# 1.\n",
    "# compute attention weights\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / math.sqrt(keys.shape[-1]), dim = -1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 masking values above diagonal to zero\n",
    "context_length = attn_scores.shape[0]\n",
    "\n",
    "#.tril -> (l)lower triangular matrix\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiply this mask with attn_weights \n",
    "masked_simple = attn_weights* mask_simple\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
       "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. renormalise these attn_weights\n",
    "row_sum = masked_simple.sum(dim=-1, keepdim= True)\n",
    "# row_sum\n",
    "masked_simple_norm = masked_simple / row_sum\n",
    "masked_simple_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a more efficient way of doing the same is to mask attention scores with -inf and apply softmax...as softmax(-inf) = 0 (because exp(-inf) tends to 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*masking-trick*\n",
    "1. mask with 1's above the diagonal\n",
    "2. replace 1's with -inf\n",
    "3. apply softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doing this using a 'trick'\n",
    "#.triu -> upper triangular matrix\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1) # diagonal = 1 (to which diagonal to consider)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
       "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
       "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
       "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
       "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply softmax\n",
    "attn_weights = torch.softmax(masked / math.sqrt(keys.shape[-1]), dim=1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Masking additional attentional weights with dropout**.\n",
    "\n",
    "these attn_weights can be used to calculate context vector,\n",
    "\n",
    "but to prevent overfitting we randomly select some attn_weights and drop them out.\n",
    "\n",
    "*Dropout*? -> only used during training\n",
    "we apply dropout after computing attn weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2., 2., 2.],\n",
       "        [0., 2., 0., 0., 0., 0.],\n",
       "        [0., 0., 2., 0., 2., 0.],\n",
       "        [2., 2., 0., 0., 0., 2.],\n",
       "        [2., 0., 0., 0., 0., 2.],\n",
       "        [0., 2., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(p=0.5) #dropout rate 50%\n",
    "\n",
    "#example\n",
    "example = torch.ones(6,6)\n",
    "dropout(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why 2?\n",
    "\n",
    "to maintain the scaling, therefore remaining elements are scaled up by a factor of `1/0.5 = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.0335, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.6804, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4889, 0.5090, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3988, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.3418, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply dropout to attn_weights\n",
    "torch.manual_seed(123)\n",
    "dropout(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Casual Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.4300, 0.1500, 0.8900],\n",
       "          [0.5500, 0.8700, 0.6600],\n",
       "          [0.5700, 0.8500, 0.6400],\n",
       "          [0.2200, 0.5800, 0.3300],\n",
       "          [0.7700, 0.2500, 0.1000],\n",
       "          [0.0500, 0.8000, 0.5500]],\n",
       " \n",
       "         [[0.4300, 0.1500, 0.8900],\n",
       "          [0.5500, 0.8700, 0.6600],\n",
       "          [0.5700, 0.8500, 0.6400],\n",
       "          [0.2200, 0.5800, 0.3300],\n",
       "          [0.7700, 0.2500, 0.1000],\n",
       "          [0.0500, 0.8000, 0.5500]]]),\n",
       " torch.Size([2, 6, 3]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch, batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias= qkv_bias)  # stores W_query as (d_out, d_in)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias= qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "            )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1,2) # keeeping batch_dim as it is\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(attn_scores/ math.sqrt(keys.shape[-1]), dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why `register_buffer('buffer_name', tensor)`\n",
    "- part of model but not trainable.\n",
    "- moves with `.to(device)`.\n",
    "- persists with the model (stored and loaded with `model.dict()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector: tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Context Vector Shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "ca = CasualAttention(d_in, d_out, context_length, dropout=0)\n",
    "context_vecs = ca(batch)\n",
    "print(f'Context Vector: {context_vecs}')\n",
    "print(f'Context Vector Shape: {context_vecs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Multi Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to run attention mechanism parallely multiple times with different, learned projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking multiple (here two) self attention modules on top of each other\n",
    "\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([CasualAttention\n",
    "                                    (d_in, d_out, context_length, dropout, qkv_bias) \n",
    "                                    for _ in range(num_heads)]\n",
    "                                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        context_vec = torch.cat([head(x) for head in self.heads], dim = -1)  # concat the two context vector matrices\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector:\n",
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "Context Vector Shape:\n",
      "torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]  # num of tokens\n",
    "d_in, d_out = 3,2 \n",
    "\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "print(f'Context Vector:\\n{context_vecs}')\n",
    "print(f'Context Vector Shape:\\n{context_vecs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**implementing MHA more efficiently with weights splits**\n",
    "\n",
    "here rather than stacking multiple casual attention blocks, we split input into multiple heads and then combining these heads after computing attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # .view() -> to reshape tensors\n",
    "        # split d_out into num_heads, head_dim\n",
    "        # (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # transpose from shape (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        # attn scores -> dot product of queries and keys for each head\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / math.sqrt(keys.shape[-1]),\n",
    "                                     dim = -1)\n",
    "\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1,2)\n",
    "\n",
    "        # transposing makes tensor non-contiguous\n",
    "        # therefore before flattening into shape (b, num_tokens, self.d_out) make them contiguous\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)  # self.d_out = self.num_heads * self.head_dim\n",
    "\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# print(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector:\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "Context Vector Shape:\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "print(f'Context Vector:\\n{context_vecs}')\n",
    "print(f'Context Vector Shape:\\n{context_vecs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Smallest GPT-2 Model**\n",
    "- `embedding_size` = 768\n",
    "- `attention_heads` = 12\n",
    "- and `d_in` = ` d_out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "# mask.bool()[:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],   \n",
    "#             [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "#             [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "\n",
    "#            [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "#             [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "#             [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "# a.transpose(2,3)\n",
    "# a @ a.transpose(2,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
