{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aashu-0/llm-from-scratch/blob/main/llm_book_notes/05pretraining-gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting scripts from github\n",
        "!git clone https://github.com/aashu-0/llm-from-scratch.git\n",
        "%cd llm-from-scratch/llm_book_notes"
      ],
      "metadata": {
        "id": "8Rv95SrcAlq2",
        "outputId": "1e86cd75-0a02-4b44-8f31-156ffd980660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm-from-scratch'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 60 (delta 27), reused 45 (delta 15), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (60/60), 74.76 KiB | 602.00 KiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n",
            "/content/llm-from-scratch/llm_book_notes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/llm-from-scratch/llm_book_notes')"
      ],
      "metadata": {
        "id": "UhOM8bgLC6V6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK4DeVYJ8rom"
      },
      "source": [
        "#### **Pretraining on unlabeled data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1H7VsCB8rop"
      },
      "source": [
        "To access weight of any layer : `layer_name.weight`\n",
        "\n",
        "To access all model trainable parameters: `model.parameters()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cstLjUCp8ror",
        "outputId": "0a4a33d2-548a-4c16-d227-1c29d4066c40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# 1. Text Generation\n",
        "\n",
        "import torch\n",
        "from GPT import GPTModel\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    'vocab_size': 50257,\n",
        "    'context_length': 256,\n",
        "    'emb_dim': 768,\n",
        "    'n_heads':12,\n",
        "    'n_layers': 12,\n",
        "    'drop_rate': 0.1,\n",
        "    'qkv_bias': False\n",
        "}\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(config=GPT_CONFIG_124M)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "E7Po_2c2EL8O",
        "outputId": "12a733ba-938d-462d-b038-3830d1582fe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m0.9/1.2 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "66HsPxNl8rov"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from GPT import generate_text_simple\n",
        "\n",
        "# function to text to token_id and token_ids_to_text\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special= {'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
        "    return decoded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DxyqjXW08row",
        "outputId": "bc67ce8c-287e-4fc5-d872-44956703e2b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text: Every Effort Moves you\n",
            "Output Text: Every Effort Moves you finisheduxeHandle appropriation pigment cotton feellike poll liberate\n"
          ]
        }
      ],
      "source": [
        "start_context = 'Every Effort Moves you'\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "input_ids = text_to_token_ids(start_context, tokenizer)\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model = model,\n",
        "    idx = input_ids,\n",
        "    max_new_tokens = 10,\n",
        "    context_size= GPT_CONFIG_124M['context_length']\n",
        ")\n",
        "\n",
        "output = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "print(f'Input Text: {start_context}')\n",
        "print(f'Output Text: {output}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.encode('every effort moves you'))\n",
        "print(tokenizer.encode('I really like chocolate'))"
      ],
      "metadata": {
        "id": "eaVZ-KLhEknj",
        "outputId": "e351fec7-9b9a-406e-f935-7920b45ad5af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16833, 3626, 6100, 345]\n",
            "[40, 1107, 588, 11311]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0w5L-n8rox"
      },
      "source": [
        "#### 1. Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2J8rGFeP8roy"
      },
      "outputs": [],
      "source": [
        "# inputs and targets (shifting concept)\n",
        "inputs = torch.tensor([[16833, 3626, 6100],\n",
        "                       [40, 1107, 588]])\n",
        "\n",
        "targets = torch.tensor([[3626, 6100, 345],\n",
        "                        [1107, 588, 11311]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6YDdOe468roz",
        "outputId": "50ca6660-b094-4c53-9e89-f6b08b7ac23d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tensor([[[ 0.1113, -0.1057, -0.3666,  ...,  0.2843, -0.8824,  0.1074],\n",
            "         [-0.6109, -0.5167, -0.7613,  ...,  0.5450, -1.0319, -0.2175],\n",
            "         [ 0.5707, -0.6459, -0.0701,  ...,  0.7419, -0.1806, -0.2217]],\n",
            "\n",
            "        [[-0.2968,  0.1949, -0.1649,  ..., -0.4867,  0.7218, -0.1714],\n",
            "         [-0.8375,  0.0612, -0.4641,  ...,  0.2327, -0.3889, -0.0770],\n",
            "         [ 0.5614,  0.6919,  0.8915,  ..., -0.9472,  1.2411, -0.2056]]])\n",
            "\n",
            "Probas Shape: torch.Size([2, 3, 50257])\n",
            "\n",
            "Probas: tensor([[[1.8849e-05, 1.5172e-05, 1.1687e-05,  ..., 2.2409e-05,\n",
            "          6.9776e-06, 1.8776e-05],\n",
            "         [9.1569e-06, 1.0062e-05, 7.8786e-06,  ..., 2.9090e-05,\n",
            "          6.0103e-06, 1.3571e-05],\n",
            "         [2.9877e-05, 8.8507e-06, 1.5741e-05,  ..., 3.5456e-05,\n",
            "          1.4094e-05, 1.3526e-05]],\n",
            "\n",
            "        [[1.2561e-05, 2.0538e-05, 1.4332e-05,  ..., 1.0389e-05,\n",
            "          3.4784e-05, 1.4239e-05],\n",
            "         [7.2731e-06, 1.7864e-05, 1.0565e-05,  ..., 2.1206e-05,\n",
            "          1.1390e-05, 1.5559e-05],\n",
            "         [2.9496e-05, 3.3605e-05, 4.1029e-05,  ..., 6.5249e-06,\n",
            "          5.8203e-05, 1.3698e-05]]])\n"
          ]
        }
      ],
      "source": [
        "# calculating probability scores\n",
        "with torch.no_grad():\n",
        "    logits = model(inputs)\n",
        "\n",
        "probas = torch.softmax(logits, dim=-1)\n",
        "print(f'Logits: {logits}\\n')\n",
        "print(f'Probas Shape: {probas.shape}\\n')   #[batch_size, n_tokens, emb_dim]\n",
        "print(f'Probas: {probas}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_2lg9sIH8ro0",
        "outputId": "e8b4c5d5-08e2-42a8-a955-d2217d68af61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: tensor([[[16657],\n",
            "         [  339],\n",
            "         [42826]],\n",
            "\n",
            "        [[49906],\n",
            "         [29669],\n",
            "         [41751]]])\n",
            "Token IDs Shape: torch.Size([2, 3, 1])\n"
          ]
        }
      ],
      "source": [
        "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "print(f'Token IDs: {token_ids}')\n",
        "print(f'Token IDs Shape: {token_ids.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "y6GNh-Ou8ro1",
        "outputId": "8818db42-c105-4c8f-e4fe-f4a2e7da43d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets Batch 1 :  effort moves you\n",
            "Output Batch 1:  Armed heNetflix\n"
          ]
        }
      ],
      "source": [
        "#token ids to output text\n",
        "\n",
        "target_batch_1 = token_ids_to_text(targets[0], tokenizer)\n",
        "output_batch_1 = token_ids_to_text(token_ids[0].flatten(), tokenizer)\n",
        "\n",
        "print(f'Targets Batch 1 : {target_batch_1}')\n",
        "print(f'Output Batch 1: {output_batch_1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVVeyj748ro2"
      },
      "source": [
        "#### 2. Text Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEo7t31I8ro3"
      },
      "source": [
        "1. `Logits`\n",
        "\n",
        "2. `Probabilities`\n",
        "\n",
        "3. `Target Probabilities`\n",
        "\n",
        "4. `Log Probabilities`\n",
        "\n",
        "5. `Average Log Probability`\n",
        "\n",
        "6. `Negative avg log probability`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_tg02EY48ro3",
        "outputId": "3c6498f0-afa9-4809-c141-f3937373f508",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([3626, 6100,  345]), tensor([ 1107,   588, 11311]))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "targets[0], targets[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MguEp1Lt8ro4",
        "outputId": "54c76ef8-80b8-4c71-af3f-f5b67c5d2013",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1 probability: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
            "Text 2 probability: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
          ]
        }
      ],
      "source": [
        "# getting the probability scores corresponding to target tokens\n",
        "# Target Probabilities\n",
        "\n",
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
        "print(f'Text 1 probability: {target_probas_1}')\n",
        "\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
        "print(f'Text 2 probability: {target_probas_2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XkUHNZR38ro5",
        "outputId": "5e3e40e1-d317-4366-a64a-cb82cf4ad86d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# log_probability\n",
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "log_probas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmGyJYC-8ro6"
      },
      "source": [
        "Why log of probability scores\n",
        "1. stronger penalization for incorrect predictions.\n",
        "    - `prob = 0.9 => -log(0.9) = 0.10`\n",
        "    - `prob = 0.01 => -log(0.01) = 4.6`\n",
        "2. makes the loss func convex\n",
        "3. prevent numerical underflow\n",
        "4. handles zero prob, avoiding undefined gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Wn6yZjab8ro6",
        "outputId": "b67cf4ed-9500-4b59-cc94-87244c19b533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-10.7940)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# avg log probabiltiy\n",
        "avg_log_probas = torch.mean(log_probas)\n",
        "avg_log_probas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "99iRK4uU8ro6",
        "outputId": "2a1ce38f-6c94-4237-c9a5-2bf20099b157",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.7940)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# neg avg log probability\n",
        "neg_avg_log_prob = avg_log_probas*-1\n",
        "neg_avg_log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0HI_oLJ8ro7"
      },
      "source": [
        "`Cross Entropy Loss`: negative log-likelihood\n",
        "\n",
        "takes care of steps `2` to `5`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Os6ghycy8ro7",
        "outputId": "0d0b0984-273a-472e-9ceb-ecd0f3e5b503",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits Shape: torch.Size([2, 3, 50257])\n",
            "Targets Shape: torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "print(f'Logits Shape: {logits.shape}')\n",
        "print(f'Targets Shape: {targets.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-xtcCkk8ro8"
      },
      "source": [
        "Why flatten?\n",
        "\n",
        "because `cross_entropy()` expects `targets` to be 1D`[N]` and `logits` as 2D tensor`[N,C]`\n",
        "\n",
        "where\n",
        "* `N` = number of samples (batch_size * seq_length)\n",
        "\n",
        "* `C` = num of classes (vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5uJ-6g5c8ro8",
        "outputId": "fc636df9-a292-4711-c567-259b75d30086",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened Logits Shape: torch.Size([6, 50257])\n",
            "Targets Logits Shape: torch.Size([6])\n"
          ]
        }
      ],
      "source": [
        "# flatten these tensors\n",
        "\n",
        "logits_flat = logits.flatten(0,1)\n",
        "targets_flat = targets.flatten()\n",
        "\n",
        "print(f'Flattened Logits Shape: {logits_flat.shape}')\n",
        "print(f'Targets Logits Shape: {targets_flat.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rsBjYqaG8ro8",
        "outputId": "ea84d7bc-afb1-4f3f-a3dc-fc7a43620398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.7940)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4IkSAuE8ro9"
      },
      "source": [
        "**Perplexity**\n",
        "- measures how well the prob distribution predicted by the model matches the actual distribution of words in the dataset\n",
        "- it quantifies uncertainty or randomness in predictions\n",
        "- lower = better model, higher = worse model\n",
        "- `perplexity = torch.exp(loss)`\n",
        "- more interpretable than raw_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xij_fMtH8ro9"
      },
      "source": [
        "#### Training and Validation losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "VXs20WKQ8ro9",
        "outputId": "f313c669-8705-44a6-c9e5-c15845600479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I HAD always thought Jack Gisburn rather a cheap g'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# fecthing the raw_data from the_verdict.txt file\n",
        "\n",
        "file_path = 'the-verdict.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "text_data[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nr9keGOZ8ro-",
        "outputId": "662305e4-221b-4395-8a5e-f2163c83650d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 20479\n",
            "Total number of tokens: 5145\n"
          ]
        }
      ],
      "source": [
        "total_chars = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(f'Total number of characters: {total_chars}')\n",
        "print(f'Total number of tokens: {total_tokens}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GAu5v5QK8ro-"
      },
      "outputs": [],
      "source": [
        "train_split = 0.9\n",
        "split_idx = int(train_split* len(text_data))\n",
        "\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "# train_data, val_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-iXFQYd68ro_"
      },
      "outputs": [],
      "source": [
        "# training and validation dataloader\n",
        "\n",
        "from CustomDatasetDataloader import create_dataloders_v1\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataloader = create_dataloders_v1(\n",
        "    txt=train_data,\n",
        "    batch_size= 2,\n",
        "    max_length= GPT_CONFIG_124M['context_length'],\n",
        "    stride = GPT_CONFIG_124M['context_length'],\n",
        "    drop_last= True,\n",
        "    shuffle = True,\n",
        "    num_workers = 0)\n",
        "\n",
        "val_dataloader = create_dataloders_v1(\n",
        "    txt=val_data,\n",
        "    batch_size= 2,\n",
        "    max_length= GPT_CONFIG_124M['context_length'],\n",
        "    stride = GPT_CONFIG_124M['context_length'],\n",
        "    drop_last= False,\n",
        "    shuffle = False,\n",
        "    num_workers = 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in train_dataloader:\n",
        "  print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "zsGMVZN9HTUg",
        "outputId": "90a3a30e-ab74-4263-b6ef-a93dee1d745c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in val_dataloader:\n",
        "  print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "O-zaaRv1Htpn",
        "outputId": "34d0f11b-f0bd-4116-a267-eb38762bafce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrXpaWaK8rpA"
      },
      "source": [
        "There are 9 training set batches with shape `[2, 256]` (i.e two samples and 256 tokens each) and 1 validation batch with shape `[2, 256]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "IS18pOKh8rpB"
      },
      "outputs": [],
      "source": [
        "# cross entropy loss for a given batch\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch = input_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "\n",
        "    logits = model(input_batch)\n",
        "    loss = F.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "uPovecGV8rpB"
      },
      "outputs": [],
      "source": [
        "# computing loss over all the batches\n",
        "# using num_batches so that we can specify lesser number of batches\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0\n",
        "    if len(data_loader) ==0:\n",
        "        return float('nan')\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i< num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss/num_batches #avg loss\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2NWvs4IJ8rpC",
        "outputId": "6fe6473e-93d9-4c8e-8d0c-4154d5274b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.98758316040039\n",
            "Testing loss: 10.981104850769043\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_dataloader, model, device)\n",
        "    val_loss = calc_loss_loader(val_dataloader, model, device)\n",
        "\n",
        "print(f'Training loss: {train_loss}')\n",
        "print(f'Testing loss: {val_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWHTs1q-8rpE"
      },
      "source": [
        "the loss values are high as we haven't trained our model yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQOx1rcs8rpE"
      },
      "source": [
        "#### Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "6hqHK2w98rpE"
      },
      "outputs": [],
      "source": [
        "# function for pretraining llm\n",
        "\n",
        "def train_model_simple(model,train_dataloader, val_dataloader, optimizer,\n",
        "                       device, num_epochs, eval_freq, eval_iter, start_context,\n",
        "                       tokenizer):\n",
        "    train_losses, val_losses, track_tokens_seen = [],[],[]\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        for input_batch, target_batch in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step +=1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_freq ==0:\n",
        "                train_loss, val_loss = evaluate_model(model, train_dataloader,\n",
        "                                                      val_dataloader, device,\n",
        "                                                      eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f'Epoch: {epoch+1} | Step: {global_step:.06d}')\n",
        "                print(f'Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')\n",
        "\n",
        "        # prints a sample text after each epoch\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwZOoPUi8rpF"
      },
      "source": [
        "`evaluate_model `->  prints train and val losses after each model update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "aJrgSTBy8rpH"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device,\n",
        "                                      num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device,\n",
        "                                    num_batches=eval_iter)\n",
        "\n",
        "        model.train()\n",
        "        return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwUrtlG88rpH"
      },
      "source": [
        "`generate_and_print_sample`: takes a text snippet as input, converts into token_ids -> feeds it to the llm -> generate a text sample using `generate_text_simple`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cedokkZ8rpR"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer.to(device))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(model, idx=encoded, max_new_tokens=50,\n",
        "                                         context_size=context_size)\n",
        "\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace('\\n', ' '))\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1KRUP-J8rpS"
      },
      "source": [
        "Training a `GPTModel` for 10 epochs.\n",
        "\n",
        "optimizer: `AdamW`\n",
        "\n",
        "lr = `0.0004`\n",
        "\n",
        "weight_decay = `0.1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GwnAphv8rpT"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(config=GPT_CONFIG_124M)\n",
        "\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay = 0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, token_seen = train_model_simple(model,\n",
        "                                                          train_loader,\n",
        "                                                          val_loader,\n",
        "                                                          optimizer,\n",
        "                                                          device,\n",
        "                                                          num_epochs,\n",
        "                                                          eval_freq=5,\n",
        "                                                          eval_iter=5,\n",
        "                                                          start_context=\"Every effort moves you\",\n",
        "                                                          tokenizer= tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bag-P4Wo8rpT"
      },
      "source": [
        "Plotting Training and Validation losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk_w3jNC8rpT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "# MaxNLocator -> tries to select resonable num of ticks with integer values instead of decimel points\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize =(5,3))\n",
        "    ax1.plot(epochs_seen, train_losses, label= 'Training Loss')\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle ='-.', label = 'Validation Loss')\n",
        "\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend(loc= 'upper right')\n",
        "\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer = True))\n",
        "    ax2 = ax1.twiny() # creates a twin x-axis (shared y-axis) for existing axis ax1\n",
        "\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0) # alpha = 0 -> invisible plot\n",
        "    ax2.set_xlabel('Tokens seen')\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJhjPUce8rpU"
      },
      "source": [
        "#### Decoding Strategies\n",
        "ways to introduce randomness in the output.\n",
        "\n",
        "1. *temperature scaling*\n",
        "\n",
        "2. *top-k sampling*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4os5eAG58rpU"
      },
      "outputs": [],
      "source": [
        "# print output text using our pretrained model\n",
        "\n",
        "model.to('cpu')\n",
        "model.eval()\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "token_ids = generate_text_simple(model= model,\n",
        "                                 idx= text_to_token_ids('Every effort moves you', tokenizer),\n",
        "                                 max_new_tokens= 25,\n",
        "                                 context_size= GPT_CONFIG_124M['context_length'])\n",
        "\n",
        "\n",
        "print(f'Output text: {token_ids_to_text(token_ids, tokenizer)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npehHEyQ8rpV"
      },
      "source": [
        "currently our pretrained llm will generate same outputs even if we run the above cell multiple times...keeping the `start_context` same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PxhvvUe8rpV"
      },
      "source": [
        "in `generate_text_simple` we are selecting token with max probability as next token...known as greddy decoding.\n",
        "\n",
        "now, we will use probabilistic sampling for next token selection using `torch.multinomial`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu_AiBij8rpW",
        "outputId": "816de2ce-b499-45a2-c393-4b9dd64a94e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('forward',\n",
              " tensor([6.0907e-02, 1.6313e-03, 1.0019e-04, 5.7212e-01, 3.4190e-03, 1.3257e-04,\n",
              "         1.0120e-04, 3.5758e-01, 4.0122e-03]))"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# illustration\n",
        "\n",
        "import torch\n",
        "\n",
        "vocab ={\"closer\": 0,\n",
        "        \"every\": 1,\n",
        "        \"effort\": 2,\n",
        "        \"forward\": 3,\n",
        "        \"inches\": 4,\n",
        "        \"moves\": 5,\n",
        "        \"pizza\": 6,\n",
        "        \"toward\": 7,\n",
        "        \"you\": 8}\n",
        "\n",
        "inv_vocab = {v:k for k, v in vocab.items()}\n",
        "\n",
        "\n",
        "# let's say returns following nect-token logits given start_context ='Every effort moves you'\n",
        "next_token_logits = torch.tensor([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])\n",
        "\n",
        "probas = torch.softmax(next_token_logits, dim=0)\n",
        "next_token_id = torch.argmax(probas).item()\n",
        "inv_vocab[next_token_id], probas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UulKiKpL8rpW",
        "outputId": "c999e822-49e8-4532-e328-3ce96d2faecf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'toward'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's do the same using probabilistic sampling\n",
        "\n",
        "torch.manual_seed(123)\n",
        "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
        "inv_vocab[next_token_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYZ13Yf_8rpX",
        "outputId": "954e9691-b75d-4018-9cbc-6e5830015647"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "71 X closer\n",
            "2 X every\n",
            "0 X effort\n",
            "544 X forward\n",
            "2 X inches\n",
            "1 X moves\n",
            "0 X pizza\n",
            "376 X toward\n",
            "4 X you\n"
          ]
        }
      ],
      "source": [
        "def print_sampled_tokens(probas):\n",
        "    torch.manual_seed(123)\n",
        "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]  # 1_000 is same as 1000(just for better readability)\n",
        "\n",
        "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
        "\n",
        "    for i, freq in enumerate(sampled_ids):\n",
        "        print(f'{freq} X {inv_vocab[i]}')\n",
        "\n",
        "print_sampled_tokens(probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or3KXhzg8rpX"
      },
      "source": [
        "`temperature scaling` -> logits divided by some number > 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jajTa21G8rpY"
      },
      "outputs": [],
      "source": [
        "def softmax_with_temp(logits, temp):\n",
        "    scaled_logits = logits/temp\n",
        "    return torch.softmax(scaled_logits, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuRE1ZB68rpZ"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "temps =[1,0,1,5]\n",
        "scaled_probas = [softmax_with_temp(next_token_logits, t) for t in temps]\n",
        "\n",
        "x = torch.arange(len(vocab))\n",
        "bar_width = 0.15\n",
        "\n",
        "fig, ax = plt.subplots(figsize= (5,3))\n",
        "for i, t in enumerate(temps):\n",
        "    rects = ax.bar(x+ i*bar_width, height=scaled_probas[i], width= bar_width,\n",
        "                   label= f'Temperature= {t}')\n",
        "\n",
        "ax.set_ylabel('Probability')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO6ovj6w8rpa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}