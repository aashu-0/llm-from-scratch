{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aashu-0/llm-from-scratch/blob/main/llm_book_notes/lora_instruction_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3oNxHWc_s7l"
      },
      "source": [
        "### Data Loading and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4PmoKwP_qyX",
        "outputId": "b565e830-ea27-41c2-d0b7-b25747413a96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of entries: 51760\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/gururise/AlpacaDataCleaned/refs/heads/main/alpaca_data_cleaned.json\"\n",
        "\n",
        "file_path = 'alpaca_data.json'\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "# load`\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "print(f'Number of entries: {len(dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdwBiq0z_0UJ",
        "outputId": "4e04438a-466d-4346-bf42-20f19e567926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of entries: 10000\n"
          ]
        }
      ],
      "source": [
        "# a subset of the dataset -> 10k samples\n",
        "\n",
        "import random\n",
        "subset_size = 10000\n",
        "random.seed(42)\n",
        "subset_data = random.sample(dataset, subset_size) # a list of dictionaries\n",
        "\n",
        "# save\n",
        "subset_file_path = 'alpaca_subset.json'\n",
        "\n",
        "# to convert list to json-formatted string\n",
        "with open(subset_file_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(subset_data, f, indent=4)\n",
        "\n",
        "print(f'Number of entries: {len(subset_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMYWSwkg_5wU"
      },
      "outputs": [],
      "source": [
        "def format_input(entry):\n",
        "  instruction_txt = (\n",
        "      f\"Below is an instruction that describes a task. \"\n",
        "      f\"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      f\"### Instruction:\\n{entry['instruction']}\"\n",
        "  )\n",
        "  input_text = (\n",
        "      f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else ''\n",
        "  )\n",
        "  return instruction_txt + input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVz6b-EFACfW",
        "outputId": "03e6a37a-a1a5-474e-c5e6-af9bf17dbf0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Edit the following sentence to make it more concise and powerful: \"My painting skills are not very good\".\n",
            "\n",
            "### Response:\n",
            "\"I need to improve my painting skills.\"\n"
          ]
        }
      ],
      "source": [
        "model_input = format_input(subset_data[1000])\n",
        "desired_response = f\"\\n\\n### Response:\\n{subset_data[1000]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fC7Jfg2AHnk",
        "outputId": "04115976-105b-4c95-9818-8ef338c7d5a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set size: 8500\n",
            "Test set size: 1000\n",
            "Validation set size: 500\n"
          ]
        }
      ],
      "source": [
        "train_set = int(len(subset_data)*0.85)\n",
        "test_set = int(len(subset_data)*0.1)\n",
        "val_set = len(subset_data) - train_set - test_set\n",
        "\n",
        "train_data = subset_data[:train_set]\n",
        "test_data = subset_data[train_set:train_set+test_set]\n",
        "val_data = subset_data[train_set+test_set:]\n",
        "\n",
        "print(f'Train set size: {len(train_data)}')\n",
        "print(f'Test set size: {len(test_data)}')\n",
        "print(f'Validation set size: {len(val_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xBUGYHBANkX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer):\n",
        "    self.data = data\n",
        "    self.encode_texts = []\n",
        "    for entry in data:\n",
        "      input_with_instruction = format_input(entry)\n",
        "      response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "      full_text = input_with_instruction + response_text\n",
        "      self.encode_texts.append(\n",
        "          tokenizer.encode(full_text)\n",
        "      )\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "     return self.encode_texts[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFtsilZWASoV"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id= 50256,\n",
        "    allowed_max_length=None,\n",
        "    ignore_index=-100,\n",
        "    device = 'cpu'):\n",
        "\n",
        "  batch_max_length = max(len(item) for item in batch)\n",
        "  inputs_lst, targets_lst = [], []\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    new_item += [pad_token_id]\n",
        "\n",
        "    padded = (new_item + [pad_token_id]* (batch_max_length - len(item)))\n",
        "    inputs= torch.tensor(padded[:-1])\n",
        "    targets= torch.tensor(padded[1:])\n",
        "\n",
        "    mask = targets == pad_token_id\n",
        "    indices = torch.nonzero(mask).squeeze() # returns the indices of True values\n",
        "    if indices.numel() >1:\n",
        "      targets[indices[1:]] = ignore_index\n",
        "\n",
        "    if allowed_max_length is not None:\n",
        "      inputs = inputs[:allowed_max_length]\n",
        "      targets = targets[:allowed_max_length]\n",
        "\n",
        "    inputs_lst.append(inputs)\n",
        "    targets_lst.append(targets)\n",
        "\n",
        "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "  targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "  return inputs_tensor, targets_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVnvmZYKAWV8",
        "outputId": "cd4c34e1-0c6c-4f5c-92c8-c380c489a76a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpD2D8VhAZQ3"
      },
      "outputs": [],
      "source": [
        "# fix or pre-fill some argument in custom_collate_function\n",
        "from functools import partial\n",
        "custom_collate_fn = partial(\n",
        "    custom_collate_fn,\n",
        "    allowed_max_length=512,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bpYMgdiAcLZ",
        "outputId": "7bfd6e26-a5c2-4a8e-fafc-8ac433fbbeef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFJCKTm0AkyO"
      },
      "outputs": [],
      "source": [
        "# tokenizer\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xRDMnQ9Al8Q"
      },
      "outputs": [],
      "source": [
        "# dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size =2\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XymapJBnApQH",
        "outputId": "dee8c52a-cbb7-4316-b4b4-939266d6f236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs: torch.Size([2, 361]), Targets: torch.Size([2, 361])\n",
            "Inputs: torch.Size([2, 81]), Targets: torch.Size([2, 81])\n",
            "Inputs: torch.Size([2, 308]), Targets: torch.Size([2, 308])\n",
            "Inputs: torch.Size([2, 65]), Targets: torch.Size([2, 65])\n",
            "Inputs: torch.Size([2, 232]), Targets: torch.Size([2, 232])\n",
            "Inputs: torch.Size([2, 167]), Targets: torch.Size([2, 167])\n",
            "Inputs: torch.Size([2, 64]), Targets: torch.Size([2, 64])\n",
            "Inputs: torch.Size([2, 228]), Targets: torch.Size([2, 228])\n",
            "Inputs: torch.Size([2, 285]), Targets: torch.Size([2, 285])\n",
            "Inputs: torch.Size([2, 347]), Targets: torch.Size([2, 347])\n",
            "Inputs: torch.Size([2, 424]), Targets: torch.Size([2, 424])\n"
          ]
        }
      ],
      "source": [
        "for i, (inputs, targets) in enumerate(train_loader):\n",
        "  print(f'Inputs: {inputs.shape}, Targets: {targets.shape}')\n",
        "  if i==10:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uglcDrhVAu8B",
        "outputId": "414f9255-5c39-4002-fd61-2b96e71e80d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llm-from-scratch'...\n",
            "remote: Enumerating objects: 160, done.\u001b[K\n",
            "remote: Counting objects: 100% (160/160), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 160 (delta 92), reused 88 (delta 37), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (160/160), 234.06 KiB | 6.16 MiB/s, done.\n",
            "Resolving deltas: 100% (92/92), done.\n",
            "/content/llm-from-scratch/llm_book_notes\n"
          ]
        }
      ],
      "source": [
        "# getting scripts from github\n",
        "!git clone https://github.com/aashu-0/llm-from-scratch.git\n",
        "%cd llm-from-scratch/llm_book_notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJuryI2VBBTp"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/llm-from-scratch/llm_book_notes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EDwdhX3BDtp",
        "outputId": "83d9dac7-d2e8-48f8-df19-e9b5b41ac17d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x797480af5cd0>)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get gpt_download.py from @rasbt github\n",
        "import urllib.request\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "urllib.request.urlretrieve(url, \"gpt_download.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BWGwk3vBF-E",
        "outputId": "93b3773b-1657-4de1-cf44-6fbf46438dff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 97.1kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.95MiB/s]\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 101kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [02:17<00:00, 10.3MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 10.9MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 927k/927k [00:00<00:00, 2.03MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.50MiB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 1024)\n",
              "  (pos_emb): Embedding(1024, 1024)\n",
              "  (drop_emb): Dropout(p=0.2, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from load_weights import load_weights_into_gpt\n",
        "from GPT import GPTModel\n",
        "from gpt_download import download_and_load_gpt2\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    'vocab_size': 50257,\n",
        "    'context_length': 1024,\n",
        "    'drop_rate': 0.2,\n",
        "    'qkv_bias': True\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    'gpt2 (124M)': {'emb_dim': 768 , 'n_layers': 12, 'n_heads': 12},\n",
        "    'gpt2-medium (355M)': {'emb_dim':1024 , 'n_layers':24, 'n_heads':16},\n",
        "    'gpt2-large (774M)': {'emb_dim': 1280 , 'n_layers': 36, 'n_heads':20},\n",
        "    'gpt2-xl (1558M)': {'emb_dim': 1600, 'n_layers':48, 'n_heads': 25}\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = 'gpt2-medium (355M)'\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(' ')[-1].lstrip('(').rstrip(')')\n",
        "\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size = model_size,\n",
        "    models_dir = 'gpt2'\n",
        ")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBUzszvYBnEL"
      },
      "source": [
        "#### Implementing a LORA Layer\n",
        "\n",
        "`rank` -> inner dimension of matrices A and B\n",
        "\n",
        "`alpha` -> scaling factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MANx024bHj_J"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "\n",
        "class LoRALayer(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "      super().__init__()\n",
        "      self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
        "      torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "      self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
        "      self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.alpha * (x @ self.A @ self.B)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHBzILQTH0T_"
      },
      "outputs": [],
      "source": [
        "# replacing linear layers with linearwithlora layer\n",
        "class LinearWithLoRA(torch.nn.Module):\n",
        "  def __init__(self, linear, rank, alpha):\n",
        "    super().__init__()\n",
        "    self.linear = linear\n",
        "    self.lora = LoRALayer(\n",
        "        linear.in_features, linear.out_features, rank, alpha)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x) + self.lora(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rVuGwzpEjFZ"
      },
      "outputs": [],
      "source": [
        "# replace linear with linearwithlora layer\n",
        "\n",
        "def replace_linear_with_lora(model, rank, alpha):\n",
        "  for name, module in model.named_children():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "      setattr(\n",
        "          model,\n",
        "          name,\n",
        "          LinearWithLoRA(module, rank, alpha)\n",
        "      )\n",
        "    else:\n",
        "      replace_linear_with_lora(module, rank, alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riyIjmsuE64k",
        "outputId": "453a4a02-39ef-45de-cd55-0236549638be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of trainable parameters: 406286336\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Total number of trainable parameters: {total_params}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKqnus_SFEeJ",
        "outputId": "f614a96f-6fac-4085-ecd2-0ce2b3b1ced5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now, Total number of trainable parameters: 0\n"
          ]
        }
      ],
      "source": [
        "# frezzing the original parameters\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Now, Total number of trainable parameters: {total_params}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndMTQjNDFaNF",
        "outputId": "921a2271-1df9-4dcb-c71e-c44eae956408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of LORA trainable parameters: 7898384\n"
          ]
        }
      ],
      "source": [
        "replace_linear_with_lora(model, rank=16, alpha=16)\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Total number of LORA trainable parameters: {total_params}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15lt88j-FnKB"
      },
      "outputs": [],
      "source": [
        "# 406286336 / 7898384"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE3vlRTWFunp",
        "outputId": "1a8b3899-87bd-473f-ecfd-a19f0929a71b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 1024)\n",
              "  (pos_emb): Embedding(1024, 1024)\n",
              "  (drop_emb): Dropout(p=0.2, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (out_proj): LinearWithLoRA(\n",
              "          (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearWithLoRA(\n",
              "            (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): LinearWithLoRA(\n",
              "    (linear): Linear(in_features=1024, out_features=50257, bias=False)\n",
              "    (lora): LoRALayer()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDmYJLfRF0M-",
        "outputId": "121c7a2f-5e3b-4b40-c54f-5f8076f0fb6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Estimate the cost of living for a student staying in Oxford for one year.\n"
          ]
        }
      ],
      "source": [
        "# assess our raw model(no fine tuning)\n",
        "torch.manual_seed(123)\n",
        "input_text = format_input(val_data[5])\n",
        "print(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "56mK3a9AGBgX",
        "outputId": "38b63c53-1ef7-426b-e935-b9a58a6ed200"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nEstimate the cost of living for a student staying in Oxford for one year.\\n\\n### Response:\\n\\nEstimate the cost of living for a student staying in Oxford for one year.\\n\\n### Instruction:\\n\\nEstimate the cost of'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from utilities import text_to_token_ids, token_ids_to_text\n",
        "from GPT import generate\n",
        "\n",
        "token_ids = generate(\n",
        "    model = model,\n",
        "    idx = text_to_token_ids(input_text, tokenizer).to(device),\n",
        "    max_new_tokens =35,\n",
        "    context_size= BASE_CONFIG['context_length'],\n",
        "    eos_id= 50256\n",
        ")\n",
        "generated_text =token_ids_to_text(token_ids, tokenizer)\n",
        "generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmM7_oG-GJVY",
        "outputId": "4d4eda58-3f55-4cf7-89a1-50f30240ab5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Response:\n",
            "\n",
            "Estimate the cost of living for a student staying in Oxford for one year.\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Estimate the cost of\n"
          ]
        }
      ],
      "source": [
        "response_text = generated_text[len(input_text):].strip()\n",
        "print(response_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZFXK0fAGh73"
      },
      "outputs": [],
      "source": [
        "from utilities import train_model_grad_accum, calc_loss_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbmfJgsMGlyJ",
        "outputId": "149b983c-419f-4047-c3ba-7acfe2643996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 2.7992, Val loss: 2.8641\n"
          ]
        }
      ],
      "source": [
        "# initial loss for the train and val dataset\n",
        "model.to(device)\n",
        "torch.manual_seed(123)\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device)\n",
        "  val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(f'Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLuQP2B6Gog9",
        "outputId": "aeace892-69fc-4f9d-fb01-4a605284e0f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Step: 000000\n",
            "Train Loss: 2.794 | Val Loss: 2.430\n",
            "Epoch: 1 | Step: 000005\n",
            "Train Loss: 1.759 | Val Loss: 2.032\n",
            "Epoch: 1 | Step: 000010\n",
            "Train Loss: 1.957 | Val Loss: 1.879\n",
            "Epoch: 1 | Step: 000015\n",
            "Train Loss: 1.935 | Val Loss: 1.795\n",
            "Epoch: 1 | Step: 000020\n",
            "Train Loss: 1.763 | Val Loss: 1.762\n",
            "Epoch: 1 | Step: 000025\n",
            "Train Loss: 1.559 | Val Loss: 1.741\n",
            "Epoch: 1 | Step: 000030\n",
            "Train Loss: 1.615 | Val Loss: 1.721\n",
            "Epoch: 1 | Step: 000035\n",
            "Train Loss: 1.683 | Val Loss: 1.710\n",
            "Epoch: 1 | Step: 000040\n",
            "Train Loss: 1.851 | Val Loss: 1.707\n",
            "Epoch: 1 | Step: 000045\n",
            "Train Loss: 1.797 | Val Loss: 1.704\n",
            "Epoch: 1 | Step: 000050\n",
            "Train Loss: 1.506 | Val Loss: 1.699\n",
            "Epoch: 1 | Step: 000055\n",
            "Train Loss: 1.611 | Val Loss: 1.691\n",
            "Epoch: 1 | Step: 000060\n",
            "Train Loss: 1.667 | Val Loss: 1.674\n",
            "Epoch: 1 | Step: 000065\n",
            "Train Loss: 1.464 | Val Loss: 1.680\n",
            "Epoch: 1 | Step: 000070\n",
            "Train Loss: 1.765 | Val Loss: 1.669\n",
            "Epoch: 1 | Step: 000075\n",
            "Train Loss: 1.689 | Val Loss: 1.665\n",
            "Epoch: 1 | Step: 000080\n",
            "Train Loss: 1.498 | Val Loss: 1.670\n",
            "Epoch: 1 | Step: 000085\n",
            "Train Loss: 1.643 | Val Loss: 1.662\n",
            "Epoch: 1 | Step: 000090\n",
            "Train Loss: 1.633 | Val Loss: 1.657\n",
            "Epoch: 1 | Step: 000095\n",
            "Train Loss: 1.603 | Val Loss: 1.664\n",
            "Epoch: 1 | Step: 000100\n",
            "Train Loss: 1.805 | Val Loss: 1.668\n",
            "Epoch: 1 | Step: 000105\n",
            "Train Loss: 1.512 | Val Loss: 1.668\n",
            "Epoch: 1 | Step: 000110\n",
            "Train Loss: 1.613 | Val Loss: 1.661\n",
            "Epoch: 1 | Step: 000115\n",
            "Train Loss: 1.649 | Val Loss: 1.657\n",
            "Epoch: 1 | Step: 000120\n",
            "Train Loss: 1.728 | Val Loss: 1.645\n",
            "Epoch: 1 | Step: 000125\n",
            "Train Loss: 1.696 | Val Loss: 1.644\n",
            "Epoch: 1 | Step: 000130\n",
            "Train Loss: 1.682 | Val Loss: 1.643\n",
            "Epoch: 1 | Step: 000135\n",
            "Train Loss: 1.827 | Val Loss: 1.645\n",
            "Epoch: 1 | Step: 000140\n",
            "Train Loss: 1.678 | Val Loss: 1.645\n",
            "Epoch: 1 | Step: 000145\n",
            "Train Loss: 1.605 | Val Loss: 1.638\n",
            "Epoch: 1 | Step: 000150\n",
            "Train Loss: 1.686 | Val Loss: 1.637\n",
            "Epoch: 1 | Step: 000155\n",
            "Train Loss: 1.519 | Val Loss: 1.642\n",
            "Epoch: 1 | Step: 000160\n",
            "Train Loss: 1.455 | Val Loss: 1.646\n",
            "Epoch: 1 | Step: 000165\n",
            "Train Loss: 1.623 | Val Loss: 1.639\n",
            "Epoch: 1 | Step: 000170\n",
            "Train Loss: 1.612 | Val Loss: 1.638\n",
            "Epoch: 1 | Step: 000175\n",
            "Train Loss: 1.703 | Val Loss: 1.645\n",
            "Epoch: 1 | Step: 000180\n",
            "Train Loss: 1.654 | Val Loss: 1.695\n",
            "Epoch: 1 | Step: 000185\n",
            "Train Loss: 1.632 | Val Loss: 1.652\n",
            "Epoch: 1 | Step: 000190\n",
            "Train Loss: 1.745 | Val Loss: 1.652\n",
            "Epoch: 1 | Step: 000195\n",
            "Train Loss: 1.632 | Val Loss: 1.644\n",
            "Epoch: 1 | Step: 000200\n",
            "Train Loss: 1.565 | Val Loss: 1.629\n",
            "Epoch: 1 | Step: 000205\n",
            "Train Loss: 1.754 | Val Loss: 1.626\n",
            "Epoch: 1 | Step: 000210\n",
            "Train Loss: 1.539 | Val Loss: 1.630\n",
            "Epoch: 1 | Step: 000215\n",
            "Train Loss: 1.620 | Val Loss: 1.626\n",
            "Epoch: 1 | Step: 000220\n",
            "Train Loss: 1.558 | Val Loss: 1.619\n",
            "Epoch: 1 | Step: 000225\n",
            "Train Loss: 1.278 | Val Loss: 1.612\n",
            "Epoch: 1 | Step: 000230\n",
            "Train Loss: 1.728 | Val Loss: 1.614\n",
            "Epoch: 1 | Step: 000235\n",
            "Train Loss: 1.466 | Val Loss: 1.614\n",
            "Epoch: 1 | Step: 000240\n",
            "Train Loss: 1.810 | Val Loss: 1.617\n",
            "Epoch: 1 | Step: 000245\n",
            "Train Loss: 1.578 | Val Loss: 1.616\n",
            "Epoch: 1 | Step: 000250\n",
            "Train Loss: 1.541 | Val Loss: 1.620\n",
            "Epoch: 1 | Step: 000255\n",
            "Train Loss: 1.466 | Val Loss: 1.619\n",
            "Epoch: 1 | Step: 000260\n",
            "Train Loss: 1.355 | Val Loss: 1.614\n",
            "Epoch: 1 | Step: 000265\n",
            "Train Loss: 1.654 | Val Loss: 1.614\n",
            "Epoch: 1 | Step: 000270\n",
            "Train Loss: 1.626 | Val Loss: 1.613\n",
            "Epoch: 1 | Step: 000275\n",
            "Train Loss: 1.625 | Val Loss: 1.612\n",
            "Epoch: 1 | Step: 000280\n",
            "Train Loss: 1.587 | Val Loss: 1.619\n",
            "Epoch: 1 | Step: 000285\n",
            "Train Loss: 1.576 | Val Loss: 1.625\n",
            "Epoch: 1 | Step: 000290\n",
            "Train Loss: 1.828 | Val Loss: 1.621\n",
            "Epoch: 1 | Step: 000295\n",
            "Train Loss: 1.582 | Val Loss: 1.615\n",
            "Epoch: 1 | Step: 000300\n",
            "Train Loss: 1.662 | Val Loss: 1.611\n",
            "Epoch: 1 | Step: 000305\n",
            "Train Loss: 1.431 | Val Loss: 1.616\n",
            "Epoch: 1 | Step: 000310\n",
            "Train Loss: 1.641 | Val Loss: 1.620\n",
            "Epoch: 1 | Step: 000315\n",
            "Train Loss: 1.485 | Val Loss: 1.617\n",
            "Epoch: 1 | Step: 000320\n",
            "Train Loss: 1.428 | Val Loss: 1.613\n",
            "Epoch: 1 | Step: 000325\n",
            "Train Loss: 1.525 | Val Loss: 1.612\n",
            "Epoch: 1 | Step: 000330\n",
            "Train Loss: 1.372 | Val Loss: 1.611\n",
            "Epoch: 1 | Step: 000335\n",
            "Train Loss: 1.527 | Val Loss: 1.610\n",
            "Epoch: 1 | Step: 000340\n",
            "Train Loss: 1.486 | Val Loss: 1.611\n",
            "Epoch: 1 | Step: 000345\n",
            "Train Loss: 1.652 | Val Loss: 1.604\n",
            "Epoch: 1 | Step: 000350\n",
            "Train Loss: 1.471 | Val Loss: 1.600\n",
            "Epoch: 1 | Step: 000355\n",
            "Train Loss: 1.682 | Val Loss: 1.600\n",
            "Epoch: 1 | Step: 000360\n",
            "Train Loss: 1.778 | Val Loss: 1.596\n",
            "Epoch: 1 | Step: 000365\n",
            "Train Loss: 1.773 | Val Loss: 1.599\n",
            "Epoch: 1 | Step: 000370\n",
            "Train Loss: 1.365 | Val Loss: 1.603\n",
            "Epoch: 1 | Step: 000375\n",
            "Train Loss: 1.470 | Val Loss: 1.610\n",
            "Epoch: 1 | Step: 000380\n",
            "Train Loss: 1.542 | Val Loss: 1.610\n",
            "Epoch: 1 | Step: 000385\n",
            "Train Loss: 1.523 | Val Loss: 1.605\n",
            "Epoch: 1 | Step: 000390\n",
            "Train Loss: 1.440 | Val Loss: 1.603\n",
            "Epoch: 1 | Step: 000395\n",
            "Train Loss: 1.497 | Val Loss: 1.597\n",
            "Epoch: 1 | Step: 000400\n",
            "Train Loss: 1.599 | Val Loss: 1.593\n",
            "Epoch: 1 | Step: 000405\n",
            "Train Loss: 1.492 | Val Loss: 1.593\n",
            "Epoch: 1 | Step: 000410\n",
            "Train Loss: 1.482 | Val Loss: 1.594\n",
            "Epoch: 1 | Step: 000415\n",
            "Train Loss: 1.479 | Val Loss: 1.598\n",
            "Epoch: 1 | Step: 000420\n",
            "Train Loss: 1.423 | Val Loss: 1.599\n",
            "Epoch: 1 | Step: 000425\n",
            "Train Loss: 1.452 | Val Loss: 1.597\n",
            "Epoch: 1 | Step: 000430\n",
            "Train Loss: 1.642 | Val Loss: 1.596\n",
            "Epoch: 1 | Step: 000435\n",
            "Train Loss: 1.630 | Val Loss: 1.597\n",
            "Epoch: 1 | Step: 000440\n",
            "Train Loss: 1.590 | Val Loss: 1.598\n",
            "Epoch: 1 | Step: 000445\n",
            "Train Loss: 1.471 | Val Loss: 1.599\n",
            "Epoch: 1 | Step: 000450\n",
            "Train Loss: 1.441 | Val Loss: 1.604\n",
            "Epoch: 1 | Step: 000455\n",
            "Train Loss: 1.492 | Val Loss: 1.604\n",
            "Epoch: 1 | Step: 000460\n",
            "Train Loss: 1.586 | Val Loss: 1.599\n",
            "Epoch: 1 | Step: 000465\n",
            "Train Loss: 1.563 | Val Loss: 1.597\n",
            "Epoch: 1 | Step: 000470\n",
            "Train Loss: 1.484 | Val Loss: 1.596\n",
            "Epoch: 1 | Step: 000475\n",
            "Train Loss: 1.588 | Val Loss: 1.595\n",
            "Epoch: 1 | Step: 000480\n",
            "Train Loss: 1.444 | Val Loss: 1.598\n",
            "Epoch: 1 | Step: 000485\n",
            "Train Loss: 1.721 | Val Loss: 1.600\n",
            "Epoch: 1 | Step: 000490\n",
            "Train Loss: 1.519 | Val Loss: 1.596\n",
            "Epoch: 1 | Step: 000495\n",
            "Train Loss: 1.547 | Val Loss: 1.591\n",
            "Epoch: 1 | Step: 000500\n",
            "Train Loss: 1.644 | Val Loss: 1.588\n",
            "Epoch: 1 | Step: 000505\n",
            "Train Loss: 1.344 | Val Loss: 1.587\n",
            "Epoch: 1 | Step: 000510\n",
            "Train Loss: 1.414 | Val Loss: 1.587\n",
            "Epoch: 1 | Step: 000515\n",
            "Train Loss: 1.481 | Val Loss: 1.585\n",
            "Epoch: 1 | Step: 000520\n",
            "Train Loss: 1.476 | Val Loss: 1.585\n",
            "Epoch: 1 | Step: 000525\n",
            "Train Loss: 1.575 | Val Loss: 1.586\n",
            "Epoch: 1 | Step: 000530\n",
            "Train Loss: 1.265 | Val Loss: 1.588\n",
            "Epoch: 1 | Step: 000535\n",
            "Train Loss: 1.696 | Val Loss: 1.587\n",
            "Epoch: 1 | Step: 000540\n",
            "Train Loss: 1.563 | Val Loss: 1.585\n",
            "Epoch: 1 | Step: 000545\n",
            "Train Loss: 1.560 | Val Loss: 1.585\n",
            "Epoch: 1 | Step: 000550\n",
            "Train Loss: 1.604 | Val Loss: 1.589\n",
            "Epoch: 1 | Step: 000555\n",
            "Train Loss: 1.585 | Val Loss: 1.592\n",
            "Epoch: 1 | Step: 000560\n",
            "Train Loss: 1.344 | Val Loss: 1.592\n",
            "Epoch: 1 | Step: 000565\n",
            "Train Loss: 1.367 | Val Loss: 1.591\n",
            "Epoch: 1 | Step: 000570\n",
            "Train Loss: 1.391 | Val Loss: 1.591\n",
            "Epoch: 1 | Step: 000575\n",
            "Train Loss: 1.472 | Val Loss: 1.596\n",
            "Epoch: 1 | Step: 000580\n",
            "Train Loss: 1.608 | Val Loss: 1.596\n",
            "Epoch: 1 | Step: 000585\n",
            "Train Loss: 1.425 | Val Loss: 1.593\n",
            "Epoch: 1 | Step: 000590\n",
            "Train Loss: 1.268 | Val Loss: 1.594\n",
            "Epoch: 1 | Step: 000595\n",
            "Train Loss: 1.339 | Val Loss: 1.594\n",
            "Epoch: 1 | Step: 000600\n",
            "Train Loss: 1.468 | Val Loss: 1.589\n",
            "Epoch: 1 | Step: 000605\n",
            "Train Loss: 1.432 | Val Loss: 1.582\n",
            "Epoch: 1 | Step: 000610\n",
            "Train Loss: 1.569 | Val Loss: 1.576\n",
            "Epoch: 1 | Step: 000615\n",
            "Train Loss: 1.591 | Val Loss: 1.578\n",
            "Epoch: 1 | Step: 000620\n",
            "Train Loss: 1.745 | Val Loss: 1.572\n",
            "Epoch: 1 | Step: 000625\n",
            "Train Loss: 1.490 | Val Loss: 1.569\n",
            "Epoch: 1 | Step: 000630\n",
            "Train Loss: 1.399 | Val Loss: 1.568\n",
            "Epoch: 1 | Step: 000635\n",
            "Train Loss: 1.544 | Val Loss: 1.567\n",
            "Epoch: 1 | Step: 000640\n",
            "Train Loss: 1.513 | Val Loss: 1.568\n",
            "Epoch: 1 | Step: 000645\n",
            "Train Loss: 1.621 | Val Loss: 1.569\n",
            "Epoch: 1 | Step: 000650\n",
            "Train Loss: 1.472 | Val Loss: 1.569\n",
            "Epoch: 1 | Step: 000655\n",
            "Train Loss: 1.279 | Val Loss: 1.566\n",
            "Epoch: 1 | Step: 000660\n",
            "Train Loss: 1.515 | Val Loss: 1.567\n",
            "Epoch: 1 | Step: 000665\n",
            "Train Loss: 1.659 | Val Loss: 1.569\n",
            "Epoch: 1 | Step: 000670\n",
            "Train Loss: 1.419 | Val Loss: 1.567\n",
            "Epoch: 1 | Step: 000675\n",
            "Train Loss: 1.358 | Val Loss: 1.563\n",
            "Epoch: 1 | Step: 000680\n",
            "Train Loss: 1.391 | Val Loss: 1.561\n",
            "Epoch: 1 | Step: 000685\n",
            "Train Loss: 1.311 | Val Loss: 1.567\n",
            "Epoch: 1 | Step: 000690\n",
            "Train Loss: 1.569 | Val Loss: 1.572\n",
            "Epoch: 1 | Step: 000695\n",
            "Train Loss: 1.428 | Val Loss: 1.574\n",
            "Epoch: 1 | Step: 000700\n",
            "Train Loss: 1.511 | Val Loss: 1.579\n",
            "Epoch: 1 | Step: 000705\n",
            "Train Loss: 1.544 | Val Loss: 1.583\n",
            "Epoch: 1 | Step: 000710\n",
            "Train Loss: 1.512 | Val Loss: 1.581\n",
            "Epoch: 1 | Step: 000715\n",
            "Train Loss: 1.330 | Val Loss: 1.581\n",
            "Epoch: 1 | Step: 000720\n",
            "Train Loss: 1.549 | Val Loss: 1.574\n",
            "Epoch: 1 | Step: 000725\n",
            "Train Loss: 1.344 | Val Loss: 1.572\n",
            "Epoch: 1 | Step: 000730\n",
            "Train Loss: 1.710 | Val Loss: 1.571\n",
            "Epoch: 1 | Step: 000735\n",
            "Train Loss: 1.541 | Val Loss: 1.573\n",
            "Epoch: 1 | Step: 000740\n",
            "Train Loss: 1.521 | Val Loss: 1.572\n",
            "Epoch: 1 | Step: 000745\n",
            "Train Loss: 1.611 | Val Loss: 1.575\n",
            "Epoch: 1 | Step: 000750\n",
            "Train Loss: 1.560 | Val Loss: 1.573\n",
            "Epoch: 1 | Step: 000755\n",
            "Train Loss: 1.430 | Val Loss: 1.574\n",
            "Epoch: 1 | Step: 000760\n",
            "Train Loss: 1.443 | Val Loss: 1.573\n",
            "Epoch: 1 | Step: 000765\n",
            "Train Loss: 1.384 | Val Loss: 1.570\n",
            "Epoch: 1 | Step: 000770\n",
            "Train Loss: 1.662 | Val Loss: 1.566\n",
            "Epoch: 1 | Step: 000775\n",
            "Train Loss: 1.737 | Val Loss: 1.566\n",
            "Epoch: 1 | Step: 000780\n",
            "Train Loss: 1.346 | Val Loss: 1.568\n",
            "Epoch: 1 | Step: 000785\n",
            "Train Loss: 1.522 | Val Loss: 1.571\n",
            "Epoch: 1 | Step: 000790\n",
            "Train Loss: 1.322 | Val Loss: 1.571\n",
            "Epoch: 1 | Step: 000795\n",
            "Train Loss: 1.589 | Val Loss: 1.574\n",
            "Epoch: 1 | Step: 000800\n",
            "Train Loss: 1.665 | Val Loss: 1.574\n",
            "Epoch: 1 | Step: 000805\n",
            "Train Loss: 1.384 | Val Loss: 1.574\n",
            "Epoch: 1 | Step: 000810\n",
            "Train Loss: 1.228 | Val Loss: 1.578\n",
            "Epoch: 1 | Step: 000815\n",
            "Train Loss: 1.474 | Val Loss: 1.582\n",
            "Epoch: 1 | Step: 000820\n",
            "Train Loss: 1.240 | Val Loss: 1.584\n",
            "Epoch: 1 | Step: 000825\n",
            "Train Loss: 1.258 | Val Loss: 1.579\n",
            "Epoch: 1 | Step: 000830\n",
            "Train Loss: 1.680 | Val Loss: 1.575\n",
            "Epoch: 1 | Step: 000835\n",
            "Train Loss: 1.499 | Val Loss: 1.576\n",
            "Epoch: 1 | Step: 000840\n",
            "Train Loss: 1.348 | Val Loss: 1.578\n",
            "Epoch: 1 | Step: 000845\n",
            "Train Loss: 1.569 | Val Loss: 1.576\n",
            "Epoch: 1 | Step: 000850\n",
            "Train Loss: 1.461 | Val Loss: 1.574\n",
            "Epoch: 1 | Step: 000855\n",
            "Train Loss: 1.574 | Val Loss: 1.571\n",
            "Epoch: 1 | Step: 000860\n",
            "Train Loss: 1.428 | Val Loss: 1.570\n",
            "Epoch: 1 | Step: 000865\n",
            "Train Loss: 1.356 | Val Loss: 1.572\n",
            "Epoch: 1 | Step: 000870\n",
            "Train Loss: 1.318 | Val Loss: 1.574\n",
            "Epoch: 1 | Step: 000875\n",
            "Train Loss: 1.583 | Val Loss: 1.580\n",
            "Epoch: 1 | Step: 000880\n",
            "Train Loss: 1.539 | Val Loss: 1.583\n",
            "Epoch: 1 | Step: 000885\n",
            "Train Loss: 1.360 | Val Loss: 1.582\n",
            "Epoch: 1 | Step: 000890\n",
            "Train Loss: 1.186 | Val Loss: 1.583\n",
            "Epoch: 1 | Step: 000895\n",
            "Train Loss: 1.569 | Val Loss: 1.583\n",
            "Epoch: 1 | Step: 000900\n",
            "Train Loss: 1.502 | Val Loss: 1.580\n",
            "Epoch: 1 | Step: 000905\n",
            "Train Loss: 1.555 | Val Loss: 1.581\n",
            "Epoch: 1 | Step: 000910\n",
            "Train Loss: 1.419 | Val Loss: 1.579\n",
            "Epoch: 1 | Step: 000915\n",
            "Train Loss: 1.294 | Val Loss: 1.575\n",
            "Epoch: 1 | Step: 000920\n",
            "Train Loss: 1.372 | Val Loss: 1.573\n",
            "Epoch: 1 | Step: 000925\n",
            "Train Loss: 1.458 | Val Loss: 1.574\n",
            "Epoch: 1 | Step: 000930\n",
            "Train Loss: 1.396 | Val Loss: 1.572\n",
            "Epoch: 1 | Step: 000935\n",
            "Train Loss: 1.480 | Val Loss: 1.574\n",
            "Epoch: 1 | Step: 000940\n",
            "Train Loss: 1.110 | Val Loss: 1.571\n",
            "Epoch: 1 | Step: 000945\n",
            "Train Loss: 1.533 | Val Loss: 1.571\n",
            "Epoch: 1 | Step: 000950\n",
            "Train Loss: 1.340 | Val Loss: 1.573\n",
            "Epoch: 1 | Step: 000955\n",
            "Train Loss: 1.493 | Val Loss: 1.575\n",
            "Epoch: 1 | Step: 000960\n",
            "Train Loss: 1.543 | Val Loss: 1.576\n",
            "Epoch: 1 | Step: 000965\n",
            "Train Loss: 1.638 | Val Loss: 1.573\n",
            "Epoch: 1 | Step: 000970\n",
            "Train Loss: 1.337 | Val Loss: 1.568\n",
            "Epoch: 1 | Step: 000975\n",
            "Train Loss: 1.246 | Val Loss: 1.563\n",
            "Epoch: 1 | Step: 000980\n",
            "Train Loss: 1.478 | Val Loss: 1.561\n",
            "Epoch: 1 | Step: 000985\n",
            "Train Loss: 1.526 | Val Loss: 1.562\n",
            "Epoch: 1 | Step: 000990\n",
            "Train Loss: 1.539 | Val Loss: 1.564\n",
            "Epoch: 1 | Step: 000995\n",
            "Train Loss: 1.396 | Val Loss: 1.566\n",
            "Epoch: 1 | Step: 001000\n",
            "Train Loss: 1.359 | Val Loss: 1.569\n",
            "Epoch: 1 | Step: 001005\n",
            "Train Loss: 1.557 | Val Loss: 1.569\n",
            "Epoch: 1 | Step: 001010\n",
            "Train Loss: 1.313 | Val Loss: 1.569\n",
            "Epoch: 1 | Step: 001015\n",
            "Train Loss: 1.363 | Val Loss: 1.573\n",
            "Epoch: 1 | Step: 001020\n",
            "Train Loss: 1.273 | Val Loss: 1.577\n",
            "Epoch: 1 | Step: 001025\n",
            "Train Loss: 1.572 | Val Loss: 1.578\n",
            "Epoch: 1 | Step: 001030\n",
            "Train Loss: 1.490 | Val Loss: 1.575\n",
            "Epoch: 1 | Step: 001035\n",
            "Train Loss: 1.378 | Val Loss: 1.569\n",
            "Epoch: 1 | Step: 001040\n",
            "Train Loss: 1.497 | Val Loss: 1.568\n",
            "Epoch: 1 | Step: 001045\n",
            "Train Loss: 1.345 | Val Loss: 1.569\n",
            "Epoch: 1 | Step: 001050\n",
            "Train Loss: 1.416 | Val Loss: 1.568\n",
            "Epoch: 1 | Step: 001055\n",
            "Train Loss: 1.469 | Val Loss: 1.569\n",
            "Epoch: 1 | Step: 001060\n",
            "Train Loss: 1.275 | Val Loss: 1.569\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Analyze why the internet is important for the economy.  ### Response: The internet is essential for the economy because it allows businesses to communicate and exchange information, and it allows individuals to access information and services that they otherwise would not have access to. This allows businesses to grow and thrive, and\n",
            "Time taken to train: 33.54 minutes\n"
          ]
        }
      ],
      "source": [
        "# training code\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr=0.00005,\n",
        "                              weight_decay=0.1)\n",
        "\n",
        "num_epochs = 1\n",
        "train_losses, val_losses, tokens_seen = train_model_grad_accum(\n",
        "    model=model,\n",
        "    train_dataloader= train_loader,\n",
        "    val_dataloader= val_loader,\n",
        "    optimizer= optimizer,\n",
        "    device=device,\n",
        "    num_epochs= num_epochs,\n",
        "    eval_freq= 5,\n",
        "    eval_iter= 5,\n",
        "    start_context= format_input(val_data[0]),\n",
        "    tokenizer= tokenizer,\n",
        "    grad_accum_steps= 4\n",
        "    )\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "time_in_mins = (end_time - start_time)/60\n",
        "print(f'Time taken to train: {time_in_mins:.2f} minutes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hRXih7UGMpo6",
        "outputId": "f407ac02-f4c8-4aee-c247-b9f305f57f42"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from utilities import plot_losses\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLKLpvfdN2qS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOy0B+XgzlIXyLCe3reU1U3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}