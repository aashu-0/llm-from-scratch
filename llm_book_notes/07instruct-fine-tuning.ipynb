{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","include_colab_link":true},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/aashu-0/llm-from-scratch/blob/main/llm_book_notes/07instruct-fine-tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"markdown","source":"**Instruction fine tuning**\n- also called supervised instruction fine tuning\n- tuning llm to follow instructions\n","metadata":{"id":"oLN12aXxnqCe"}},{"cell_type":"markdown","source":"#### 1. Preparing the dataset","metadata":{"id":"gqSVZCg6n4Pf"}},{"cell_type":"markdown","source":"##### Download the Stanford Alpaca dataset from github","metadata":{"id":"ILOcvWqPpfyK"}},{"cell_type":"code","source":"import json\nimport urllib.request\n\nurl = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/refs/heads/main/alpaca_data.json\"\n\nfile_path = 'alpaca_data.json'\nurllib.request.urlretrieve(url, file_path)\n\n# load\nwith open(file_path, 'r', encoding='utf-8') as f:\n    dataset = json.load(f)\n\nprint(f'Number of entries: {len(dataset)}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dN_nNbNQnvPQ","outputId":"a08fdf47-b426-44ba-c079-ef3497ff19b3","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:07:57.859133Z","iopub.execute_input":"2025-03-29T15:07:57.859436Z","iopub.status.idle":"2025-03-29T15:07:58.535686Z","shell.execute_reply.started":"2025-03-29T15:07:57.859411Z","shell.execute_reply":"2025-03-29T15:07:58.534886Z"}},"outputs":[{"name":"stdout","text":"Number of entries: 52002\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# a subset of the dataset -> 10k samples\n\nimport random\nsubset_size = 5000\nrandom.seed(42)\nsubset_data = random.sample(dataset, subset_size) # a list of dictionaries\n\n# save\nsubset_file_path = 'alpaca_subset.json'\n\n# to convert list to json-formatted string\nwith open(subset_file_path, 'w', encoding='utf-8') as f:\n    json.dump(subset_data, f, indent=4)\n\nprint(f'Number of entries: {len(subset_data)}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HlE9moQwnqCi","outputId":"b9aeabe5-c4cd-4878-8fd6-6dadb2840cc2","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:07:58.536548Z","iopub.execute_input":"2025-03-29T15:07:58.536765Z","iopub.status.idle":"2025-03-29T15:07:58.589844Z","shell.execute_reply.started":"2025-03-29T15:07:58.536747Z","shell.execute_reply":"2025-03-29T15:07:58.589008Z"}},"outputs":[{"name":"stdout","text":"Number of entries: 5000\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# subset_data[:18]","metadata":{"id":"ndmhhUpvnqCj","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:07:58.591646Z","iopub.execute_input":"2025-03-29T15:07:58.591925Z","iopub.status.idle":"2025-03-29T15:07:58.595425Z","shell.execute_reply.started":"2025-03-29T15:07:58.591903Z","shell.execute_reply":"2025-03-29T15:07:58.594365Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# load the subset_data\n\nwith open(subset_file_path, 'r', encoding='utf-8') as f:\n    subset_dataset = json.load(f)\n\nprint(f'Number of entries: {len(subset_dataset)}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yj6cb4KzsJSQ","outputId":"acf137eb-86c3-4b4b-bb6a-119bf2c9b78c","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:07:58.596656Z","iopub.execute_input":"2025-03-29T15:07:58.596986Z","iopub.status.idle":"2025-03-29T15:07:58.627399Z","shell.execute_reply.started":"2025-03-29T15:07:58.596956Z","shell.execute_reply":"2025-03-29T15:07:58.626643Z"}},"outputs":[{"name":"stdout","text":"Number of entries: 5000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(f'Example:\\n {subset_dataset[4000]}') # no input section","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_IhYoeGsheI","outputId":"89837247-29b8-410a-f4a9-1cc7d1a37af6","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:07:58.628195Z","iopub.execute_input":"2025-03-29T15:07:58.628439Z","iopub.status.idle":"2025-03-29T15:07:58.632760Z","shell.execute_reply.started":"2025-03-29T15:07:58.628420Z","shell.execute_reply":"2025-03-29T15:07:58.631845Z"}},"outputs":[{"name":"stdout","text":"Example:\n {'instruction': 'Edit this sentence to use proper English: I dont think so', 'input': '', 'output': \"I don't think so.\"}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(f'Example:\\n {subset_dataset[1000]}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A_odMQW8urv2","outputId":"76372535-3c84-43e0-8ac6-e37561115792","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:07:58.633653Z","iopub.execute_input":"2025-03-29T15:07:58.634034Z","iopub.status.idle":"2025-03-29T15:07:58.650947Z","shell.execute_reply.started":"2025-03-29T15:07:58.633985Z","shell.execute_reply":"2025-03-29T15:07:58.650093Z"}},"outputs":[{"name":"stdout","text":"Example:\n {'instruction': \"Explain why the Earth's temperature fluctuates.\", 'input': '', 'output': \"The Earth's temperature fluctuates due to changes in the amount of energy from the Sun that is received, released or reflected by the atmosphere and surface of the Earth. Changes in clouds, aerosols, oceans, and land also affect temperature. Additionally, increases in certain gasses, such as carbon dioxide, traps heat within the atmosphere and causes further warming.\"}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"so the structure of the dataset looks like:\n\n\n```\n{'instruction\":\n  'input':  #may be empty\n  'output':}\n```\nthere are various ways to format these entries:\n- `Alpaca prompt style`\n- `Phi-3 prompt style`\n","metadata":{"id":"mcgUXRJLs3MX"}},{"cell_type":"markdown","source":"##### Implementing the formatting function","metadata":{"id":"-BrCFCxStmBn"}},{"cell_type":"code","source":"def format_input(entry):\n  instruction_txt = (\n      f\"Below is an instruction that describes a task. \"\n      f\"Write a response that appropriately completes the request.\\n\\n\"\n      f\"### Instruction:\\n{entry['instruction']}\"\n  )\n  input_text = (\n      f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else ''\n  )\n  return instruction_txt + input_text\n","metadata":{"id":"NwPvC0XJsqAV","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:07:58.651694Z","iopub.execute_input":"2025-03-29T15:07:58.651964Z","iopub.status.idle":"2025-03-29T15:07:58.666551Z","shell.execute_reply.started":"2025-03-29T15:07:58.651944Z","shell.execute_reply":"2025-03-29T15:07:58.665904Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model_input = format_input(subset_dataset[1000])\ndesired_response = f\"\\n\\n### Response:\\n{subset_dataset[1000]['output']}\"\n\nprint(model_input + desired_response)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPCvopLduljG","outputId":"e60d488e-bd48-4c48-f53b-7018291997b6","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:07:58.668932Z","iopub.execute_input":"2025-03-29T15:07:58.669156Z","iopub.status.idle":"2025-03-29T15:07:58.685401Z","shell.execute_reply.started":"2025-03-29T15:07:58.669135Z","shell.execute_reply":"2025-03-29T15:07:58.684734Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nExplain why the Earth's temperature fluctuates.\n\n### Response:\nThe Earth's temperature fluctuates due to changes in the amount of energy from the Sun that is received, released or reflected by the atmosphere and surface of the Earth. Changes in clouds, aerosols, oceans, and land also affect temperature. Additionally, increases in certain gasses, such as carbon dioxide, traps heat within the atmosphere and causes further warming.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"##### Train Test Split","metadata":{"id":"SHxyLt_Kvvpd"}},{"cell_type":"code","source":"train_set = int(len(subset_dataset)*0.85)\ntest_set = int(len(subset_dataset)*0.1)\nval_set = len(subset_dataset) - train_set - test_set\n\ntrain_data = subset_dataset[:train_set]\ntest_data = subset_dataset[train_set:train_set+test_set]\nval_data = subset_dataset[train_set+test_set:]\n\nprint(f'Train set size: {len(train_data)}')\nprint(f'Test set size: {len(test_data)}')\nprint(f'Validation set size: {len(val_data)}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3KXJhaLRupYe","outputId":"c5897ccd-53d1-4113-b400-83e9b3714b70","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:07:58.687036Z","iopub.execute_input":"2025-03-29T15:07:58.687249Z","iopub.status.idle":"2025-03-29T15:07:58.704742Z","shell.execute_reply.started":"2025-03-29T15:07:58.687230Z","shell.execute_reply":"2025-03-29T15:07:58.704105Z"}},"outputs":[{"name":"stdout","text":"Train set size: 4250\nTest set size: 500\nValidation set size: 250\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"#### 2. Organizing data into batches","metadata":{"id":"nslTe1olwPH7"}},{"cell_type":"markdown","source":"`collate` function in pytorch\n- used to batch samples together into a single batch\n- allows custom preprocessing, when dealing with variable-length data\n\n1. `default_collate`\n- stackes tensor along the first dim\n- convert lists into tensors\n- leaves dic and other data structures untouched\n2. `collate_fn`\n- for creating custom collate function","metadata":{"id":"Pc7zp_gzwc_-"}},{"cell_type":"markdown","source":"**Custom Collate function**\n\n1. format data\n2. tokenize\n3. adjust them to have same length using padding tokens\n4. create target token ids ---inputs shifted by `1`\n5. replace certain pad tokens\n  * why? --- do not contain useful info so excluded from loss computation\n  * `ignore_index`: placeholder value(`-100`)\n\n","metadata":{"id":"ecrRbrB-yHm2"}},{"cell_type":"markdown","source":"##### Instruction dataset class","metadata":{"id":"abKUIstV0Y4S"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass InstructionDataset(Dataset):\n  def __init__(self, data, tokenizer):\n    self.data = data\n    self.encode_texts = []\n    for entry in data:\n      input_with_instruction = format_input(entry)\n      response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n      full_text = input_with_instruction + response_text\n      self.encode_texts.append(\n          tokenizer.encode(full_text)\n      )\n\n  def __getitem__(self, index):\n     return self.encode_texts[index]\n\n  def __len__(self):\n    return len(self.data)","metadata":{"id":"P7RNq-GY0n1I","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:07:58.705473Z","iopub.execute_input":"2025-03-29T15:07:58.705657Z","iopub.status.idle":"2025-03-29T15:08:05.148991Z","shell.execute_reply.started":"2025-03-29T15:07:58.705640Z","shell.execute_reply":"2025-03-29T15:08:05.148289Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"##### Custom Batch collate function","metadata":{"id":"ejphi6jd3jCs"}},{"cell_type":"code","source":"def custom_collate_fn(\n    batch,\n    pad_token_id= 50256,\n    allowed_max_length=None,\n    ignore_index=-100,\n    device = 'cpu'):\n\n  batch_max_length = max(len(item) for item in batch)\n  inputs_lst, targets_lst = [], []\n  for item in batch:\n    new_item = item.copy()\n    new_item += [pad_token_id]\n\n    padded = (new_item + [pad_token_id]* (batch_max_length - len(item)))\n    inputs= torch.tensor(padded[:-1])\n    targets= torch.tensor(padded[1:])\n\n    mask = targets == pad_token_id\n    indices = torch.nonzero(mask).squeeze() # returns the indices of True values\n    if indices.numel() >1:\n      targets[indices[1:]] = ignore_index\n\n    if allowed_max_length is not None:\n      inputs = inputs[:allowed_max_length]\n      targets = targets[:allowed_max_length]\n\n    inputs_lst.append(inputs)\n    targets_lst.append(targets)\n\n  inputs_tensor = torch.stack(inputs_lst).to(device)\n  targets_tensor = torch.stack(targets_lst).to(device)\n\n  return inputs_tensor, targets_tensor\n","metadata":{"id":"GLoBwzGY3n4r","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:05.149760Z","iopub.execute_input":"2025-03-29T15:08:05.150122Z","iopub.status.idle":"2025-03-29T15:08:05.156375Z","shell.execute_reply.started":"2025-03-29T15:08:05.150099Z","shell.execute_reply":"2025-03-29T15:08:05.155463Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# example\ninput1 = [0,1,2,3,4]\ninput2 = [5,6]\ninput3 = [7,8,9]\n\nbatch = [input1, input2, input3]\n\ninput, target = custom_collate_fn(batch)\nprint(input)\nprint(target)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZ3MMTyu3n01","outputId":"84df218f-e423-46e1-faf4-a6a586b34b3d","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:05.157408Z","iopub.execute_input":"2025-03-29T15:08:05.157684Z","iopub.status.idle":"2025-03-29T15:08:05.327268Z","shell.execute_reply.started":"2025-03-29T15:08:05.157662Z","shell.execute_reply":"2025-03-29T15:08:05.326266Z"}},"outputs":[{"name":"stdout","text":"tensor([[    0,     1,     2,     3,     4],\n        [    5,     6, 50256, 50256, 50256],\n        [    7,     8,     9, 50256, 50256]])\ntensor([[    1,     2,     3,     4, 50256],\n        [    6, 50256,  -100,  -100,  -100],\n        [    8,     9, 50256,  -100,  -100]])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"Why `-100`?\n\nby default `ignore_index` in `cross_entropy()` is equal to `-100`.\n\ntherefore, it ignores targets labeled with -100","metadata":{"id":"drBrpXOe88DJ"}},{"cell_type":"markdown","source":"Masking out the instruction text- so that model focuses on generating accurate responses ratehr than memorizing instructions.\n\nwill do later","metadata":{"id":"49QTXQwO9ath"}},{"cell_type":"markdown","source":"##### Creating dataloaders","metadata":{"id":"PhrzJAon-Ttz"}},{"cell_type":"code","source":"# device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hvQYI40d1fge","outputId":"b4551316-562a-46b1-8fc6-2eb069ef776d","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:05.328301Z","iopub.execute_input":"2025-03-29T15:08:05.328620Z","iopub.status.idle":"2025-03-29T15:08:05.414204Z","shell.execute_reply.started":"2025-03-29T15:08:05.328589Z","shell.execute_reply":"2025-03-29T15:08:05.413295Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# fix or pre-fill some argument in custom_collate_function\nfrom functools import partial\ncustom_collate_fn = partial(\n    custom_collate_fn,\n    allowed_max_length=512,\n    device=device\n)","metadata":{"id":"OU-tPPhH-hsh","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:05.415118Z","iopub.execute_input":"2025-03-29T15:08:05.415380Z","iopub.status.idle":"2025-03-29T15:08:05.438734Z","shell.execute_reply.started":"2025-03-29T15:08:05.415362Z","shell.execute_reply":"2025-03-29T15:08:05.437893Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"!pip install tiktoken","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QHlRyY2j_iz4","outputId":"70dea3ff-f511-4050-db41-3fe1195f9da2","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:05.439628Z","iopub.execute_input":"2025-03-29T15:08:05.440111Z","iopub.status.idle":"2025-03-29T15:08:12.097742Z","shell.execute_reply.started":"2025-03-29T15:08:05.440080Z","shell.execute_reply":"2025-03-29T15:08:12.096846Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.9.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# tokenizer\nimport tiktoken\ntokenizer = tiktoken.get_encoding('gpt2')","metadata":{"id":"-CHPb3yj_cV7","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:12.098839Z","iopub.execute_input":"2025-03-29T15:08:12.099139Z","iopub.status.idle":"2025-03-29T15:08:13.168155Z","shell.execute_reply.started":"2025-03-29T15:08:12.099114Z","shell.execute_reply":"2025-03-29T15:08:13.167400Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# dataloader\nfrom torch.utils.data import DataLoader\n\nnum_workers = 0\nbatch_size =4\n\ntrain_dataset = InstructionDataset(train_data, tokenizer)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=custom_collate_fn,\n    num_workers=num_workers,\n    drop_last=True,\n)\n\nval_dataset = InstructionDataset(val_data, tokenizer)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=custom_collate_fn,\n    num_workers=num_workers,\n    drop_last=False,\n)\n\ntest_dataset = InstructionDataset(test_data, tokenizer)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=custom_collate_fn,\n    num_workers=num_workers,\n    drop_last=False,\n)","metadata":{"id":"a4gBllU-_HxU","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:13.168976Z","iopub.execute_input":"2025-03-29T15:08:13.169214Z","iopub.status.idle":"2025-03-29T15:08:13.482473Z","shell.execute_reply.started":"2025-03-29T15:08:13.169194Z","shell.execute_reply":"2025-03-29T15:08:13.481733Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"for inputs, targets in train_loader:\n  print(f'Inputs: {inputs.shape}, Targets: {targets.shape}')\n  break","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bf7R4JAEAZnI","outputId":"1da1b28b-c458-4fc6-e383-15d734599ec2","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:13.483270Z","iopub.execute_input":"2025-03-29T15:08:13.483474Z","iopub.status.idle":"2025-03-29T15:08:13.778128Z","shell.execute_reply.started":"2025-03-29T15:08:13.483456Z","shell.execute_reply":"2025-03-29T15:08:13.777154Z"}},"outputs":[{"name":"stdout","text":"Inputs: torch.Size([4, 99]), Targets: torch.Size([4, 99])\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"#### Loading Pretrained LLM","metadata":{"id":"dSgqkHPNBPaM"}},{"cell_type":"code","source":"# getting scripts from github\n!git clone https://github.com/aashu-0/llm-from-scratch.git\n%cd llm-from-scratch/llm_book_notes","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ICcOslIPAi-d","outputId":"0ae3472d-cb57-4d22-9363-52af25b7e0db","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:13.779229Z","iopub.execute_input":"2025-03-29T15:08:13.779536Z","iopub.status.idle":"2025-03-29T15:08:14.991096Z","shell.execute_reply.started":"2025-03-29T15:08:13.779506Z","shell.execute_reply":"2025-03-29T15:08:14.990123Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'llm-from-scratch'...\nremote: Enumerating objects: 132, done.\u001b[K\nremote: Counting objects: 100% (132/132), done.\u001b[K\nremote: Compressing objects: 100% (96/96), done.\u001b[K\nremote: Total 132 (delta 71), reused 79 (delta 32), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (132/132), 176.77 KiB | 2.60 MiB/s, done.\nResolving deltas: 100% (71/71), done.\n/kaggle/working/llm-from-scratch/llm_book_notes\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import sys\nsys.path.append('/content/llm-from-scratch/llm_book_notes')","metadata":{"id":"98QVze3pDO8i","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:14.992258Z","iopub.execute_input":"2025-03-29T15:08:14.992626Z","iopub.status.idle":"2025-03-29T15:08:14.997461Z","shell.execute_reply.started":"2025-03-29T15:08:14.992590Z","shell.execute_reply":"2025-03-29T15:08:14.996574Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# get gpt_download.py from @rasbt github\nimport urllib.request\nurl = (\n    \"https://raw.githubusercontent.com/rasbt/\"\n    \"LLMs-from-scratch/main/ch05/\"\n    \"01_main-chapter-code/gpt_download.py\"\n)\nurllib.request.urlretrieve(url, \"gpt_download.py\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8YFmNihMDecq","outputId":"d0f83097-293f-4253-a5be-223127c1ecb4","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:14.998538Z","iopub.execute_input":"2025-03-29T15:08:14.998887Z","iopub.status.idle":"2025-03-29T15:08:15.173521Z","shell.execute_reply.started":"2025-03-29T15:08:14.998857Z","shell.execute_reply":"2025-03-29T15:08:15.172645Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('gpt_download.py', <http.client.HTTPMessage at 0x7f0889b5b7c0>)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"from load_weights import load_weights_into_gpt\nfrom GPT import GPTModel\nfrom gpt_download import download_and_load_gpt2\n\nBASE_CONFIG = {\n    'vocab_size': 50257,\n    'context_length': 1024,\n    'drop_rate': 0.2,\n    'qkv_bias': True\n}\n\nmodel_configs = {\n    'gpt2 (124M)': {'emb_dim': 768 , 'n_layers': 12, 'n_heads': 12},\n    'gpt2-medium (355M)': {'emb_dim':1024 , 'n_layers':24, 'n_heads':16},\n    'gpt2-large (774M)': {'emb_dim': 1280 , 'n_layers': 36, 'n_heads':20},\n    'gpt2-xl (1558M)': {'emb_dim': 1600, 'n_layers':48, 'n_heads': 25}\n}\n\nCHOOSE_MODEL = 'gpt2-medium (355M)'\nBASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n\nmodel_size = CHOOSE_MODEL.split(' ')[-1].lstrip('(').rstrip(')')\n\nsettings, params = download_and_load_gpt2(\n    model_size = model_size,\n    models_dir = 'gpt2'\n)\n\nmodel = GPTModel(BASE_CONFIG)\nload_weights_into_gpt(model, params)\nmodel.eval();","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CBFDzZPDDS2r","outputId":"1c81cb55-6061-46cc-e1cc-4b9071079c49","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:08:15.174538Z","iopub.execute_input":"2025-03-29T15:08:15.174862Z","iopub.status.idle":"2025-03-29T15:09:33.754656Z","shell.execute_reply.started":"2025-03-29T15:08:15.174837Z","shell.execute_reply":"2025-03-29T15:09:33.753838Z"}},"outputs":[{"name":"stderr","text":"checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 95.6kiB/s]\nencoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 6.13MiB/s]\nhparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 120kiB/s]\nmodel.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [00:49<00:00, 28.8MiB/s]\nmodel.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 11.5MiB/s]\nmodel.ckpt.meta: 100%|██████████| 927k/927k [00:00<00:00, 7.15MiB/s]\nvocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 3.08MiB/s]\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# assess our raw model(no fine tuning)\ntorch.manual_seed(123)\ninput_text = format_input(val_data[0])\nprint(input_text)","metadata":{"id":"9BSQP98CIHhW","outputId":"1f720515-6e15-476b-d48c-b3afe392c681","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:09:33.757930Z","iopub.execute_input":"2025-03-29T15:09:33.758485Z","iopub.status.idle":"2025-03-29T15:09:33.782045Z","shell.execute_reply.started":"2025-03-29T15:09:33.758458Z","shell.execute_reply":"2025-03-29T15:09:33.781232Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a sentence that represents the content in the paragraph.\n\n### Input:\nA new law was introduced in 2020 outlining five safety measures all workplaces must follow to prevent the spread of Covid-19. This includes regularly sanitizing the premises, implementing social distancing measures, and introducing a screening and temperature checking procedure.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# model's response\nfrom utilities import text_to_token_ids, token_ids_to_text\nfrom GPT import generate\n\ntoken_ids = generate(\n    model = model,\n    idx = text_to_token_ids(input_text, tokenizer),\n    max_new_tokens =35,\n    context_size= BASE_CONFIG['context_length'],\n    eos_id= 50256\n)\ngenerated_text =token_ids_to_text(token_ids, tokenizer)\ngenerated_text","metadata":{"id":"ATSSV8UxJBI0","outputId":"033c74fc-1db1-4f6f-e021-cf436c03494c","colab":{"base_uri":"https://localhost:8080/","height":87},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:09:33.783184Z","iopub.execute_input":"2025-03-29T15:09:33.783431Z","iopub.status.idle":"2025-03-29T15:09:57.837736Z","shell.execute_reply.started":"2025-03-29T15:09:33.783410Z","shell.execute_reply":"2025-03-29T15:09:57.836913Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a sentence that represents the content in the paragraph.\\n\\n### Input:\\nA new law was introduced in 2020 outlining five safety measures all workplaces must follow to prevent the spread of Covid-19. This includes regularly sanitizing the premises, implementing social distancing measures, and introducing a screening and temperature checking procedure.\\n\\n### Output:\\n\\nThe law was passed and the new law is now in effect.\\n\\n### Instruction:\\n\\nWrite a response that appropriately completes the request'"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"the generate function returns combined input and output text,separating the output","metadata":{"id":"vj9r6qFsKq1I"}},{"cell_type":"code","source":"response_text = generated_text[len(input_text):].strip()\nprint(response_text)","metadata":{"id":"lgJkvgJfJts6","outputId":"ddcb2157-ee5f-4447-bd7f-4a49da268b25","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:09:57.838822Z","iopub.execute_input":"2025-03-29T15:09:57.839218Z","iopub.status.idle":"2025-03-29T15:09:57.844489Z","shell.execute_reply.started":"2025-03-29T15:09:57.839172Z","shell.execute_reply":"2025-03-29T15:09:57.843786Z"}},"outputs":[{"name":"stdout","text":"### Output:\n\nThe law was passed and the new law is now in effect.\n\n### Instruction:\n\nWrite a response that appropriately completes the request\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"#### Finetuning LLM on instruction dataset we loaded eariler","metadata":{"id":"n7BEzxq9F4rt"}},{"cell_type":"code","source":"from utilities import train_model_simple, calc_loss_loader","metadata":{"id":"_NUYwkftGFRs","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:09:57.845276Z","iopub.execute_input":"2025-03-29T15:09:57.845575Z","iopub.status.idle":"2025-03-29T15:09:57.863545Z","shell.execute_reply.started":"2025-03-29T15:09:57.845542Z","shell.execute_reply":"2025-03-29T15:09:57.862788Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# initial loss for the train and val dataset\nmodel.to(device)\ntorch.manual_seed(123)\nwith torch.no_grad():\n  train_loss = calc_loss_loader(train_loader, model, device)\n  val_loss = calc_loss_loader(val_loader, model, device)\n\nprint(f'Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}')","metadata":{"id":"vxvxerzLGFL3","outputId":"ed4ea737-4762-4c97-91b4-4ebdfbefbb43","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:09:57.864589Z","iopub.execute_input":"2025-03-29T15:09:57.864975Z","iopub.status.idle":"2025-03-29T15:13:12.056420Z","shell.execute_reply.started":"2025-03-29T15:09:57.864942Z","shell.execute_reply":"2025-03-29T15:13:12.055341Z"}},"outputs":[{"name":"stdout","text":"Train loss: 3.2105, Val loss: 3.2689\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# training code\nimport time\n\nstart_time = time.time()\ntorch.manual_seed(123)\n\noptimizer = torch.optim.AdamW(model.parameters(),\n                              lr=0.00005,\n                              weight_decay=0.1)\n\nnum_epochs = 2\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model=model,\n    train_dataloader= train_loader,\n    val_dataloader= val_loader,\n    optimizer= optimizer,\n    device=device,\n    num_epochs= num_epochs,\n    eval_freq= 5,\n    eval_iter= 5,\n    start_context= format_input(val_data[0]),\n    tokenizer= tokenizer\n    )\n\nend_time = time.time()\n\ntime_in_mins = (end_time - start_time)/60\nprint(f'Time taken to train: {time_in_mins:.2f} minutes')","metadata":{"id":"E0WKl65uK70h","outputId":"d54f670a-9265-420a-85b1-414ed37f4f9c","colab":{"base_uri":"https://localhost:8080/","height":408},"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:13:12.057499Z","iopub.execute_input":"2025-03-29T15:13:12.057853Z","iopub.status.idle":"2025-03-29T15:15:15.595229Z","shell.execute_reply.started":"2025-03-29T15:13:12.057828Z","shell.execute_reply":"2025-03-29T15:15:15.593780Z"}},"outputs":[{"name":"stdout","text":"Epoch: 1 | Step: 000000\nTrain Loss: 2.897 | Val Loss: 2.797\nEpoch: 1 | Step: 000005\nTrain Loss: 1.931 | Val Loss: 1.959\nEpoch: 1 | Step: 000010\nTrain Loss: 1.687 | Val Loss: 1.660\nEpoch: 1 | Step: 000015\nTrain Loss: 1.596 | Val Loss: 1.622\nEpoch: 1 | Step: 000020\nTrain Loss: 1.721 | Val Loss: 1.584\nEpoch: 1 | Step: 000025\nTrain Loss: 1.609 | Val Loss: 1.562\nEpoch: 1 | Step: 000030\nTrain Loss: 1.688 | Val Loss: 1.557\nEpoch: 1 | Step: 000035\nTrain Loss: 1.620 | Val Loss: 1.548\nEpoch: 1 | Step: 000040\nTrain Loss: 1.522 | Val Loss: 1.532\nEpoch: 1 | Step: 000045\nTrain Loss: 1.510 | Val Loss: 1.529\nEpoch: 1 | Step: 000050\nTrain Loss: 1.439 | Val Loss: 1.525\nEpoch: 1 | Step: 000055\nTrain Loss: 1.479 | Val Loss: 1.519\nEpoch: 1 | Step: 000060\nTrain Loss: 1.699 | Val Loss: 1.519\nEpoch: 1 | Step: 000065\nTrain Loss: 1.521 | Val Loss: 1.525\nEpoch: 1 | Step: 000070\nTrain Loss: 1.510 | Val Loss: 1.523\nEpoch: 1 | Step: 000075\nTrain Loss: 1.668 | Val Loss: 1.519\nEpoch: 1 | Step: 000080\nTrain Loss: 1.444 | Val Loss: 1.506\nEpoch: 1 | Step: 000085\nTrain Loss: 1.504 | Val Loss: 1.503\nEpoch: 1 | Step: 000090\nTrain Loss: 1.459 | Val Loss: 1.498\nEpoch: 1 | Step: 000095\nTrain Loss: 1.688 | Val Loss: 1.500\nEpoch: 1 | Step: 000100\nTrain Loss: 1.664 | Val Loss: 1.508\nEpoch: 1 | Step: 000105\nTrain Loss: 1.610 | Val Loss: 1.504\nEpoch: 1 | Step: 000110\nTrain Loss: 1.478 | Val Loss: 1.496\nEpoch: 1 | Step: 000115\nTrain Loss: 1.584 | Val Loss: 1.495\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-8dd747450c23>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m train_losses, val_losses, tokens_seen = train_model_simple(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/llm-from-scratch/llm_book_notes/utilities.py\u001b[0m in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/llm-from-scratch/llm_book_notes/utilities.py\u001b[0m in \u001b[0;36mcalc_loss_batch\u001b[0;34m(input_batch, target_batch, model, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtarget_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/llm-from-scratch/llm_book_notes/GPT.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrf_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/llm-from-scratch/llm_book_notes/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# pre-layer norm (opposite to original transformer model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_shortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshortcut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/llm-from-scratch/llm_book_notes/MHA.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mcontext_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dropout probability has to be between 0 and 1, but got {p}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     return (\n\u001b[0;32m-> 1425\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m     )\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 50.12 MiB is free. Process 2923 has 14.69 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 597.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 50.12 MiB is free. Process 2923 has 14.69 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 597.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"KRnWHrerNil3","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:15:15.595795Z","iopub.status.idle":"2025-03-29T15:15:15.596096Z","shell.execute_reply":"2025-03-29T15:15:15.595975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}