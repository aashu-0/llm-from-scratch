{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aashu-0/llm-from-scratch/blob/main/llm_book_notes/07instruct-fine-tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZKJ7RZeon6i",
        "outputId": "a7d1c094-c065-4bed-c550-c5516adc56c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar 29 16:16:14 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instruction fine tuning**\n",
        "- also called supervised instruction fine tuning\n",
        "- tuning llm to follow instructions\n"
      ],
      "metadata": {
        "id": "oLN12aXxnqCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Preparing the dataset"
      ],
      "metadata": {
        "id": "gqSVZCg6n4Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Download the Stanford Alpaca dataset from github"
      ],
      "metadata": {
        "id": "ILOcvWqPpfyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/refs/heads/main/alpaca_data.json\"\n",
        "\n",
        "file_path = 'alpaca_data.json'\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "# load\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "print(f'Number of entries: {len(dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN_nNbNQnvPQ",
        "outputId": "1a8e4c99-fbc0-4131-ddee-863407ee427f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:07:57.859133Z",
          "iopub.execute_input": "2025-03-29T15:07:57.859436Z",
          "iopub.status.idle": "2025-03-29T15:07:58.535686Z",
          "shell.execute_reply.started": "2025-03-29T15:07:57.859411Z",
          "shell.execute_reply": "2025-03-29T15:07:58.534886Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 52002\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# a subset of the dataset -> 10k samples\n",
        "\n",
        "import random\n",
        "subset_size = 5000\n",
        "random.seed(42)\n",
        "subset_data = random.sample(dataset, subset_size) # a list of dictionaries\n",
        "\n",
        "# save\n",
        "subset_file_path = 'alpaca_subset.json'\n",
        "\n",
        "# to convert list to json-formatted string\n",
        "with open(subset_file_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(subset_data, f, indent=4)\n",
        "\n",
        "print(f'Number of entries: {len(subset_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlE9moQwnqCi",
        "outputId": "d79a3df7-5835-4e7d-d834-f85ccc0db7f1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:07:58.536548Z",
          "iopub.execute_input": "2025-03-29T15:07:58.536765Z",
          "iopub.status.idle": "2025-03-29T15:07:58.589844Z",
          "shell.execute_reply.started": "2025-03-29T15:07:58.536747Z",
          "shell.execute_reply": "2025-03-29T15:07:58.589008Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 5000\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "# subset_data[:18]"
      ],
      "metadata": {
        "id": "ndmhhUpvnqCj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:07:58.591646Z",
          "iopub.execute_input": "2025-03-29T15:07:58.591925Z",
          "iopub.status.idle": "2025-03-29T15:07:58.595425Z",
          "shell.execute_reply.started": "2025-03-29T15:07:58.591903Z",
          "shell.execute_reply": "2025-03-29T15:07:58.594365Z"
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "# load the subset_data\n",
        "\n",
        "with open(subset_file_path, 'r', encoding='utf-8') as f:\n",
        "    subset_dataset = json.load(f)\n",
        "\n",
        "print(f'Number of entries: {len(subset_dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj6cb4KzsJSQ",
        "outputId": "f8d7ec1b-086f-4c7c-8c3d-968164a1b82c",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:07:58.596656Z",
          "iopub.execute_input": "2025-03-29T15:07:58.596986Z",
          "iopub.status.idle": "2025-03-29T15:07:58.627399Z",
          "shell.execute_reply.started": "2025-03-29T15:07:58.596956Z",
          "shell.execute_reply": "2025-03-29T15:07:58.626643Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 5000\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Example:\\n {subset_dataset[4000]}') # no input section"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_IhYoeGsheI",
        "outputId": "ced8c694-b13b-438d-d81f-04d3f2a463ec",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:07:58.628195Z",
          "iopub.execute_input": "2025-03-29T15:07:58.628439Z",
          "iopub.status.idle": "2025-03-29T15:07:58.632760Z",
          "shell.execute_reply.started": "2025-03-29T15:07:58.628420Z",
          "shell.execute_reply": "2025-03-29T15:07:58.631845Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example:\n",
            " {'instruction': 'Edit this sentence to use proper English: I dont think so', 'input': '', 'output': \"I don't think so.\"}\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Example:\\n {subset_dataset[1000]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_odMQW8urv2",
        "outputId": "64cacc1f-22b9-4c84-c017-db88dfd1b036",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:07:58.633653Z",
          "iopub.execute_input": "2025-03-29T15:07:58.634034Z",
          "iopub.status.idle": "2025-03-29T15:07:58.650947Z",
          "shell.execute_reply.started": "2025-03-29T15:07:58.633985Z",
          "shell.execute_reply": "2025-03-29T15:07:58.650093Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example:\n",
            " {'instruction': \"Explain why the Earth's temperature fluctuates.\", 'input': '', 'output': \"The Earth's temperature fluctuates due to changes in the amount of energy from the Sun that is received, released or reflected by the atmosphere and surface of the Earth. Changes in clouds, aerosols, oceans, and land also affect temperature. Additionally, increases in certain gasses, such as carbon dioxide, traps heat within the atmosphere and causes further warming.\"}\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "so the structure of the dataset looks like:\n",
        "\n",
        "\n",
        "```\n",
        "{'instruction\":\n",
        "  'input':  #may be empty\n",
        "  'output':}\n",
        "```\n",
        "there are various ways to format these entries:\n",
        "- `Alpaca prompt style`\n",
        "- `Phi-3 prompt style`\n"
      ],
      "metadata": {
        "id": "mcgUXRJLs3MX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Implementing the formatting function"
      ],
      "metadata": {
        "id": "-BrCFCxStmBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(entry):\n",
        "  instruction_txt = (\n",
        "      f\"Below is an instruction that describes a task. \"\n",
        "      f\"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      f\"### Instruction:\\n{entry['instruction']}\"\n",
        "  )\n",
        "  input_text = (\n",
        "      f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else ''\n",
        "  )\n",
        "  return instruction_txt + input_text\n"
      ],
      "metadata": {
        "id": "NwPvC0XJsqAV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:07:58.651694Z",
          "iopub.execute_input": "2025-03-29T15:07:58.651964Z",
          "iopub.status.idle": "2025-03-29T15:07:58.666551Z",
          "shell.execute_reply.started": "2025-03-29T15:07:58.651944Z",
          "shell.execute_reply": "2025-03-29T15:07:58.665904Z"
        }
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = format_input(subset_dataset[1000])\n",
        "desired_response = f\"\\n\\n### Response:\\n{subset_dataset[1000]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPCvopLduljG",
        "outputId": "b4f65a69-3d55-45fc-9bfd-a4bdcaa9a6ad",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:07:58.668932Z",
          "iopub.execute_input": "2025-03-29T15:07:58.669156Z",
          "iopub.status.idle": "2025-03-29T15:07:58.685401Z",
          "shell.execute_reply.started": "2025-03-29T15:07:58.669135Z",
          "shell.execute_reply": "2025-03-29T15:07:58.684734Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Explain why the Earth's temperature fluctuates.\n",
            "\n",
            "### Response:\n",
            "The Earth's temperature fluctuates due to changes in the amount of energy from the Sun that is received, released or reflected by the atmosphere and surface of the Earth. Changes in clouds, aerosols, oceans, and land also affect temperature. Additionally, increases in certain gasses, such as carbon dioxide, traps heat within the atmosphere and causes further warming.\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train Test Split"
      ],
      "metadata": {
        "id": "SHxyLt_Kvvpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = int(len(subset_dataset)*0.85)\n",
        "test_set = int(len(subset_dataset)*0.1)\n",
        "val_set = len(subset_dataset) - train_set - test_set\n",
        "\n",
        "train_data = subset_dataset[:train_set]\n",
        "test_data = subset_dataset[train_set:train_set+test_set]\n",
        "val_data = subset_dataset[train_set+test_set:]\n",
        "\n",
        "print(f'Train set size: {len(train_data)}')\n",
        "print(f'Test set size: {len(test_data)}')\n",
        "print(f'Validation set size: {len(val_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KXJhaLRupYe",
        "outputId": "66e653ec-d8c6-481f-d75b-d402313cc5ea",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:07:58.687036Z",
          "iopub.execute_input": "2025-03-29T15:07:58.687249Z",
          "iopub.status.idle": "2025-03-29T15:07:58.704742Z",
          "shell.execute_reply.started": "2025-03-29T15:07:58.687230Z",
          "shell.execute_reply": "2025-03-29T15:07:58.704105Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 4250\n",
            "Test set size: 500\n",
            "Validation set size: 250\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Organizing data into batches"
      ],
      "metadata": {
        "id": "nslTe1olwPH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`collate` function in pytorch\n",
        "- used to batch samples together into a single batch\n",
        "- allows custom preprocessing, when dealing with variable-length data\n",
        "\n",
        "1. `default_collate`\n",
        "- stackes tensor along the first dim\n",
        "- convert lists into tensors\n",
        "- leaves dic and other data structures untouched\n",
        "2. `collate_fn`\n",
        "- for creating custom collate function"
      ],
      "metadata": {
        "id": "Pc7zp_gzwc_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Collate function**\n",
        "\n",
        "1. format data\n",
        "2. tokenize\n",
        "3. adjust them to have same length using padding tokens\n",
        "4. create target token ids ---inputs shifted by `1`\n",
        "5. replace certain pad tokens\n",
        "  * why? --- do not contain useful info so excluded from loss computation\n",
        "  * `ignore_index`: placeholder value(`-100`)\n",
        "\n"
      ],
      "metadata": {
        "id": "ecrRbrB-yHm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Instruction dataset class"
      ],
      "metadata": {
        "id": "abKUIstV0Y4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer):\n",
        "    self.data = data\n",
        "    self.encode_texts = []\n",
        "    for entry in data:\n",
        "      input_with_instruction = format_input(entry)\n",
        "      response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "      full_text = input_with_instruction + response_text\n",
        "      self.encode_texts.append(\n",
        "          tokenizer.encode(full_text)\n",
        "      )\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "     return self.encode_texts[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "metadata": {
        "id": "P7RNq-GY0n1I",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:07:58.705473Z",
          "iopub.execute_input": "2025-03-29T15:07:58.705657Z",
          "iopub.status.idle": "2025-03-29T15:08:05.148991Z",
          "shell.execute_reply.started": "2025-03-29T15:07:58.705640Z",
          "shell.execute_reply": "2025-03-29T15:08:05.148289Z"
        }
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Custom Batch collate function"
      ],
      "metadata": {
        "id": "ejphi6jd3jCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id= 50256,\n",
        "    allowed_max_length=None,\n",
        "    ignore_index=-100,\n",
        "    device = 'cpu'):\n",
        "\n",
        "  batch_max_length = max(len(item) for item in batch)\n",
        "  inputs_lst, targets_lst = [], []\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    new_item += [pad_token_id]\n",
        "\n",
        "    padded = (new_item + [pad_token_id]* (batch_max_length - len(item)))\n",
        "    inputs= torch.tensor(padded[:-1])\n",
        "    targets= torch.tensor(padded[1:])\n",
        "\n",
        "    mask = targets == pad_token_id\n",
        "    indices = torch.nonzero(mask).squeeze() # returns the indices of True values\n",
        "    if indices.numel() >1:\n",
        "      targets[indices[1:]] = ignore_index\n",
        "\n",
        "    if allowed_max_length is not None:\n",
        "      inputs = inputs[:allowed_max_length]\n",
        "      targets = targets[:allowed_max_length]\n",
        "\n",
        "    inputs_lst.append(inputs)\n",
        "    targets_lst.append(targets)\n",
        "\n",
        "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "  targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "  return inputs_tensor, targets_tensor\n"
      ],
      "metadata": {
        "id": "GLoBwzGY3n4r",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:08:05.149760Z",
          "iopub.execute_input": "2025-03-29T15:08:05.150122Z",
          "iopub.status.idle": "2025-03-29T15:08:05.156375Z",
          "shell.execute_reply.started": "2025-03-29T15:08:05.150099Z",
          "shell.execute_reply": "2025-03-29T15:08:05.155463Z"
        }
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "input1 = [0,1,2,3,4]\n",
        "input2 = [5,6]\n",
        "input3 = [7,8,9]\n",
        "\n",
        "batch = [input1, input2, input3]\n",
        "\n",
        "input, target = custom_collate_fn(batch)\n",
        "print(input)\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ3MMTyu3n01",
        "outputId": "62c44404-4130-479f-deb1-1f127ced1f14",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:08:05.157408Z",
          "iopub.execute_input": "2025-03-29T15:08:05.157684Z",
          "iopub.status.idle": "2025-03-29T15:08:05.327268Z",
          "shell.execute_reply.started": "2025-03-29T15:08:05.157662Z",
          "shell.execute_reply": "2025-03-29T15:08:05.326266Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]])\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why `-100`?\n",
        "\n",
        "by default `ignore_index` in `cross_entropy()` is equal to `-100`.\n",
        "\n",
        "therefore, it ignores targets labeled with -100"
      ],
      "metadata": {
        "id": "drBrpXOe88DJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Masking out the instruction text- so that model focuses on generating accurate responses ratehr than memorizing instructions.\n",
        "\n",
        "will do later"
      ],
      "metadata": {
        "id": "49QTXQwO9ath"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creating dataloaders"
      ],
      "metadata": {
        "id": "PhrzJAon-Ttz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvQYI40d1fge",
        "outputId": "552af94b-292e-41dd-839f-435844aeadd8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:08:05.328301Z",
          "iopub.execute_input": "2025-03-29T15:08:05.328620Z",
          "iopub.status.idle": "2025-03-29T15:08:05.414204Z",
          "shell.execute_reply.started": "2025-03-29T15:08:05.328589Z",
          "shell.execute_reply": "2025-03-29T15:08:05.413295Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "# fix or pre-fill some argument in custom_collate_function\n",
        "from functools import partial\n",
        "custom_collate_fn = partial(\n",
        "    custom_collate_fn,\n",
        "    allowed_max_length=512,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "OU-tPPhH-hsh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:08:05.415118Z",
          "iopub.execute_input": "2025-03-29T15:08:05.415380Z",
          "iopub.status.idle": "2025-03-29T15:08:05.438734Z",
          "shell.execute_reply.started": "2025-03-29T15:08:05.415362Z",
          "shell.execute_reply": "2025-03-29T15:08:05.437893Z"
        }
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHlRyY2j_iz4",
        "outputId": "96ff3d69-2383-4470-b9c6-a5de75a5bacd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:08:05.439628Z",
          "iopub.execute_input": "2025-03-29T15:08:05.440111Z",
          "iopub.status.idle": "2025-03-29T15:08:12.097742Z",
          "shell.execute_reply.started": "2025-03-29T15:08:05.440080Z",
          "shell.execute_reply": "2025-03-29T15:08:12.096846Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "-CHPb3yj_cV7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:08:12.098839Z",
          "iopub.execute_input": "2025-03-29T15:08:12.099139Z",
          "iopub.status.idle": "2025-03-29T15:08:13.168155Z",
          "shell.execute_reply.started": "2025-03-29T15:08:12.099114Z",
          "shell.execute_reply": "2025-03-29T15:08:13.167400Z"
        }
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size =2\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ],
      "metadata": {
        "id": "a4gBllU-_HxU",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:08:13.168976Z",
          "iopub.execute_input": "2025-03-29T15:08:13.169214Z",
          "iopub.status.idle": "2025-03-29T15:08:13.482473Z",
          "shell.execute_reply.started": "2025-03-29T15:08:13.169194Z",
          "shell.execute_reply": "2025-03-29T15:08:13.481733Z"
        }
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_loader:\n",
        "  print(f'Inputs: {inputs.shape}, Targets: {targets.shape}')\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf7R4JAEAZnI",
        "outputId": "e2aecbe0-5712-4d4f-93fd-44c5d1749222",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:08:13.483270Z",
          "iopub.execute_input": "2025-03-29T15:08:13.483474Z",
          "iopub.status.idle": "2025-03-29T15:08:13.778128Z",
          "shell.execute_reply.started": "2025-03-29T15:08:13.483456Z",
          "shell.execute_reply": "2025-03-29T15:08:13.777154Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: torch.Size([2, 104]), Targets: torch.Size([2, 104])\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading Pretrained LLM"
      ],
      "metadata": {
        "id": "dSgqkHPNBPaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting scripts from github\n",
        "!git clone https://github.com/aashu-0/llm-from-scratch.git\n",
        "%cd llm-from-scratch/llm_book_notes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICcOslIPAi-d",
        "outputId": "d50a3097-ce22-41ca-cc4f-3dbdc9940370",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:08:13.779229Z",
          "iopub.execute_input": "2025-03-29T15:08:13.779536Z",
          "iopub.status.idle": "2025-03-29T15:08:14.991096Z",
          "shell.execute_reply.started": "2025-03-29T15:08:13.779506Z",
          "shell.execute_reply": "2025-03-29T15:08:14.990123Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm-from-scratch'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 136 (delta 74), reused 79 (delta 32), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (136/136), 183.51 KiB | 15.29 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n",
            "/content/llm-from-scratch/llm_book_notes\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/llm-from-scratch/llm_book_notes')"
      ],
      "metadata": {
        "id": "98QVze3pDO8i",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:08:14.992258Z",
          "iopub.execute_input": "2025-03-29T15:08:14.992626Z",
          "iopub.status.idle": "2025-03-29T15:08:14.997461Z",
          "shell.execute_reply.started": "2025-03-29T15:08:14.992590Z",
          "shell.execute_reply": "2025-03-29T15:08:14.996574Z"
        }
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "# get gpt_download.py from @rasbt github\n",
        "import urllib.request\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "urllib.request.urlretrieve(url, \"gpt_download.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YFmNihMDecq",
        "outputId": "6ed81598-af3d-4f11-9fbf-885dba486884",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:08:14.998538Z",
          "iopub.execute_input": "2025-03-29T15:08:14.998887Z",
          "iopub.status.idle": "2025-03-29T15:08:15.173521Z",
          "shell.execute_reply.started": "2025-03-29T15:08:14.998857Z",
          "shell.execute_reply": "2025-03-29T15:08:15.172645Z"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x79c16d93b990>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "from load_weights import load_weights_into_gpt\n",
        "from GPT import GPTModel\n",
        "from gpt_download import download_and_load_gpt2\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    'vocab_size': 50257,\n",
        "    'context_length': 1024,\n",
        "    'drop_rate': 0.2,\n",
        "    'qkv_bias': True\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    'gpt2 (124M)': {'emb_dim': 768 , 'n_layers': 12, 'n_heads': 12},\n",
        "    'gpt2-medium (355M)': {'emb_dim':1024 , 'n_layers':24, 'n_heads':16},\n",
        "    'gpt2-large (774M)': {'emb_dim': 1280 , 'n_layers': 36, 'n_heads':20},\n",
        "    'gpt2-xl (1558M)': {'emb_dim': 1600, 'n_layers':48, 'n_heads': 25}\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = 'gpt2-medium (355M)'\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(' ')[-1].lstrip('(').rstrip(')')\n",
        "\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size = model_size,\n",
        "    models_dir = 'gpt2'\n",
        ")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBFDzZPDDS2r",
        "outputId": "6cffb065-b131-4340-d703-6609bf31af7c",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:08:15.174538Z",
          "iopub.execute_input": "2025-03-29T15:08:15.174862Z",
          "iopub.status.idle": "2025-03-29T15:09:33.754656Z",
          "shell.execute_reply.started": "2025-03-29T15:08:15.174837Z",
          "shell.execute_reply": "2025-03-29T15:09:33.753838Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 104kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:02<00:00, 515kiB/s]\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 113kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [08:17<00:00, 2.85MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 12.4MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 927k/927k [00:01<00:00, 581kiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 370kiB/s]\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [
        "# assess our raw model(no fine tuning)\n",
        "torch.manual_seed(123)\n",
        "input_text = format_input(val_data[0])\n",
        "print(input_text)"
      ],
      "metadata": {
        "id": "9BSQP98CIHhW",
        "outputId": "de9d58a2-3290-46bd-98a7-f9661a91b9f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:09:33.757930Z",
          "iopub.execute_input": "2025-03-29T15:09:33.758485Z",
          "iopub.status.idle": "2025-03-29T15:09:33.782045Z",
          "shell.execute_reply.started": "2025-03-29T15:09:33.758458Z",
          "shell.execute_reply": "2025-03-29T15:09:33.781232Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a sentence that represents the content in the paragraph.\n",
            "\n",
            "### Input:\n",
            "A new law was introduced in 2020 outlining five safety measures all workplaces must follow to prevent the spread of Covid-19. This includes regularly sanitizing the premises, implementing social distancing measures, and introducing a screening and temperature checking procedure.\n"
          ]
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "# model's response\n",
        "from utilities import text_to_token_ids, token_ids_to_text\n",
        "from GPT import generate\n",
        "\n",
        "token_ids = generate(\n",
        "    model = model,\n",
        "    idx = text_to_token_ids(input_text, tokenizer),\n",
        "    max_new_tokens =35,\n",
        "    context_size= BASE_CONFIG['context_length'],\n",
        "    eos_id= 50256\n",
        ")\n",
        "generated_text =token_ids_to_text(token_ids, tokenizer)\n",
        "generated_text"
      ],
      "metadata": {
        "id": "ATSSV8UxJBI0",
        "outputId": "cb2a4d75-2af4-4338-e7ff-5f12737dd22d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:09:33.783184Z",
          "iopub.execute_input": "2025-03-29T15:09:33.783431Z",
          "iopub.status.idle": "2025-03-29T15:09:57.837736Z",
          "shell.execute_reply.started": "2025-03-29T15:09:33.783410Z",
          "shell.execute_reply": "2025-03-29T15:09:57.836913Z"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a sentence that represents the content in the paragraph.\\n\\n### Input:\\nA new law was introduced in 2020 outlining five safety measures all workplaces must follow to prevent the spread of Covid-19. This includes regularly sanitizing the premises, implementing social distancing measures, and introducing a screening and temperature checking procedure.\\n\\n### Output:\\n\\nThe law was passed and the new law is now in effect.\\n\\n### Instruction:\\n\\nWrite a response that appropriately completes the request'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": [
        "the generate function returns combined input and output text,separating the output"
      ],
      "metadata": {
        "id": "vj9r6qFsKq1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_text = generated_text[len(input_text):].strip()\n",
        "print(response_text)"
      ],
      "metadata": {
        "id": "lgJkvgJfJts6",
        "outputId": "b4381859-a464-4ad6-aa2c-e782d7d5e6c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:09:57.838822Z",
          "iopub.execute_input": "2025-03-29T15:09:57.839218Z",
          "iopub.status.idle": "2025-03-29T15:09:57.844489Z",
          "shell.execute_reply.started": "2025-03-29T15:09:57.839172Z",
          "shell.execute_reply": "2025-03-29T15:09:57.843786Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Output:\n",
            "\n",
            "The law was passed and the new law is now in effect.\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Write a response that appropriately completes the request\n"
          ]
        }
      ],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finetuning LLM on instruction dataset we loaded eariler"
      ],
      "metadata": {
        "id": "n7BEzxq9F4rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utilities import train_model_simple, calc_loss_loader"
      ],
      "metadata": {
        "id": "_NUYwkftGFRs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:09:57.845276Z",
          "iopub.execute_input": "2025-03-29T15:09:57.845575Z",
          "iopub.status.idle": "2025-03-29T15:09:57.863545Z",
          "shell.execute_reply.started": "2025-03-29T15:09:57.845542Z",
          "shell.execute_reply": "2025-03-29T15:09:57.862788Z"
        }
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "# initial loss for the train and val dataset\n",
        "model.to(device)\n",
        "torch.manual_seed(123)\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device)\n",
        "  val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(f'Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "id": "vxvxerzLGFL3",
        "outputId": "a5fe7c53-62c1-4453-a001-dcf14bdd2a81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:09:57.864589Z",
          "iopub.execute_input": "2025-03-29T15:09:57.864975Z",
          "iopub.status.idle": "2025-03-29T15:13:12.056420Z",
          "shell.execute_reply.started": "2025-03-29T15:09:57.864942Z",
          "shell.execute_reply": "2025-03-29T15:13:12.055341Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.2672, Val loss: 3.3169\n"
          ]
        }
      ],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "FWlK5BrIqXJl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training code\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr=0.00005,\n",
        "                              weight_decay=0.1)\n",
        "\n",
        "num_epochs = 2\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model=model,\n",
        "    train_dataloader= train_loader,\n",
        "    val_dataloader= val_loader,\n",
        "    optimizer= optimizer,\n",
        "    device=device,\n",
        "    num_epochs= num_epochs,\n",
        "    eval_freq= 5,\n",
        "    eval_iter= 5,\n",
        "    start_context= format_input(val_data[0]),\n",
        "    tokenizer= tokenizer\n",
        "    )\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "time_in_mins = (end_time - start_time)/60\n",
        "print(f'Time taken to train: {time_in_mins:.2f} minutes')"
      ],
      "metadata": {
        "id": "E0WKl65uK70h",
        "outputId": "f572dd9a-a262-4682-8981-0650b6d13f5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-29T15:13:12.057499Z",
          "iopub.execute_input": "2025-03-29T15:13:12.057853Z",
          "iopub.status.idle": "2025-03-29T15:15:15.595229Z",
          "shell.execute_reply.started": "2025-03-29T15:13:12.057828Z",
          "shell.execute_reply": "2025-03-29T15:15:15.593780Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Step: 000000\n",
            "Train Loss: 2.974 | Val Loss: 2.815\n",
            "Epoch: 1 | Step: 000005\n",
            "Train Loss: 2.010 | Val Loss: 2.103\n",
            "Epoch: 1 | Step: 000010\n",
            "Train Loss: 1.771 | Val Loss: 1.797\n",
            "Epoch: 1 | Step: 000015\n",
            "Train Loss: 1.459 | Val Loss: 1.722\n",
            "Epoch: 1 | Step: 000020\n",
            "Train Loss: 1.930 | Val Loss: 1.687\n",
            "Epoch: 1 | Step: 000025\n",
            "Train Loss: 1.695 | Val Loss: 1.643\n",
            "Epoch: 1 | Step: 000030\n",
            "Train Loss: 1.728 | Val Loss: 1.628\n",
            "Epoch: 1 | Step: 000035\n",
            "Train Loss: 1.680 | Val Loss: 1.622\n",
            "Epoch: 1 | Step: 000040\n",
            "Train Loss: 1.562 | Val Loss: 1.598\n",
            "Epoch: 1 | Step: 000045\n",
            "Train Loss: 1.516 | Val Loss: 1.592\n",
            "Epoch: 1 | Step: 000050\n",
            "Train Loss: 1.439 | Val Loss: 1.586\n",
            "Epoch: 1 | Step: 000055\n",
            "Train Loss: 1.609 | Val Loss: 1.584\n",
            "Epoch: 1 | Step: 000060\n",
            "Train Loss: 1.521 | Val Loss: 1.588\n",
            "Epoch: 1 | Step: 000065\n",
            "Train Loss: 1.565 | Val Loss: 1.588\n",
            "Epoch: 1 | Step: 000070\n",
            "Train Loss: 1.545 | Val Loss: 1.589\n",
            "Epoch: 1 | Step: 000075\n",
            "Train Loss: 1.665 | Val Loss: 1.598\n",
            "Epoch: 1 | Step: 000080\n",
            "Train Loss: 1.333 | Val Loss: 1.583\n",
            "Epoch: 1 | Step: 000085\n",
            "Train Loss: 1.405 | Val Loss: 1.582\n",
            "Epoch: 1 | Step: 000090\n",
            "Train Loss: 1.364 | Val Loss: 1.587\n",
            "Epoch: 1 | Step: 000095\n",
            "Train Loss: 1.610 | Val Loss: 1.611\n",
            "Epoch: 1 | Step: 000100\n",
            "Train Loss: 1.642 | Val Loss: 1.589\n",
            "Epoch: 1 | Step: 000105\n",
            "Train Loss: 1.459 | Val Loss: 1.583\n",
            "Epoch: 1 | Step: 000110\n",
            "Train Loss: 1.517 | Val Loss: 1.583\n",
            "Epoch: 1 | Step: 000115\n",
            "Train Loss: 1.583 | Val Loss: 1.583\n",
            "Epoch: 1 | Step: 000120\n",
            "Train Loss: 1.685 | Val Loss: 1.582\n",
            "Epoch: 1 | Step: 000125\n",
            "Train Loss: 1.611 | Val Loss: 1.585\n",
            "Epoch: 1 | Step: 000130\n",
            "Train Loss: 1.669 | Val Loss: 1.583\n",
            "Epoch: 1 | Step: 000135\n",
            "Train Loss: 1.745 | Val Loss: 1.580\n",
            "Epoch: 1 | Step: 000140\n",
            "Train Loss: 1.527 | Val Loss: 1.584\n",
            "Epoch: 1 | Step: 000145\n",
            "Train Loss: 1.551 | Val Loss: 1.584\n",
            "Epoch: 1 | Step: 000150\n",
            "Train Loss: 1.238 | Val Loss: 1.573\n",
            "Epoch: 1 | Step: 000155\n",
            "Train Loss: 1.357 | Val Loss: 1.565\n",
            "Epoch: 1 | Step: 000160\n",
            "Train Loss: 1.547 | Val Loss: 1.565\n",
            "Epoch: 1 | Step: 000165\n",
            "Train Loss: 1.501 | Val Loss: 1.568\n",
            "Epoch: 1 | Step: 000170\n",
            "Train Loss: 1.376 | Val Loss: 1.567\n",
            "Epoch: 1 | Step: 000175\n",
            "Train Loss: 1.576 | Val Loss: 1.561\n",
            "Epoch: 1 | Step: 000180\n",
            "Train Loss: 1.579 | Val Loss: 1.565\n",
            "Epoch: 1 | Step: 000185\n",
            "Train Loss: 1.523 | Val Loss: 1.571\n",
            "Epoch: 1 | Step: 000190\n",
            "Train Loss: 1.549 | Val Loss: 1.575\n",
            "Epoch: 1 | Step: 000195\n",
            "Train Loss: 1.503 | Val Loss: 1.576\n",
            "Epoch: 1 | Step: 000200\n",
            "Train Loss: 1.661 | Val Loss: 1.575\n",
            "Epoch: 1 | Step: 000205\n",
            "Train Loss: 1.569 | Val Loss: 1.570\n",
            "Epoch: 1 | Step: 000210\n",
            "Train Loss: 1.470 | Val Loss: 1.565\n",
            "Epoch: 1 | Step: 000215\n",
            "Train Loss: 1.440 | Val Loss: 1.559\n",
            "Epoch: 1 | Step: 000220\n",
            "Train Loss: 1.444 | Val Loss: 1.552\n",
            "Epoch: 1 | Step: 000225\n",
            "Train Loss: 1.743 | Val Loss: 1.556\n",
            "Epoch: 1 | Step: 000230\n",
            "Train Loss: 1.386 | Val Loss: 1.556\n",
            "Epoch: 1 | Step: 000235\n",
            "Train Loss: 1.565 | Val Loss: 1.556\n",
            "Epoch: 1 | Step: 000240\n",
            "Train Loss: 1.643 | Val Loss: 1.554\n",
            "Epoch: 1 | Step: 000245\n",
            "Train Loss: 1.307 | Val Loss: 1.551\n",
            "Epoch: 1 | Step: 000250\n",
            "Train Loss: 1.439 | Val Loss: 1.554\n",
            "Epoch: 1 | Step: 000255\n",
            "Train Loss: 1.484 | Val Loss: 1.565\n",
            "Epoch: 1 | Step: 000260\n",
            "Train Loss: 1.290 | Val Loss: 1.577\n",
            "Epoch: 1 | Step: 000265\n",
            "Train Loss: 1.277 | Val Loss: 1.558\n",
            "Epoch: 1 | Step: 000270\n",
            "Train Loss: 1.155 | Val Loss: 1.551\n",
            "Epoch: 1 | Step: 000275\n",
            "Train Loss: 1.396 | Val Loss: 1.548\n",
            "Epoch: 1 | Step: 000280\n",
            "Train Loss: 1.611 | Val Loss: 1.545\n",
            "Epoch: 1 | Step: 000285\n",
            "Train Loss: 1.567 | Val Loss: 1.541\n",
            "Epoch: 1 | Step: 000290\n",
            "Train Loss: 1.378 | Val Loss: 1.542\n",
            "Epoch: 1 | Step: 000295\n",
            "Train Loss: 1.467 | Val Loss: 1.545\n",
            "Epoch: 1 | Step: 000300\n",
            "Train Loss: 1.493 | Val Loss: 1.539\n",
            "Epoch: 1 | Step: 000305\n",
            "Train Loss: 1.686 | Val Loss: 1.540\n",
            "Epoch: 1 | Step: 000310\n",
            "Train Loss: 1.446 | Val Loss: 1.541\n",
            "Epoch: 1 | Step: 000315\n",
            "Train Loss: 1.774 | Val Loss: 1.539\n",
            "Epoch: 1 | Step: 000320\n",
            "Train Loss: 1.109 | Val Loss: 1.536\n",
            "Epoch: 1 | Step: 000325\n",
            "Train Loss: 1.565 | Val Loss: 1.539\n",
            "Epoch: 1 | Step: 000330\n",
            "Train Loss: 1.301 | Val Loss: 1.537\n",
            "Epoch: 1 | Step: 000335\n",
            "Train Loss: 1.483 | Val Loss: 1.530\n",
            "Epoch: 1 | Step: 000340\n",
            "Train Loss: 1.688 | Val Loss: 1.523\n",
            "Epoch: 1 | Step: 000345\n",
            "Train Loss: 1.318 | Val Loss: 1.525\n",
            "Epoch: 1 | Step: 000350\n",
            "Train Loss: 1.384 | Val Loss: 1.530\n",
            "Epoch: 1 | Step: 000355\n",
            "Train Loss: 1.503 | Val Loss: 1.536\n",
            "Epoch: 1 | Step: 000360\n",
            "Train Loss: 1.485 | Val Loss: 1.539\n",
            "Epoch: 1 | Step: 000365\n",
            "Train Loss: 1.343 | Val Loss: 1.537\n",
            "Epoch: 1 | Step: 000370\n",
            "Train Loss: 1.233 | Val Loss: 1.536\n",
            "Epoch: 1 | Step: 000375\n",
            "Train Loss: 1.466 | Val Loss: 1.532\n",
            "Epoch: 1 | Step: 000380\n",
            "Train Loss: 1.331 | Val Loss: 1.529\n",
            "Epoch: 1 | Step: 000385\n",
            "Train Loss: 1.510 | Val Loss: 1.531\n",
            "Epoch: 1 | Step: 000390\n",
            "Train Loss: 1.559 | Val Loss: 1.539\n",
            "Epoch: 1 | Step: 000395\n",
            "Train Loss: 1.183 | Val Loss: 1.541\n",
            "Epoch: 1 | Step: 000400\n",
            "Train Loss: 1.373 | Val Loss: 1.535\n",
            "Epoch: 1 | Step: 000405\n",
            "Train Loss: 1.222 | Val Loss: 1.532\n",
            "Epoch: 1 | Step: 000410\n",
            "Train Loss: 1.638 | Val Loss: 1.537\n",
            "Epoch: 1 | Step: 000415\n",
            "Train Loss: 1.393 | Val Loss: 1.542\n",
            "Epoch: 1 | Step: 000420\n",
            "Train Loss: 1.434 | Val Loss: 1.541\n",
            "Epoch: 1 | Step: 000425\n",
            "Train Loss: 1.456 | Val Loss: 1.545\n",
            "Epoch: 1 | Step: 000430\n",
            "Train Loss: 1.530 | Val Loss: 1.537\n",
            "Epoch: 1 | Step: 000435\n",
            "Train Loss: 1.537 | Val Loss: 1.537\n",
            "Epoch: 1 | Step: 000440\n",
            "Train Loss: 1.403 | Val Loss: 1.540\n",
            "Epoch: 1 | Step: 000445\n",
            "Train Loss: 1.440 | Val Loss: 1.544\n",
            "Epoch: 1 | Step: 000450\n",
            "Train Loss: 1.256 | Val Loss: 1.551\n",
            "Epoch: 1 | Step: 000455\n",
            "Train Loss: 1.386 | Val Loss: 1.544\n",
            "Epoch: 1 | Step: 000460\n",
            "Train Loss: 1.430 | Val Loss: 1.547\n",
            "Epoch: 1 | Step: 000465\n",
            "Train Loss: 1.390 | Val Loss: 1.549\n",
            "Epoch: 1 | Step: 000470\n",
            "Train Loss: 1.289 | Val Loss: 1.549\n",
            "Epoch: 1 | Step: 000475\n",
            "Train Loss: 1.420 | Val Loss: 1.541\n",
            "Epoch: 1 | Step: 000480\n",
            "Train Loss: 1.489 | Val Loss: 1.541\n",
            "Epoch: 1 | Step: 000485\n",
            "Train Loss: 1.366 | Val Loss: 1.543\n",
            "Epoch: 1 | Step: 000490\n",
            "Train Loss: 1.243 | Val Loss: 1.547\n",
            "Epoch: 1 | Step: 000495\n",
            "Train Loss: 1.480 | Val Loss: 1.551\n",
            "Epoch: 1 | Step: 000500\n",
            "Train Loss: 1.384 | Val Loss: 1.547\n",
            "Epoch: 1 | Step: 000505\n",
            "Train Loss: 1.468 | Val Loss: 1.529\n",
            "Epoch: 1 | Step: 000510\n",
            "Train Loss: 1.454 | Val Loss: 1.519\n",
            "Epoch: 1 | Step: 000515\n",
            "Train Loss: 1.196 | Val Loss: 1.521\n",
            "Epoch: 1 | Step: 000520\n",
            "Train Loss: 1.641 | Val Loss: 1.518\n",
            "Epoch: 1 | Step: 000525\n",
            "Train Loss: 1.300 | Val Loss: 1.511\n",
            "Epoch: 1 | Step: 000530\n",
            "Train Loss: 1.479 | Val Loss: 1.509\n",
            "Epoch: 1 | Step: 000535\n",
            "Train Loss: 1.530 | Val Loss: 1.504\n",
            "Epoch: 1 | Step: 000540\n",
            "Train Loss: 1.447 | Val Loss: 1.502\n",
            "Epoch: 1 | Step: 000545\n",
            "Train Loss: 1.577 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 000550\n",
            "Train Loss: 1.473 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 000555\n",
            "Train Loss: 1.393 | Val Loss: 1.497\n",
            "Epoch: 1 | Step: 000560\n",
            "Train Loss: 0.984 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 000565\n",
            "Train Loss: 1.373 | Val Loss: 1.505\n",
            "Epoch: 1 | Step: 000570\n",
            "Train Loss: 1.414 | Val Loss: 1.509\n",
            "Epoch: 1 | Step: 000575\n",
            "Train Loss: 1.446 | Val Loss: 1.518\n",
            "Epoch: 1 | Step: 000580\n",
            "Train Loss: 1.445 | Val Loss: 1.518\n",
            "Epoch: 1 | Step: 000585\n",
            "Train Loss: 1.288 | Val Loss: 1.519\n",
            "Epoch: 1 | Step: 000590\n",
            "Train Loss: 1.283 | Val Loss: 1.516\n",
            "Epoch: 1 | Step: 000595\n",
            "Train Loss: 1.362 | Val Loss: 1.508\n",
            "Epoch: 1 | Step: 000600\n",
            "Train Loss: 1.351 | Val Loss: 1.505\n",
            "Epoch: 1 | Step: 000605\n",
            "Train Loss: 1.495 | Val Loss: 1.506\n",
            "Epoch: 1 | Step: 000610\n",
            "Train Loss: 1.460 | Val Loss: 1.509\n",
            "Epoch: 1 | Step: 000615\n",
            "Train Loss: 1.318 | Val Loss: 1.509\n",
            "Epoch: 1 | Step: 000620\n",
            "Train Loss: 1.410 | Val Loss: 1.509\n",
            "Epoch: 1 | Step: 000625\n",
            "Train Loss: 1.341 | Val Loss: 1.509\n",
            "Epoch: 1 | Step: 000630\n",
            "Train Loss: 1.412 | Val Loss: 1.509\n",
            "Epoch: 1 | Step: 000635\n",
            "Train Loss: 1.327 | Val Loss: 1.508\n",
            "Epoch: 1 | Step: 000640\n",
            "Train Loss: 1.234 | Val Loss: 1.509\n",
            "Epoch: 1 | Step: 000645\n",
            "Train Loss: 1.217 | Val Loss: 1.511\n",
            "Epoch: 1 | Step: 000650\n",
            "Train Loss: 1.492 | Val Loss: 1.506\n",
            "Epoch: 1 | Step: 000655\n",
            "Train Loss: 1.377 | Val Loss: 1.504\n",
            "Epoch: 1 | Step: 000660\n",
            "Train Loss: 1.283 | Val Loss: 1.504\n",
            "Epoch: 1 | Step: 000665\n",
            "Train Loss: 1.588 | Val Loss: 1.503\n",
            "Epoch: 1 | Step: 000670\n",
            "Train Loss: 1.519 | Val Loss: 1.501\n",
            "Epoch: 1 | Step: 000675\n",
            "Train Loss: 1.331 | Val Loss: 1.497\n",
            "Epoch: 1 | Step: 000680\n",
            "Train Loss: 1.377 | Val Loss: 1.494\n",
            "Epoch: 1 | Step: 000685\n",
            "Train Loss: 1.383 | Val Loss: 1.494\n",
            "Epoch: 1 | Step: 000690\n",
            "Train Loss: 1.518 | Val Loss: 1.494\n",
            "Epoch: 1 | Step: 000695\n",
            "Train Loss: 1.451 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 000700\n",
            "Train Loss: 1.565 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 000705\n",
            "Train Loss: 1.386 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 000710\n",
            "Train Loss: 1.363 | Val Loss: 1.494\n",
            "Epoch: 1 | Step: 000715\n",
            "Train Loss: 1.288 | Val Loss: 1.488\n",
            "Epoch: 1 | Step: 000720\n",
            "Train Loss: 1.679 | Val Loss: 1.482\n",
            "Epoch: 1 | Step: 000725\n",
            "Train Loss: 1.387 | Val Loss: 1.478\n",
            "Epoch: 1 | Step: 000730\n",
            "Train Loss: 1.285 | Val Loss: 1.481\n",
            "Epoch: 1 | Step: 000735\n",
            "Train Loss: 1.351 | Val Loss: 1.484\n",
            "Epoch: 1 | Step: 000740\n",
            "Train Loss: 1.380 | Val Loss: 1.494\n",
            "Epoch: 1 | Step: 000745\n",
            "Train Loss: 1.477 | Val Loss: 1.502\n",
            "Epoch: 1 | Step: 000750\n",
            "Train Loss: 1.327 | Val Loss: 1.506\n",
            "Epoch: 1 | Step: 000755\n",
            "Train Loss: 1.527 | Val Loss: 1.508\n",
            "Epoch: 1 | Step: 000760\n",
            "Train Loss: 1.047 | Val Loss: 1.510\n",
            "Epoch: 1 | Step: 000765\n",
            "Train Loss: 1.342 | Val Loss: 1.508\n",
            "Epoch: 1 | Step: 000770\n",
            "Train Loss: 1.203 | Val Loss: 1.506\n",
            "Epoch: 1 | Step: 000775\n",
            "Train Loss: 1.326 | Val Loss: 1.506\n",
            "Epoch: 1 | Step: 000780\n",
            "Train Loss: 1.150 | Val Loss: 1.502\n",
            "Epoch: 1 | Step: 000785\n",
            "Train Loss: 1.353 | Val Loss: 1.504\n",
            "Epoch: 1 | Step: 000790\n",
            "Train Loss: 1.532 | Val Loss: 1.504\n",
            "Epoch: 1 | Step: 000795\n",
            "Train Loss: 1.288 | Val Loss: 1.504\n",
            "Epoch: 1 | Step: 000800\n",
            "Train Loss: 1.665 | Val Loss: 1.502\n",
            "Epoch: 1 | Step: 000805\n",
            "Train Loss: 1.261 | Val Loss: 1.502\n",
            "Epoch: 1 | Step: 000810\n",
            "Train Loss: 1.224 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 000815\n",
            "Train Loss: 1.450 | Val Loss: 1.497\n",
            "Epoch: 1 | Step: 000820\n",
            "Train Loss: 1.635 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 000825\n",
            "Train Loss: 1.280 | Val Loss: 1.496\n",
            "Epoch: 1 | Step: 000830\n",
            "Train Loss: 1.320 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 000835\n",
            "Train Loss: 1.488 | Val Loss: 1.501\n",
            "Epoch: 1 | Step: 000840\n",
            "Train Loss: 1.177 | Val Loss: 1.500\n",
            "Epoch: 1 | Step: 000845\n",
            "Train Loss: 1.383 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 000850\n",
            "Train Loss: 1.149 | Val Loss: 1.500\n",
            "Epoch: 1 | Step: 000855\n",
            "Train Loss: 1.492 | Val Loss: 1.521\n",
            "Epoch: 1 | Step: 000860\n",
            "Train Loss: 1.423 | Val Loss: 1.535\n",
            "Epoch: 1 | Step: 000865\n",
            "Train Loss: 1.586 | Val Loss: 1.537\n",
            "Epoch: 1 | Step: 000870\n",
            "Train Loss: 1.006 | Val Loss: 1.535\n",
            "Epoch: 1 | Step: 000875\n",
            "Train Loss: 1.154 | Val Loss: 1.538\n",
            "Epoch: 1 | Step: 000880\n",
            "Train Loss: 1.215 | Val Loss: 1.538\n",
            "Epoch: 1 | Step: 000885\n",
            "Train Loss: 1.315 | Val Loss: 1.534\n",
            "Epoch: 1 | Step: 000890\n",
            "Train Loss: 1.398 | Val Loss: 1.526\n",
            "Epoch: 1 | Step: 000895\n",
            "Train Loss: 1.536 | Val Loss: 1.520\n",
            "Epoch: 1 | Step: 000900\n",
            "Train Loss: 1.331 | Val Loss: 1.519\n",
            "Epoch: 1 | Step: 000905\n",
            "Train Loss: 1.361 | Val Loss: 1.524\n",
            "Epoch: 1 | Step: 000910\n",
            "Train Loss: 1.333 | Val Loss: 1.528\n",
            "Epoch: 1 | Step: 000915\n",
            "Train Loss: 1.283 | Val Loss: 1.531\n",
            "Epoch: 1 | Step: 000920\n",
            "Train Loss: 1.375 | Val Loss: 1.529\n",
            "Epoch: 1 | Step: 000925\n",
            "Train Loss: 1.259 | Val Loss: 1.521\n",
            "Epoch: 1 | Step: 000930\n",
            "Train Loss: 1.253 | Val Loss: 1.515\n",
            "Epoch: 1 | Step: 000935\n",
            "Train Loss: 1.211 | Val Loss: 1.512\n",
            "Epoch: 1 | Step: 000940\n",
            "Train Loss: 1.218 | Val Loss: 1.513\n",
            "Epoch: 1 | Step: 000945\n",
            "Train Loss: 1.154 | Val Loss: 1.512\n",
            "Epoch: 1 | Step: 000950\n",
            "Train Loss: 1.487 | Val Loss: 1.509\n",
            "Epoch: 1 | Step: 000955\n",
            "Train Loss: 1.348 | Val Loss: 1.508\n",
            "Epoch: 1 | Step: 000960\n",
            "Train Loss: 1.174 | Val Loss: 1.510\n",
            "Epoch: 1 | Step: 000965\n",
            "Train Loss: 1.384 | Val Loss: 1.513\n",
            "Epoch: 1 | Step: 000970\n",
            "Train Loss: 1.708 | Val Loss: 1.513\n",
            "Epoch: 1 | Step: 000975\n",
            "Train Loss: 1.568 | Val Loss: 1.511\n",
            "Epoch: 1 | Step: 000980\n",
            "Train Loss: 1.259 | Val Loss: 1.514\n",
            "Epoch: 1 | Step: 000985\n",
            "Train Loss: 1.079 | Val Loss: 1.511\n",
            "Epoch: 1 | Step: 000990\n",
            "Train Loss: 1.537 | Val Loss: 1.508\n",
            "Epoch: 1 | Step: 000995\n",
            "Train Loss: 1.359 | Val Loss: 1.506\n",
            "Epoch: 1 | Step: 001000\n",
            "Train Loss: 1.169 | Val Loss: 1.505\n",
            "Epoch: 1 | Step: 001005\n",
            "Train Loss: 1.604 | Val Loss: 1.503\n",
            "Epoch: 1 | Step: 001010\n",
            "Train Loss: 1.453 | Val Loss: 1.502\n",
            "Epoch: 1 | Step: 001015\n",
            "Train Loss: 1.327 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 001020\n",
            "Train Loss: 1.503 | Val Loss: 1.493\n",
            "Epoch: 1 | Step: 001025\n",
            "Train Loss: 1.329 | Val Loss: 1.492\n",
            "Epoch: 1 | Step: 001030\n",
            "Train Loss: 1.393 | Val Loss: 1.495\n",
            "Epoch: 1 | Step: 001035\n",
            "Train Loss: 1.131 | Val Loss: 1.501\n",
            "Epoch: 1 | Step: 001040\n",
            "Train Loss: 1.377 | Val Loss: 1.506\n",
            "Epoch: 1 | Step: 001045\n",
            "Train Loss: 1.387 | Val Loss: 1.511\n",
            "Epoch: 1 | Step: 001050\n",
            "Train Loss: 1.438 | Val Loss: 1.514\n",
            "Epoch: 1 | Step: 001055\n",
            "Train Loss: 1.240 | Val Loss: 1.507\n",
            "Epoch: 1 | Step: 001060\n",
            "Train Loss: 1.434 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 001065\n",
            "Train Loss: 1.226 | Val Loss: 1.494\n",
            "Epoch: 1 | Step: 001070\n",
            "Train Loss: 1.380 | Val Loss: 1.490\n",
            "Epoch: 1 | Step: 001075\n",
            "Train Loss: 1.195 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 001080\n",
            "Train Loss: 1.570 | Val Loss: 1.485\n",
            "Epoch: 1 | Step: 001085\n",
            "Train Loss: 1.142 | Val Loss: 1.482\n",
            "Epoch: 1 | Step: 001090\n",
            "Train Loss: 1.423 | Val Loss: 1.484\n",
            "Epoch: 1 | Step: 001095\n",
            "Train Loss: 1.532 | Val Loss: 1.487\n",
            "Epoch: 1 | Step: 001100\n",
            "Train Loss: 1.307 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 001105\n",
            "Train Loss: 1.329 | Val Loss: 1.491\n",
            "Epoch: 1 | Step: 001110\n",
            "Train Loss: 1.345 | Val Loss: 1.493\n",
            "Epoch: 1 | Step: 001115\n",
            "Train Loss: 1.269 | Val Loss: 1.495\n",
            "Epoch: 1 | Step: 001120\n",
            "Train Loss: 1.037 | Val Loss: 1.494\n",
            "Epoch: 1 | Step: 001125\n",
            "Train Loss: 1.576 | Val Loss: 1.490\n",
            "Epoch: 1 | Step: 001130\n",
            "Train Loss: 1.236 | Val Loss: 1.490\n",
            "Epoch: 1 | Step: 001135\n",
            "Train Loss: 1.477 | Val Loss: 1.490\n",
            "Epoch: 1 | Step: 001140\n",
            "Train Loss: 1.128 | Val Loss: 1.486\n",
            "Epoch: 1 | Step: 001145\n",
            "Train Loss: 1.385 | Val Loss: 1.485\n",
            "Epoch: 1 | Step: 001150\n",
            "Train Loss: 1.391 | Val Loss: 1.488\n",
            "Epoch: 1 | Step: 001155\n",
            "Train Loss: 1.214 | Val Loss: 1.492\n",
            "Epoch: 1 | Step: 001160\n",
            "Train Loss: 1.311 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 001165\n",
            "Train Loss: 1.194 | Val Loss: 1.505\n",
            "Epoch: 1 | Step: 001170\n",
            "Train Loss: 1.142 | Val Loss: 1.503\n",
            "Epoch: 1 | Step: 001175\n",
            "Train Loss: 0.981 | Val Loss: 1.501\n",
            "Epoch: 1 | Step: 001180\n",
            "Train Loss: 1.194 | Val Loss: 1.495\n",
            "Epoch: 1 | Step: 001185\n",
            "Train Loss: 1.200 | Val Loss: 1.483\n",
            "Epoch: 1 | Step: 001190\n",
            "Train Loss: 1.376 | Val Loss: 1.481\n",
            "Epoch: 1 | Step: 001195\n",
            "Train Loss: 1.234 | Val Loss: 1.485\n",
            "Epoch: 1 | Step: 001200\n",
            "Train Loss: 1.182 | Val Loss: 1.491\n",
            "Epoch: 1 | Step: 001205\n",
            "Train Loss: 1.142 | Val Loss: 1.495\n",
            "Epoch: 1 | Step: 001210\n",
            "Train Loss: 1.225 | Val Loss: 1.496\n",
            "Epoch: 1 | Step: 001215\n",
            "Train Loss: 1.239 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 001220\n",
            "Train Loss: 1.378 | Val Loss: 1.501\n",
            "Epoch: 1 | Step: 001225\n",
            "Train Loss: 1.336 | Val Loss: 1.503\n",
            "Epoch: 1 | Step: 001230\n",
            "Train Loss: 1.253 | Val Loss: 1.502\n",
            "Epoch: 1 | Step: 001235\n",
            "Train Loss: 1.259 | Val Loss: 1.500\n",
            "Epoch: 1 | Step: 001240\n",
            "Train Loss: 1.322 | Val Loss: 1.500\n",
            "Epoch: 1 | Step: 001245\n",
            "Train Loss: 1.325 | Val Loss: 1.501\n",
            "Epoch: 1 | Step: 001250\n",
            "Train Loss: 1.603 | Val Loss: 1.501\n",
            "Epoch: 1 | Step: 001255\n",
            "Train Loss: 1.420 | Val Loss: 1.500\n",
            "Epoch: 1 | Step: 001260\n",
            "Train Loss: 1.400 | Val Loss: 1.500\n",
            "Epoch: 1 | Step: 001265\n",
            "Train Loss: 0.969 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 001270\n",
            "Train Loss: 1.154 | Val Loss: 1.495\n",
            "Epoch: 1 | Step: 001275\n",
            "Train Loss: 1.330 | Val Loss: 1.493\n",
            "Epoch: 1 | Step: 001280\n",
            "Train Loss: 1.605 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 001285\n",
            "Train Loss: 1.158 | Val Loss: 1.487\n",
            "Epoch: 1 | Step: 001290\n",
            "Train Loss: 1.440 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 001295\n",
            "Train Loss: 1.173 | Val Loss: 1.492\n",
            "Epoch: 1 | Step: 001300\n",
            "Train Loss: 1.229 | Val Loss: 1.487\n",
            "Epoch: 1 | Step: 001305\n",
            "Train Loss: 1.320 | Val Loss: 1.480\n",
            "Epoch: 1 | Step: 001310\n",
            "Train Loss: 1.273 | Val Loss: 1.479\n",
            "Epoch: 1 | Step: 001315\n",
            "Train Loss: 1.282 | Val Loss: 1.481\n",
            "Epoch: 1 | Step: 001320\n",
            "Train Loss: 1.079 | Val Loss: 1.484\n",
            "Epoch: 1 | Step: 001325\n",
            "Train Loss: 1.562 | Val Loss: 1.487\n",
            "Epoch: 1 | Step: 001330\n",
            "Train Loss: 1.065 | Val Loss: 1.485\n",
            "Epoch: 1 | Step: 001335\n",
            "Train Loss: 1.448 | Val Loss: 1.490\n",
            "Epoch: 1 | Step: 001340\n",
            "Train Loss: 1.365 | Val Loss: 1.494\n",
            "Epoch: 1 | Step: 001345\n",
            "Train Loss: 1.478 | Val Loss: 1.493\n",
            "Epoch: 1 | Step: 001350\n",
            "Train Loss: 1.215 | Val Loss: 1.488\n",
            "Epoch: 1 | Step: 001355\n",
            "Train Loss: 1.213 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 001360\n",
            "Train Loss: 1.356 | Val Loss: 1.493\n",
            "Epoch: 1 | Step: 001365\n",
            "Train Loss: 1.107 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 001370\n",
            "Train Loss: 1.060 | Val Loss: 1.501\n",
            "Epoch: 1 | Step: 001375\n",
            "Train Loss: 1.584 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 001380\n",
            "Train Loss: 1.205 | Val Loss: 1.492\n",
            "Epoch: 1 | Step: 001385\n",
            "Train Loss: 1.148 | Val Loss: 1.485\n",
            "Epoch: 1 | Step: 001390\n",
            "Train Loss: 1.090 | Val Loss: 1.486\n",
            "Epoch: 1 | Step: 001395\n",
            "Train Loss: 1.416 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 001400\n",
            "Train Loss: 1.173 | Val Loss: 1.496\n",
            "Epoch: 1 | Step: 001405\n",
            "Train Loss: 1.182 | Val Loss: 1.506\n",
            "Epoch: 1 | Step: 001410\n",
            "Train Loss: 1.208 | Val Loss: 1.513\n",
            "Epoch: 1 | Step: 001415\n",
            "Train Loss: 1.157 | Val Loss: 1.512\n",
            "Epoch: 1 | Step: 001420\n",
            "Train Loss: 1.428 | Val Loss: 1.507\n",
            "Epoch: 1 | Step: 001425\n",
            "Train Loss: 1.122 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 001430\n",
            "Train Loss: 1.346 | Val Loss: 1.492\n",
            "Epoch: 1 | Step: 001435\n",
            "Train Loss: 1.317 | Val Loss: 1.490\n",
            "Epoch: 1 | Step: 001440\n",
            "Train Loss: 1.057 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 001445\n",
            "Train Loss: 1.321 | Val Loss: 1.486\n",
            "Epoch: 1 | Step: 001450\n",
            "Train Loss: 1.176 | Val Loss: 1.485\n",
            "Epoch: 1 | Step: 001455\n",
            "Train Loss: 1.271 | Val Loss: 1.484\n",
            "Epoch: 1 | Step: 001460\n",
            "Train Loss: 1.062 | Val Loss: 1.482\n",
            "Epoch: 1 | Step: 001465\n",
            "Train Loss: 1.155 | Val Loss: 1.480\n",
            "Epoch: 1 | Step: 001470\n",
            "Train Loss: 1.098 | Val Loss: 1.481\n",
            "Epoch: 1 | Step: 001475\n",
            "Train Loss: 1.286 | Val Loss: 1.479\n",
            "Epoch: 1 | Step: 001480\n",
            "Train Loss: 1.291 | Val Loss: 1.477\n",
            "Epoch: 1 | Step: 001485\n",
            "Train Loss: 1.166 | Val Loss: 1.480\n",
            "Epoch: 1 | Step: 001490\n",
            "Train Loss: 0.964 | Val Loss: 1.485\n",
            "Epoch: 1 | Step: 001495\n",
            "Train Loss: 1.193 | Val Loss: 1.491\n",
            "Epoch: 1 | Step: 001500\n",
            "Train Loss: 1.337 | Val Loss: 1.494\n",
            "Epoch: 1 | Step: 001505\n",
            "Train Loss: 1.168 | Val Loss: 1.495\n",
            "Epoch: 1 | Step: 001510\n",
            "Train Loss: 1.336 | Val Loss: 1.493\n",
            "Epoch: 1 | Step: 001515\n",
            "Train Loss: 1.312 | Val Loss: 1.491\n",
            "Epoch: 1 | Step: 001520\n",
            "Train Loss: 1.223 | Val Loss: 1.492\n",
            "Epoch: 1 | Step: 001525\n",
            "Train Loss: 1.227 | Val Loss: 1.492\n",
            "Epoch: 1 | Step: 001530\n",
            "Train Loss: 1.250 | Val Loss: 1.491\n",
            "Epoch: 1 | Step: 001535\n",
            "Train Loss: 1.267 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 001540\n",
            "Train Loss: 1.040 | Val Loss: 1.491\n",
            "Epoch: 1 | Step: 001545\n",
            "Train Loss: 1.219 | Val Loss: 1.494\n",
            "Epoch: 1 | Step: 001550\n",
            "Train Loss: 0.995 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 001555\n",
            "Train Loss: 1.374 | Val Loss: 1.502\n",
            "Epoch: 1 | Step: 001560\n",
            "Train Loss: 1.068 | Val Loss: 1.507\n",
            "Epoch: 1 | Step: 001565\n",
            "Train Loss: 1.145 | Val Loss: 1.505\n",
            "Epoch: 1 | Step: 001570\n",
            "Train Loss: 1.209 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 001575\n",
            "Train Loss: 1.182 | Val Loss: 1.493\n",
            "Epoch: 1 | Step: 001580\n",
            "Train Loss: 1.195 | Val Loss: 1.492\n",
            "Epoch: 1 | Step: 001585\n",
            "Train Loss: 1.272 | Val Loss: 1.493\n",
            "Epoch: 1 | Step: 001590\n",
            "Train Loss: 1.064 | Val Loss: 1.493\n",
            "Epoch: 1 | Step: 001595\n",
            "Train Loss: 1.500 | Val Loss: 1.494\n",
            "Epoch: 1 | Step: 001600\n",
            "Train Loss: 1.087 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 001605\n",
            "Train Loss: 1.160 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 001610\n",
            "Train Loss: 1.171 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 001615\n",
            "Train Loss: 1.160 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 001620\n",
            "Train Loss: 1.247 | Val Loss: 1.500\n",
            "Epoch: 1 | Step: 001625\n",
            "Train Loss: 1.066 | Val Loss: 1.505\n",
            "Epoch: 1 | Step: 001630\n",
            "Train Loss: 1.371 | Val Loss: 1.509\n",
            "Epoch: 1 | Step: 001635\n",
            "Train Loss: 1.217 | Val Loss: 1.509\n",
            "Epoch: 1 | Step: 001640\n",
            "Train Loss: 1.370 | Val Loss: 1.510\n",
            "Epoch: 1 | Step: 001645\n",
            "Train Loss: 1.123 | Val Loss: 1.508\n",
            "Epoch: 1 | Step: 001650\n",
            "Train Loss: 1.075 | Val Loss: 1.504\n",
            "Epoch: 1 | Step: 001655\n",
            "Train Loss: 1.220 | Val Loss: 1.497\n",
            "Epoch: 1 | Step: 001660\n",
            "Train Loss: 1.186 | Val Loss: 1.490\n",
            "Epoch: 1 | Step: 001665\n",
            "Train Loss: 1.107 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 001670\n",
            "Train Loss: 1.308 | Val Loss: 1.484\n",
            "Epoch: 1 | Step: 001675\n",
            "Train Loss: 1.248 | Val Loss: 1.479\n",
            "Epoch: 1 | Step: 001680\n",
            "Train Loss: 1.414 | Val Loss: 1.480\n",
            "Epoch: 1 | Step: 001685\n",
            "Train Loss: 1.345 | Val Loss: 1.485\n",
            "Epoch: 1 | Step: 001690\n",
            "Train Loss: 1.226 | Val Loss: 1.484\n",
            "Epoch: 1 | Step: 001695\n",
            "Train Loss: 1.144 | Val Loss: 1.481\n",
            "Epoch: 1 | Step: 001700\n",
            "Train Loss: 1.221 | Val Loss: 1.479\n",
            "Epoch: 1 | Step: 001705\n",
            "Train Loss: 1.161 | Val Loss: 1.480\n",
            "Epoch: 1 | Step: 001710\n",
            "Train Loss: 1.268 | Val Loss: 1.484\n",
            "Epoch: 1 | Step: 001715\n",
            "Train Loss: 1.202 | Val Loss: 1.491\n",
            "Epoch: 1 | Step: 001720\n",
            "Train Loss: 1.091 | Val Loss: 1.496\n",
            "Epoch: 1 | Step: 001725\n",
            "Train Loss: 1.292 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 001730\n",
            "Train Loss: 1.504 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 001735\n",
            "Train Loss: 1.249 | Val Loss: 1.497\n",
            "Epoch: 1 | Step: 001740\n",
            "Train Loss: 1.331 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 001745\n",
            "Train Loss: 1.242 | Val Loss: 1.499\n",
            "Epoch: 1 | Step: 001750\n",
            "Train Loss: 1.283 | Val Loss: 1.493\n",
            "Epoch: 1 | Step: 001755\n",
            "Train Loss: 1.228 | Val Loss: 1.490\n",
            "Epoch: 1 | Step: 001760\n",
            "Train Loss: 1.145 | Val Loss: 1.485\n",
            "Epoch: 1 | Step: 001765\n",
            "Train Loss: 1.347 | Val Loss: 1.480\n",
            "Epoch: 1 | Step: 001770\n",
            "Train Loss: 1.310 | Val Loss: 1.481\n",
            "Epoch: 1 | Step: 001775\n",
            "Train Loss: 1.185 | Val Loss: 1.484\n",
            "Epoch: 1 | Step: 001780\n",
            "Train Loss: 1.334 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 001785\n",
            "Train Loss: 1.236 | Val Loss: 1.495\n",
            "Epoch: 1 | Step: 001790\n",
            "Train Loss: 1.037 | Val Loss: 1.492\n",
            "Epoch: 1 | Step: 001795\n",
            "Train Loss: 1.462 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 001800\n",
            "Train Loss: 1.158 | Val Loss: 1.486\n",
            "Epoch: 1 | Step: 001805\n",
            "Train Loss: 1.398 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 001810\n",
            "Train Loss: 1.265 | Val Loss: 1.490\n",
            "Epoch: 1 | Step: 001815\n",
            "Train Loss: 1.310 | Val Loss: 1.492\n",
            "Epoch: 1 | Step: 001820\n",
            "Train Loss: 1.328 | Val Loss: 1.491\n",
            "Epoch: 1 | Step: 001825\n",
            "Train Loss: 1.196 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 001830\n",
            "Train Loss: 1.163 | Val Loss: 1.491\n",
            "Epoch: 1 | Step: 001835\n",
            "Train Loss: 1.264 | Val Loss: 1.492\n",
            "Epoch: 1 | Step: 001840\n",
            "Train Loss: 1.315 | Val Loss: 1.486\n",
            "Epoch: 1 | Step: 001845\n",
            "Train Loss: 1.130 | Val Loss: 1.483\n",
            "Epoch: 1 | Step: 001850\n",
            "Train Loss: 1.236 | Val Loss: 1.482\n",
            "Epoch: 1 | Step: 001855\n",
            "Train Loss: 1.167 | Val Loss: 1.483\n",
            "Epoch: 1 | Step: 001860\n",
            "Train Loss: 1.083 | Val Loss: 1.478\n",
            "Epoch: 1 | Step: 001865\n",
            "Train Loss: 1.302 | Val Loss: 1.474\n",
            "Epoch: 1 | Step: 001870\n",
            "Train Loss: 1.181 | Val Loss: 1.468\n",
            "Epoch: 1 | Step: 001875\n",
            "Train Loss: 1.141 | Val Loss: 1.465\n",
            "Epoch: 1 | Step: 001880\n",
            "Train Loss: 1.139 | Val Loss: 1.465\n",
            "Epoch: 1 | Step: 001885\n",
            "Train Loss: 1.026 | Val Loss: 1.467\n",
            "Epoch: 1 | Step: 001890\n",
            "Train Loss: 1.115 | Val Loss: 1.472\n",
            "Epoch: 1 | Step: 001895\n",
            "Train Loss: 1.086 | Val Loss: 1.476\n",
            "Epoch: 1 | Step: 001900\n",
            "Train Loss: 1.162 | Val Loss: 1.475\n",
            "Epoch: 1 | Step: 001905\n",
            "Train Loss: 1.065 | Val Loss: 1.478\n",
            "Epoch: 1 | Step: 001910\n",
            "Train Loss: 1.351 | Val Loss: 1.477\n",
            "Epoch: 1 | Step: 001915\n",
            "Train Loss: 1.240 | Val Loss: 1.478\n",
            "Epoch: 1 | Step: 001920\n",
            "Train Loss: 1.293 | Val Loss: 1.480\n",
            "Epoch: 1 | Step: 001925\n",
            "Train Loss: 1.031 | Val Loss: 1.485\n",
            "Epoch: 1 | Step: 001930\n",
            "Train Loss: 1.147 | Val Loss: 1.491\n",
            "Epoch: 1 | Step: 001935\n",
            "Train Loss: 1.107 | Val Loss: 1.497\n",
            "Epoch: 1 | Step: 001940\n",
            "Train Loss: 1.407 | Val Loss: 1.498\n",
            "Epoch: 1 | Step: 001945\n",
            "Train Loss: 1.215 | Val Loss: 1.492\n",
            "Epoch: 1 | Step: 001950\n",
            "Train Loss: 1.121 | Val Loss: 1.486\n",
            "Epoch: 1 | Step: 001955\n",
            "Train Loss: 1.262 | Val Loss: 1.481\n",
            "Epoch: 1 | Step: 001960\n",
            "Train Loss: 1.175 | Val Loss: 1.478\n",
            "Epoch: 1 | Step: 001965\n",
            "Train Loss: 1.273 | Val Loss: 1.504\n",
            "Epoch: 1 | Step: 001970\n",
            "Train Loss: 1.399 | Val Loss: 1.530\n",
            "Epoch: 1 | Step: 001975\n",
            "Train Loss: 1.107 | Val Loss: 1.540\n",
            "Epoch: 1 | Step: 001980\n",
            "Train Loss: 1.293 | Val Loss: 1.535\n",
            "Epoch: 1 | Step: 001985\n",
            "Train Loss: 1.169 | Val Loss: 1.530\n",
            "Epoch: 1 | Step: 001990\n",
            "Train Loss: 1.168 | Val Loss: 1.526\n",
            "Epoch: 1 | Step: 001995\n",
            "Train Loss: 1.058 | Val Loss: 1.525\n",
            "Epoch: 1 | Step: 002000\n",
            "Train Loss: 1.087 | Val Loss: 1.525\n",
            "Epoch: 1 | Step: 002005\n",
            "Train Loss: 1.175 | Val Loss: 1.526\n",
            "Epoch: 1 | Step: 002010\n",
            "Train Loss: 0.953 | Val Loss: 1.528\n",
            "Epoch: 1 | Step: 002015\n",
            "Train Loss: 0.976 | Val Loss: 1.530\n",
            "Epoch: 1 | Step: 002020\n",
            "Train Loss: 1.036 | Val Loss: 1.535\n",
            "Epoch: 1 | Step: 002025\n",
            "Train Loss: 1.301 | Val Loss: 1.551\n",
            "Epoch: 1 | Step: 002030\n",
            "Train Loss: 1.252 | Val Loss: 1.566\n",
            "Epoch: 1 | Step: 002035\n",
            "Train Loss: 1.145 | Val Loss: 1.569\n",
            "Epoch: 1 | Step: 002040\n",
            "Train Loss: 1.184 | Val Loss: 1.557\n",
            "Epoch: 1 | Step: 002045\n",
            "Train Loss: 1.175 | Val Loss: 1.546\n",
            "Epoch: 1 | Step: 002050\n",
            "Train Loss: 1.352 | Val Loss: 1.533\n",
            "Epoch: 1 | Step: 002055\n",
            "Train Loss: 1.060 | Val Loss: 1.525\n",
            "Epoch: 1 | Step: 002060\n",
            "Train Loss: 1.071 | Val Loss: 1.519\n",
            "Epoch: 1 | Step: 002065\n",
            "Train Loss: 1.187 | Val Loss: 1.518\n",
            "Epoch: 1 | Step: 002070\n",
            "Train Loss: 1.160 | Val Loss: 1.519\n",
            "Epoch: 1 | Step: 002075\n",
            "Train Loss: 1.284 | Val Loss: 1.519\n",
            "Epoch: 1 | Step: 002080\n",
            "Train Loss: 1.263 | Val Loss: 1.518\n",
            "Epoch: 1 | Step: 002085\n",
            "Train Loss: 1.200 | Val Loss: 1.519\n",
            "Epoch: 1 | Step: 002090\n",
            "Train Loss: 1.227 | Val Loss: 1.520\n",
            "Epoch: 1 | Step: 002095\n",
            "Train Loss: 1.133 | Val Loss: 1.521\n",
            "Epoch: 1 | Step: 002100\n",
            "Train Loss: 1.168 | Val Loss: 1.517\n",
            "Epoch: 1 | Step: 002105\n",
            "Train Loss: 1.100 | Val Loss: 1.511\n",
            "Epoch: 1 | Step: 002110\n",
            "Train Loss: 1.251 | Val Loss: 1.502\n",
            "Epoch: 1 | Step: 002115\n",
            "Train Loss: 1.055 | Val Loss: 1.489\n",
            "Epoch: 1 | Step: 002120\n",
            "Train Loss: 1.083 | Val Loss: 1.490\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Generate a sentence that represents the content in the paragraph.  ### Input: A new law was introduced in 2020 outlining five safety measures all workplaces must follow to prevent the spread of Covid-19. This includes regularly sanitizing the premises, implementing social distancing measures, and introducing a screening and temperature checking procedure.  ### Response: A new law was introduced in 2020 to ensure the safety of all workplaces.<|endoftext|>The following article is an opinion piece written by a professional writer. It is not an opinion piece. It is an opinion piece written by a\n",
            "Epoch: 2 | Step: 002125\n",
            "Train Loss: 1.161 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 002130\n",
            "Train Loss: 1.239 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 002135\n",
            "Train Loss: 1.166 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 002140\n",
            "Train Loss: 1.012 | Val Loss: 1.502\n",
            "Epoch: 2 | Step: 002145\n",
            "Train Loss: 1.275 | Val Loss: 1.513\n",
            "Epoch: 2 | Step: 002150\n",
            "Train Loss: 1.277 | Val Loss: 1.522\n",
            "Epoch: 2 | Step: 002155\n",
            "Train Loss: 1.167 | Val Loss: 1.527\n",
            "Epoch: 2 | Step: 002160\n",
            "Train Loss: 1.260 | Val Loss: 1.530\n",
            "Epoch: 2 | Step: 002165\n",
            "Train Loss: 0.953 | Val Loss: 1.532\n",
            "Epoch: 2 | Step: 002170\n",
            "Train Loss: 1.451 | Val Loss: 1.532\n",
            "Epoch: 2 | Step: 002175\n",
            "Train Loss: 1.069 | Val Loss: 1.528\n",
            "Epoch: 2 | Step: 002180\n",
            "Train Loss: 1.325 | Val Loss: 1.522\n",
            "Epoch: 2 | Step: 002185\n",
            "Train Loss: 1.055 | Val Loss: 1.521\n",
            "Epoch: 2 | Step: 002190\n",
            "Train Loss: 1.234 | Val Loss: 1.522\n",
            "Epoch: 2 | Step: 002195\n",
            "Train Loss: 1.094 | Val Loss: 1.520\n",
            "Epoch: 2 | Step: 002200\n",
            "Train Loss: 1.155 | Val Loss: 1.515\n",
            "Epoch: 2 | Step: 002205\n",
            "Train Loss: 1.097 | Val Loss: 1.515\n",
            "Epoch: 2 | Step: 002210\n",
            "Train Loss: 0.964 | Val Loss: 1.511\n",
            "Epoch: 2 | Step: 002215\n",
            "Train Loss: 1.221 | Val Loss: 1.510\n",
            "Epoch: 2 | Step: 002220\n",
            "Train Loss: 1.091 | Val Loss: 1.514\n",
            "Epoch: 2 | Step: 002225\n",
            "Train Loss: 1.051 | Val Loss: 1.520\n",
            "Epoch: 2 | Step: 002230\n",
            "Train Loss: 1.142 | Val Loss: 1.526\n",
            "Epoch: 2 | Step: 002235\n",
            "Train Loss: 0.836 | Val Loss: 1.528\n",
            "Epoch: 2 | Step: 002240\n",
            "Train Loss: 1.198 | Val Loss: 1.528\n",
            "Epoch: 2 | Step: 002245\n",
            "Train Loss: 1.087 | Val Loss: 1.528\n",
            "Epoch: 2 | Step: 002250\n",
            "Train Loss: 1.238 | Val Loss: 1.522\n",
            "Epoch: 2 | Step: 002255\n",
            "Train Loss: 1.115 | Val Loss: 1.516\n",
            "Epoch: 2 | Step: 002260\n",
            "Train Loss: 1.197 | Val Loss: 1.519\n",
            "Epoch: 2 | Step: 002265\n",
            "Train Loss: 1.083 | Val Loss: 1.520\n",
            "Epoch: 2 | Step: 002270\n",
            "Train Loss: 1.240 | Val Loss: 1.518\n",
            "Epoch: 2 | Step: 002275\n",
            "Train Loss: 1.103 | Val Loss: 1.519\n",
            "Epoch: 2 | Step: 002280\n",
            "Train Loss: 1.210 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 002285\n",
            "Train Loss: 1.045 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 002290\n",
            "Train Loss: 0.977 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 002295\n",
            "Train Loss: 1.218 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 002300\n",
            "Train Loss: 0.959 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002305\n",
            "Train Loss: 1.192 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002310\n",
            "Train Loss: 1.130 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 002315\n",
            "Train Loss: 1.217 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 002320\n",
            "Train Loss: 1.184 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 002325\n",
            "Train Loss: 1.098 | Val Loss: 1.490\n",
            "Epoch: 2 | Step: 002330\n",
            "Train Loss: 1.150 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002335\n",
            "Train Loss: 0.951 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002340\n",
            "Train Loss: 0.981 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 002345\n",
            "Train Loss: 1.090 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 002350\n",
            "Train Loss: 1.146 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 002355\n",
            "Train Loss: 0.988 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002360\n",
            "Train Loss: 1.054 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002365\n",
            "Train Loss: 1.076 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 002370\n",
            "Train Loss: 0.970 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002375\n",
            "Train Loss: 1.274 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 002380\n",
            "Train Loss: 1.040 | Val Loss: 1.509\n",
            "Epoch: 2 | Step: 002385\n",
            "Train Loss: 1.000 | Val Loss: 1.517\n",
            "Epoch: 2 | Step: 002390\n",
            "Train Loss: 0.952 | Val Loss: 1.519\n",
            "Epoch: 2 | Step: 002395\n",
            "Train Loss: 1.142 | Val Loss: 1.521\n",
            "Epoch: 2 | Step: 002400\n",
            "Train Loss: 1.137 | Val Loss: 1.520\n",
            "Epoch: 2 | Step: 002405\n",
            "Train Loss: 1.194 | Val Loss: 1.513\n",
            "Epoch: 2 | Step: 002410\n",
            "Train Loss: 1.021 | Val Loss: 1.504\n",
            "Epoch: 2 | Step: 002415\n",
            "Train Loss: 0.976 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 002420\n",
            "Train Loss: 1.125 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002425\n",
            "Train Loss: 1.038 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002430\n",
            "Train Loss: 1.119 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 002435\n",
            "Train Loss: 1.207 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002440\n",
            "Train Loss: 1.193 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002445\n",
            "Train Loss: 0.943 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002450\n",
            "Train Loss: 1.164 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 002455\n",
            "Train Loss: 1.006 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002460\n",
            "Train Loss: 1.201 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002465\n",
            "Train Loss: 1.222 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 002470\n",
            "Train Loss: 1.160 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002475\n",
            "Train Loss: 1.146 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 002480\n",
            "Train Loss: 1.052 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 002485\n",
            "Train Loss: 1.136 | Val Loss: 1.489\n",
            "Epoch: 2 | Step: 002490\n",
            "Train Loss: 1.019 | Val Loss: 1.490\n",
            "Epoch: 2 | Step: 002495\n",
            "Train Loss: 1.050 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 002500\n",
            "Train Loss: 1.371 | Val Loss: 1.490\n",
            "Epoch: 2 | Step: 002505\n",
            "Train Loss: 1.004 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 002510\n",
            "Train Loss: 0.980 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 002515\n",
            "Train Loss: 1.102 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 002520\n",
            "Train Loss: 1.142 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 002525\n",
            "Train Loss: 1.203 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002530\n",
            "Train Loss: 1.105 | Val Loss: 1.506\n",
            "Epoch: 2 | Step: 002535\n",
            "Train Loss: 0.943 | Val Loss: 1.511\n",
            "Epoch: 2 | Step: 002540\n",
            "Train Loss: 1.075 | Val Loss: 1.514\n",
            "Epoch: 2 | Step: 002545\n",
            "Train Loss: 1.094 | Val Loss: 1.515\n",
            "Epoch: 2 | Step: 002550\n",
            "Train Loss: 0.942 | Val Loss: 1.512\n",
            "Epoch: 2 | Step: 002555\n",
            "Train Loss: 1.133 | Val Loss: 1.506\n",
            "Epoch: 2 | Step: 002560\n",
            "Train Loss: 1.180 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 002565\n",
            "Train Loss: 0.903 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 002570\n",
            "Train Loss: 1.105 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002575\n",
            "Train Loss: 0.938 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 002580\n",
            "Train Loss: 1.078 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002585\n",
            "Train Loss: 1.090 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002590\n",
            "Train Loss: 1.138 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 002595\n",
            "Train Loss: 1.062 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 002600\n",
            "Train Loss: 1.002 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 002605\n",
            "Train Loss: 1.039 | Val Loss: 1.490\n",
            "Epoch: 2 | Step: 002610\n",
            "Train Loss: 1.082 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 002615\n",
            "Train Loss: 1.040 | Val Loss: 1.484\n",
            "Epoch: 2 | Step: 002620\n",
            "Train Loss: 0.860 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 002625\n",
            "Train Loss: 1.035 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 002630\n",
            "Train Loss: 1.083 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 002635\n",
            "Train Loss: 1.015 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 002640\n",
            "Train Loss: 0.921 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 002645\n",
            "Train Loss: 1.081 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002650\n",
            "Train Loss: 1.039 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 002655\n",
            "Train Loss: 1.036 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 002660\n",
            "Train Loss: 1.121 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 002665\n",
            "Train Loss: 1.145 | Val Loss: 1.483\n",
            "Epoch: 2 | Step: 002670\n",
            "Train Loss: 0.852 | Val Loss: 1.479\n",
            "Epoch: 2 | Step: 002675\n",
            "Train Loss: 0.995 | Val Loss: 1.475\n",
            "Epoch: 2 | Step: 002680\n",
            "Train Loss: 1.450 | Val Loss: 1.477\n",
            "Epoch: 2 | Step: 002685\n",
            "Train Loss: 1.054 | Val Loss: 1.480\n",
            "Epoch: 2 | Step: 002690\n",
            "Train Loss: 1.330 | Val Loss: 1.481\n",
            "Epoch: 2 | Step: 002695\n",
            "Train Loss: 0.750 | Val Loss: 1.482\n",
            "Epoch: 2 | Step: 002700\n",
            "Train Loss: 0.933 | Val Loss: 1.484\n",
            "Epoch: 2 | Step: 002705\n",
            "Train Loss: 0.966 | Val Loss: 1.485\n",
            "Epoch: 2 | Step: 002710\n",
            "Train Loss: 1.018 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 002715\n",
            "Train Loss: 1.297 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 002720\n",
            "Train Loss: 0.970 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002725\n",
            "Train Loss: 0.959 | Val Loss: 1.499\n",
            "Epoch: 2 | Step: 002730\n",
            "Train Loss: 1.187 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 002735\n",
            "Train Loss: 1.096 | Val Loss: 1.499\n",
            "Epoch: 2 | Step: 002740\n",
            "Train Loss: 1.041 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 002745\n",
            "Train Loss: 0.954 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 002750\n",
            "Train Loss: 1.274 | Val Loss: 1.483\n",
            "Epoch: 2 | Step: 002755\n",
            "Train Loss: 0.999 | Val Loss: 1.485\n",
            "Epoch: 2 | Step: 002760\n",
            "Train Loss: 1.062 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 002765\n",
            "Train Loss: 1.025 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 002770\n",
            "Train Loss: 0.773 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 002775\n",
            "Train Loss: 0.978 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 002780\n",
            "Train Loss: 1.039 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002785\n",
            "Train Loss: 1.224 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002790\n",
            "Train Loss: 1.018 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 002795\n",
            "Train Loss: 1.099 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 002800\n",
            "Train Loss: 1.204 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 002805\n",
            "Train Loss: 1.012 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 002810\n",
            "Train Loss: 1.296 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002815\n",
            "Train Loss: 0.926 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 002820\n",
            "Train Loss: 1.170 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 002825\n",
            "Train Loss: 1.029 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 002830\n",
            "Train Loss: 0.981 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002835\n",
            "Train Loss: 1.086 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002840\n",
            "Train Loss: 0.942 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 002845\n",
            "Train Loss: 0.972 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002850\n",
            "Train Loss: 1.105 | Val Loss: 1.499\n",
            "Epoch: 2 | Step: 002855\n",
            "Train Loss: 0.950 | Val Loss: 1.503\n",
            "Epoch: 2 | Step: 002860\n",
            "Train Loss: 0.996 | Val Loss: 1.504\n",
            "Epoch: 2 | Step: 002865\n",
            "Train Loss: 0.892 | Val Loss: 1.502\n",
            "Epoch: 2 | Step: 002870\n",
            "Train Loss: 1.229 | Val Loss: 1.499\n",
            "Epoch: 2 | Step: 002875\n",
            "Train Loss: 1.096 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 002880\n",
            "Train Loss: 1.202 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 002885\n",
            "Train Loss: 0.981 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002890\n",
            "Train Loss: 1.133 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 002895\n",
            "Train Loss: 0.959 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 002900\n",
            "Train Loss: 1.066 | Val Loss: 1.490\n",
            "Epoch: 2 | Step: 002905\n",
            "Train Loss: 1.232 | Val Loss: 1.489\n",
            "Epoch: 2 | Step: 002910\n",
            "Train Loss: 1.116 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 002915\n",
            "Train Loss: 1.036 | Val Loss: 1.500\n",
            "Epoch: 2 | Step: 002920\n",
            "Train Loss: 1.054 | Val Loss: 1.505\n",
            "Epoch: 2 | Step: 002925\n",
            "Train Loss: 1.077 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 002930\n",
            "Train Loss: 1.030 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 002935\n",
            "Train Loss: 1.171 | Val Loss: 1.500\n",
            "Epoch: 2 | Step: 002940\n",
            "Train Loss: 0.898 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 002945\n",
            "Train Loss: 0.828 | Val Loss: 1.499\n",
            "Epoch: 2 | Step: 002950\n",
            "Train Loss: 0.981 | Val Loss: 1.500\n",
            "Epoch: 2 | Step: 002955\n",
            "Train Loss: 1.019 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 002960\n",
            "Train Loss: 0.851 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 002965\n",
            "Train Loss: 1.012 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 002970\n",
            "Train Loss: 0.876 | Val Loss: 1.503\n",
            "Epoch: 2 | Step: 002975\n",
            "Train Loss: 0.762 | Val Loss: 1.503\n",
            "Epoch: 2 | Step: 002980\n",
            "Train Loss: 1.191 | Val Loss: 1.499\n",
            "Epoch: 2 | Step: 002985\n",
            "Train Loss: 1.108 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 002990\n",
            "Train Loss: 1.291 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 002995\n",
            "Train Loss: 1.055 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 003000\n",
            "Train Loss: 1.006 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 003005\n",
            "Train Loss: 1.074 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 003010\n",
            "Train Loss: 1.088 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 003015\n",
            "Train Loss: 0.988 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 003020\n",
            "Train Loss: 0.990 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 003025\n",
            "Train Loss: 1.104 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 003030\n",
            "Train Loss: 0.926 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 003035\n",
            "Train Loss: 0.970 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 003040\n",
            "Train Loss: 0.908 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 003045\n",
            "Train Loss: 1.111 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 003050\n",
            "Train Loss: 1.161 | Val Loss: 1.502\n",
            "Epoch: 2 | Step: 003055\n",
            "Train Loss: 1.042 | Val Loss: 1.503\n",
            "Epoch: 2 | Step: 003060\n",
            "Train Loss: 1.027 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 003065\n",
            "Train Loss: 1.253 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 003070\n",
            "Train Loss: 0.873 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 003075\n",
            "Train Loss: 0.899 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 003080\n",
            "Train Loss: 1.056 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 003085\n",
            "Train Loss: 0.852 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 003090\n",
            "Train Loss: 0.984 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 003095\n",
            "Train Loss: 0.853 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 003100\n",
            "Train Loss: 0.976 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 003105\n",
            "Train Loss: 0.902 | Val Loss: 1.485\n",
            "Epoch: 2 | Step: 003110\n",
            "Train Loss: 0.818 | Val Loss: 1.485\n",
            "Epoch: 2 | Step: 003115\n",
            "Train Loss: 0.947 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 003120\n",
            "Train Loss: 0.954 | Val Loss: 1.490\n",
            "Epoch: 2 | Step: 003125\n",
            "Train Loss: 1.152 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 003130\n",
            "Train Loss: 0.911 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 003135\n",
            "Train Loss: 0.994 | Val Loss: 1.489\n",
            "Epoch: 2 | Step: 003140\n",
            "Train Loss: 0.989 | Val Loss: 1.485\n",
            "Epoch: 2 | Step: 003145\n",
            "Train Loss: 1.131 | Val Loss: 1.481\n",
            "Epoch: 2 | Step: 003150\n",
            "Train Loss: 1.087 | Val Loss: 1.480\n",
            "Epoch: 2 | Step: 003155\n",
            "Train Loss: 1.032 | Val Loss: 1.478\n",
            "Epoch: 2 | Step: 003160\n",
            "Train Loss: 0.890 | Val Loss: 1.479\n",
            "Epoch: 2 | Step: 003165\n",
            "Train Loss: 0.930 | Val Loss: 1.480\n",
            "Epoch: 2 | Step: 003170\n",
            "Train Loss: 1.032 | Val Loss: 1.480\n",
            "Epoch: 2 | Step: 003175\n",
            "Train Loss: 0.869 | Val Loss: 1.483\n",
            "Epoch: 2 | Step: 003180\n",
            "Train Loss: 0.986 | Val Loss: 1.489\n",
            "Epoch: 2 | Step: 003185\n",
            "Train Loss: 0.989 | Val Loss: 1.490\n",
            "Epoch: 2 | Step: 003190\n",
            "Train Loss: 1.035 | Val Loss: 1.490\n",
            "Epoch: 2 | Step: 003195\n",
            "Train Loss: 0.828 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 003200\n",
            "Train Loss: 1.028 | Val Loss: 1.485\n",
            "Epoch: 2 | Step: 003205\n",
            "Train Loss: 0.880 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 003210\n",
            "Train Loss: 1.074 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 003215\n",
            "Train Loss: 0.887 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 003220\n",
            "Train Loss: 0.896 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 003225\n",
            "Train Loss: 0.804 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 003230\n",
            "Train Loss: 0.995 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 003235\n",
            "Train Loss: 0.978 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 003240\n",
            "Train Loss: 0.984 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 003245\n",
            "Train Loss: 0.932 | Val Loss: 1.482\n",
            "Epoch: 2 | Step: 003250\n",
            "Train Loss: 1.019 | Val Loss: 1.484\n",
            "Epoch: 2 | Step: 003255\n",
            "Train Loss: 1.101 | Val Loss: 1.485\n",
            "Epoch: 2 | Step: 003260\n",
            "Train Loss: 0.969 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 003265\n",
            "Train Loss: 0.870 | Val Loss: 1.483\n",
            "Epoch: 2 | Step: 003270\n",
            "Train Loss: 1.201 | Val Loss: 1.482\n",
            "Epoch: 2 | Step: 003275\n",
            "Train Loss: 1.146 | Val Loss: 1.479\n",
            "Epoch: 2 | Step: 003280\n",
            "Train Loss: 0.774 | Val Loss: 1.477\n",
            "Epoch: 2 | Step: 003285\n",
            "Train Loss: 1.052 | Val Loss: 1.477\n",
            "Epoch: 2 | Step: 003290\n",
            "Train Loss: 0.944 | Val Loss: 1.478\n",
            "Epoch: 2 | Step: 003295\n",
            "Train Loss: 1.032 | Val Loss: 1.481\n",
            "Epoch: 2 | Step: 003300\n",
            "Train Loss: 1.092 | Val Loss: 1.484\n",
            "Epoch: 2 | Step: 003305\n",
            "Train Loss: 0.812 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 003310\n",
            "Train Loss: 0.888 | Val Loss: 1.485\n",
            "Epoch: 2 | Step: 003315\n",
            "Train Loss: 1.215 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 003320\n",
            "Train Loss: 1.071 | Val Loss: 1.484\n",
            "Epoch: 2 | Step: 003325\n",
            "Train Loss: 1.327 | Val Loss: 1.482\n",
            "Epoch: 2 | Step: 003330\n",
            "Train Loss: 1.029 | Val Loss: 1.476\n",
            "Epoch: 2 | Step: 003335\n",
            "Train Loss: 0.834 | Val Loss: 1.474\n",
            "Epoch: 2 | Step: 003340\n",
            "Train Loss: 1.003 | Val Loss: 1.475\n",
            "Epoch: 2 | Step: 003345\n",
            "Train Loss: 1.022 | Val Loss: 1.479\n",
            "Epoch: 2 | Step: 003350\n",
            "Train Loss: 0.751 | Val Loss: 1.484\n",
            "Epoch: 2 | Step: 003355\n",
            "Train Loss: 0.922 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 003360\n",
            "Train Loss: 0.964 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 003365\n",
            "Train Loss: 1.007 | Val Loss: 1.484\n",
            "Epoch: 2 | Step: 003370\n",
            "Train Loss: 1.091 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 003375\n",
            "Train Loss: 1.026 | Val Loss: 1.489\n",
            "Epoch: 2 | Step: 003380\n",
            "Train Loss: 0.876 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 003385\n",
            "Train Loss: 1.030 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 003390\n",
            "Train Loss: 0.822 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 003395\n",
            "Train Loss: 0.983 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 003400\n",
            "Train Loss: 1.149 | Val Loss: 1.490\n",
            "Epoch: 2 | Step: 003405\n",
            "Train Loss: 1.085 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 003410\n",
            "Train Loss: 1.015 | Val Loss: 1.485\n",
            "Epoch: 2 | Step: 003415\n",
            "Train Loss: 1.187 | Val Loss: 1.483\n",
            "Epoch: 2 | Step: 003420\n",
            "Train Loss: 0.999 | Val Loss: 1.483\n",
            "Epoch: 2 | Step: 003425\n",
            "Train Loss: 0.957 | Val Loss: 1.483\n",
            "Epoch: 2 | Step: 003430\n",
            "Train Loss: 0.958 | Val Loss: 1.481\n",
            "Epoch: 2 | Step: 003435\n",
            "Train Loss: 0.741 | Val Loss: 1.479\n",
            "Epoch: 2 | Step: 003440\n",
            "Train Loss: 0.962 | Val Loss: 1.479\n",
            "Epoch: 2 | Step: 003445\n",
            "Train Loss: 0.971 | Val Loss: 1.479\n",
            "Epoch: 2 | Step: 003450\n",
            "Train Loss: 1.144 | Val Loss: 1.480\n",
            "Epoch: 2 | Step: 003455\n",
            "Train Loss: 0.946 | Val Loss: 1.481\n",
            "Epoch: 2 | Step: 003460\n",
            "Train Loss: 1.109 | Val Loss: 1.481\n",
            "Epoch: 2 | Step: 003465\n",
            "Train Loss: 0.955 | Val Loss: 1.480\n",
            "Epoch: 2 | Step: 003470\n",
            "Train Loss: 0.978 | Val Loss: 1.477\n",
            "Epoch: 2 | Step: 003475\n",
            "Train Loss: 1.007 | Val Loss: 1.473\n",
            "Epoch: 2 | Step: 003480\n",
            "Train Loss: 0.839 | Val Loss: 1.472\n",
            "Epoch: 2 | Step: 003485\n",
            "Train Loss: 0.858 | Val Loss: 1.471\n",
            "Epoch: 2 | Step: 003490\n",
            "Train Loss: 1.037 | Val Loss: 1.472\n",
            "Epoch: 2 | Step: 003495\n",
            "Train Loss: 0.948 | Val Loss: 1.476\n",
            "Epoch: 2 | Step: 003500\n",
            "Train Loss: 0.828 | Val Loss: 1.477\n",
            "Epoch: 2 | Step: 003505\n",
            "Train Loss: 1.051 | Val Loss: 1.477\n",
            "Epoch: 2 | Step: 003510\n",
            "Train Loss: 0.760 | Val Loss: 1.477\n",
            "Epoch: 2 | Step: 003515\n",
            "Train Loss: 0.858 | Val Loss: 1.478\n",
            "Epoch: 2 | Step: 003520\n",
            "Train Loss: 0.896 | Val Loss: 1.481\n",
            "Epoch: 2 | Step: 003525\n",
            "Train Loss: 0.816 | Val Loss: 1.484\n",
            "Epoch: 2 | Step: 003530\n",
            "Train Loss: 0.861 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 003535\n",
            "Train Loss: 0.869 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 003540\n",
            "Train Loss: 0.953 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 003545\n",
            "Train Loss: 0.900 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 003550\n",
            "Train Loss: 1.178 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 003555\n",
            "Train Loss: 1.053 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 003560\n",
            "Train Loss: 0.885 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 003565\n",
            "Train Loss: 1.007 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 003570\n",
            "Train Loss: 0.955 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 003575\n",
            "Train Loss: 0.954 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 003580\n",
            "Train Loss: 1.095 | Val Loss: 1.482\n",
            "Epoch: 2 | Step: 003585\n",
            "Train Loss: 0.856 | Val Loss: 1.480\n",
            "Epoch: 2 | Step: 003590\n",
            "Train Loss: 1.028 | Val Loss: 1.481\n",
            "Epoch: 2 | Step: 003595\n",
            "Train Loss: 0.912 | Val Loss: 1.482\n",
            "Epoch: 2 | Step: 003600\n",
            "Train Loss: 0.763 | Val Loss: 1.483\n",
            "Epoch: 2 | Step: 003605\n",
            "Train Loss: 0.841 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 003610\n",
            "Train Loss: 0.950 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 003615\n",
            "Train Loss: 0.916 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 003620\n",
            "Train Loss: 0.801 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 003625\n",
            "Train Loss: 1.116 | Val Loss: 1.482\n",
            "Epoch: 2 | Step: 003630\n",
            "Train Loss: 0.822 | Val Loss: 1.478\n",
            "Epoch: 2 | Step: 003635\n",
            "Train Loss: 0.907 | Val Loss: 1.479\n",
            "Epoch: 2 | Step: 003640\n",
            "Train Loss: 0.771 | Val Loss: 1.479\n",
            "Epoch: 2 | Step: 003645\n",
            "Train Loss: 1.157 | Val Loss: 1.479\n",
            "Epoch: 2 | Step: 003650\n",
            "Train Loss: 1.072 | Val Loss: 1.474\n",
            "Epoch: 2 | Step: 003655\n",
            "Train Loss: 0.840 | Val Loss: 1.468\n",
            "Epoch: 2 | Step: 003660\n",
            "Train Loss: 0.835 | Val Loss: 1.467\n",
            "Epoch: 2 | Step: 003665\n",
            "Train Loss: 0.954 | Val Loss: 1.472\n",
            "Epoch: 2 | Step: 003670\n",
            "Train Loss: 0.767 | Val Loss: 1.476\n",
            "Epoch: 2 | Step: 003675\n",
            "Train Loss: 1.092 | Val Loss: 1.478\n",
            "Epoch: 2 | Step: 003680\n",
            "Train Loss: 1.279 | Val Loss: 1.480\n",
            "Epoch: 2 | Step: 003685\n",
            "Train Loss: 1.066 | Val Loss: 1.484\n",
            "Epoch: 2 | Step: 003690\n",
            "Train Loss: 1.068 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 003695\n",
            "Train Loss: 1.125 | Val Loss: 1.483\n",
            "Epoch: 2 | Step: 003700\n",
            "Train Loss: 0.953 | Val Loss: 1.478\n",
            "Epoch: 2 | Step: 003705\n",
            "Train Loss: 0.916 | Val Loss: 1.479\n",
            "Epoch: 2 | Step: 003710\n",
            "Train Loss: 0.898 | Val Loss: 1.482\n",
            "Epoch: 2 | Step: 003715\n",
            "Train Loss: 1.180 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 003720\n",
            "Train Loss: 0.766 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 003725\n",
            "Train Loss: 1.143 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 003730\n",
            "Train Loss: 0.798 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 003735\n",
            "Train Loss: 1.044 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 003740\n",
            "Train Loss: 0.913 | Val Loss: 1.499\n",
            "Epoch: 2 | Step: 003745\n",
            "Train Loss: 1.071 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 003750\n",
            "Train Loss: 1.270 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 003755\n",
            "Train Loss: 0.957 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 003760\n",
            "Train Loss: 0.734 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 003765\n",
            "Train Loss: 0.801 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 003770\n",
            "Train Loss: 0.987 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 003775\n",
            "Train Loss: 0.874 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 003780\n",
            "Train Loss: 0.974 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 003785\n",
            "Train Loss: 1.000 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 003790\n",
            "Train Loss: 0.957 | Val Loss: 1.500\n",
            "Epoch: 2 | Step: 003795\n",
            "Train Loss: 0.928 | Val Loss: 1.500\n",
            "Epoch: 2 | Step: 003800\n",
            "Train Loss: 1.010 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 003805\n",
            "Train Loss: 1.014 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 003810\n",
            "Train Loss: 0.832 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 003815\n",
            "Train Loss: 0.741 | Val Loss: 1.490\n",
            "Epoch: 2 | Step: 003820\n",
            "Train Loss: 0.956 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 003825\n",
            "Train Loss: 0.808 | Val Loss: 1.486\n",
            "Epoch: 2 | Step: 003830\n",
            "Train Loss: 0.841 | Val Loss: 1.483\n",
            "Epoch: 2 | Step: 003835\n",
            "Train Loss: 0.980 | Val Loss: 1.479\n",
            "Epoch: 2 | Step: 003840\n",
            "Train Loss: 0.929 | Val Loss: 1.479\n",
            "Epoch: 2 | Step: 003845\n",
            "Train Loss: 0.808 | Val Loss: 1.484\n",
            "Epoch: 2 | Step: 003850\n",
            "Train Loss: 0.885 | Val Loss: 1.490\n",
            "Epoch: 2 | Step: 003855\n",
            "Train Loss: 0.801 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 003860\n",
            "Train Loss: 0.874 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 003865\n",
            "Train Loss: 1.222 | Val Loss: 1.499\n",
            "Epoch: 2 | Step: 003870\n",
            "Train Loss: 1.111 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 003875\n",
            "Train Loss: 0.861 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 003880\n",
            "Train Loss: 0.829 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 003885\n",
            "Train Loss: 1.017 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 003890\n",
            "Train Loss: 1.022 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 003895\n",
            "Train Loss: 1.090 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 003900\n",
            "Train Loss: 1.091 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 003905\n",
            "Train Loss: 0.904 | Val Loss: 1.489\n",
            "Epoch: 2 | Step: 003910\n",
            "Train Loss: 0.919 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 003915\n",
            "Train Loss: 0.864 | Val Loss: 1.489\n",
            "Epoch: 2 | Step: 003920\n",
            "Train Loss: 0.885 | Val Loss: 1.490\n",
            "Epoch: 2 | Step: 003925\n",
            "Train Loss: 0.906 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 003930\n",
            "Train Loss: 0.828 | Val Loss: 1.496\n",
            "Epoch: 2 | Step: 003935\n",
            "Train Loss: 0.963 | Val Loss: 1.502\n",
            "Epoch: 2 | Step: 003940\n",
            "Train Loss: 1.050 | Val Loss: 1.508\n",
            "Epoch: 2 | Step: 003945\n",
            "Train Loss: 0.882 | Val Loss: 1.515\n",
            "Epoch: 2 | Step: 003950\n",
            "Train Loss: 1.014 | Val Loss: 1.519\n",
            "Epoch: 2 | Step: 003955\n",
            "Train Loss: 0.790 | Val Loss: 1.519\n",
            "Epoch: 2 | Step: 003960\n",
            "Train Loss: 0.939 | Val Loss: 1.521\n",
            "Epoch: 2 | Step: 003965\n",
            "Train Loss: 0.990 | Val Loss: 1.521\n",
            "Epoch: 2 | Step: 003970\n",
            "Train Loss: 0.881 | Val Loss: 1.518\n",
            "Epoch: 2 | Step: 003975\n",
            "Train Loss: 0.833 | Val Loss: 1.515\n",
            "Epoch: 2 | Step: 003980\n",
            "Train Loss: 0.929 | Val Loss: 1.512\n",
            "Epoch: 2 | Step: 003985\n",
            "Train Loss: 0.852 | Val Loss: 1.510\n",
            "Epoch: 2 | Step: 003990\n",
            "Train Loss: 0.988 | Val Loss: 1.512\n",
            "Epoch: 2 | Step: 003995\n",
            "Train Loss: 0.980 | Val Loss: 1.514\n",
            "Epoch: 2 | Step: 004000\n",
            "Train Loss: 0.910 | Val Loss: 1.513\n",
            "Epoch: 2 | Step: 004005\n",
            "Train Loss: 0.866 | Val Loss: 1.511\n",
            "Epoch: 2 | Step: 004010\n",
            "Train Loss: 0.962 | Val Loss: 1.511\n",
            "Epoch: 2 | Step: 004015\n",
            "Train Loss: 0.829 | Val Loss: 1.510\n",
            "Epoch: 2 | Step: 004020\n",
            "Train Loss: 0.992 | Val Loss: 1.506\n",
            "Epoch: 2 | Step: 004025\n",
            "Train Loss: 0.953 | Val Loss: 1.505\n",
            "Epoch: 2 | Step: 004030\n",
            "Train Loss: 0.812 | Val Loss: 1.507\n",
            "Epoch: 2 | Step: 004035\n",
            "Train Loss: 1.045 | Val Loss: 1.507\n",
            "Epoch: 2 | Step: 004040\n",
            "Train Loss: 0.841 | Val Loss: 1.505\n",
            "Epoch: 2 | Step: 004045\n",
            "Train Loss: 1.016 | Val Loss: 1.507\n",
            "Epoch: 2 | Step: 004050\n",
            "Train Loss: 0.973 | Val Loss: 1.507\n",
            "Epoch: 2 | Step: 004055\n",
            "Train Loss: 0.924 | Val Loss: 1.508\n",
            "Epoch: 2 | Step: 004060\n",
            "Train Loss: 0.918 | Val Loss: 1.506\n",
            "Epoch: 2 | Step: 004065\n",
            "Train Loss: 0.962 | Val Loss: 1.503\n",
            "Epoch: 2 | Step: 004070\n",
            "Train Loss: 0.963 | Val Loss: 1.504\n",
            "Epoch: 2 | Step: 004075\n",
            "Train Loss: 0.977 | Val Loss: 1.502\n",
            "Epoch: 2 | Step: 004080\n",
            "Train Loss: 0.726 | Val Loss: 1.498\n",
            "Epoch: 2 | Step: 004085\n",
            "Train Loss: 0.817 | Val Loss: 1.494\n",
            "Epoch: 2 | Step: 004090\n",
            "Train Loss: 0.941 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 004095\n",
            "Train Loss: 0.957 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 004100\n",
            "Train Loss: 0.832 | Val Loss: 1.489\n",
            "Epoch: 2 | Step: 004105\n",
            "Train Loss: 0.937 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 004110\n",
            "Train Loss: 0.746 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 004115\n",
            "Train Loss: 1.090 | Val Loss: 1.491\n",
            "Epoch: 2 | Step: 004120\n",
            "Train Loss: 0.928 | Val Loss: 1.490\n",
            "Epoch: 2 | Step: 004125\n",
            "Train Loss: 0.844 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 004130\n",
            "Train Loss: 0.782 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 004135\n",
            "Train Loss: 0.832 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 004140\n",
            "Train Loss: 0.918 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 004145\n",
            "Train Loss: 0.938 | Val Loss: 1.487\n",
            "Epoch: 2 | Step: 004150\n",
            "Train Loss: 0.819 | Val Loss: 1.485\n",
            "Epoch: 2 | Step: 004155\n",
            "Train Loss: 0.920 | Val Loss: 1.483\n",
            "Epoch: 2 | Step: 004160\n",
            "Train Loss: 0.701 | Val Loss: 1.484\n",
            "Epoch: 2 | Step: 004165\n",
            "Train Loss: 0.874 | Val Loss: 1.488\n",
            "Epoch: 2 | Step: 004170\n",
            "Train Loss: 0.849 | Val Loss: 1.492\n",
            "Epoch: 2 | Step: 004175\n",
            "Train Loss: 0.807 | Val Loss: 1.495\n",
            "Epoch: 2 | Step: 004180\n",
            "Train Loss: 0.964 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 004185\n",
            "Train Loss: 0.929 | Val Loss: 1.499\n",
            "Epoch: 2 | Step: 004190\n",
            "Train Loss: 0.762 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 004195\n",
            "Train Loss: 0.976 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 004200\n",
            "Train Loss: 0.995 | Val Loss: 1.497\n",
            "Epoch: 2 | Step: 004205\n",
            "Train Loss: 0.806 | Val Loss: 1.501\n",
            "Epoch: 2 | Step: 004210\n",
            "Train Loss: 0.933 | Val Loss: 1.508\n",
            "Epoch: 2 | Step: 004215\n",
            "Train Loss: 0.911 | Val Loss: 1.514\n",
            "Epoch: 2 | Step: 004220\n",
            "Train Loss: 0.854 | Val Loss: 1.514\n",
            "Epoch: 2 | Step: 004225\n",
            "Train Loss: 0.888 | Val Loss: 1.506\n",
            "Epoch: 2 | Step: 004230\n",
            "Train Loss: 0.748 | Val Loss: 1.499\n",
            "Epoch: 2 | Step: 004235\n",
            "Train Loss: 1.040 | Val Loss: 1.493\n",
            "Epoch: 2 | Step: 004240\n",
            "Train Loss: 0.707 | Val Loss: 1.489\n",
            "Epoch: 2 | Step: 004245\n",
            "Train Loss: 0.784 | Val Loss: 1.489\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Generate a sentence that represents the content in the paragraph.  ### Input: A new law was introduced in 2020 outlining five safety measures all workplaces must follow to prevent the spread of Covid-19. This includes regularly sanitizing the premises, implementing social distancing measures, and introducing a screening and temperature checking procedure.  ### Response: A new law was introduced in 2020 to ensure the safety of all employees.<|endoftext|>The following article is about the importance of education in the workplace.  Education is a key factor in the success of any business. It\n",
            "Time taken to train: 36.55 minutes\n"
          ]
        }
      ],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": [
        "from utilities import plot_losses\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "pHpqGDXq3cN8",
        "outputId": "8ddef072-5731-45a8-e90b-acb446ef3310"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation"
      ],
      "metadata": {
        "id": "6r92q5Fm36Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1234)\n",
        "\n",
        "for entry in test_data[-5:-1]:\n",
        "  input_text = format_input(entry)\n",
        "  token_ids= generate(\n",
        "      model= model,\n",
        "      idx= text_to_token_ids(input_text, tokenizer).to(device),\n",
        "      max_new_tokens= 256,\n",
        "      context_size= BASE_CONFIG['context_length'],\n",
        "      eos_id= 50256\n",
        "  )\n",
        "\n",
        "  generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "  response_text = (generated_text[len(input_text):]\n",
        "                   .replace(\"### Response:\", \"\")\n",
        "                   .strip()\n",
        "  )\n",
        "\n",
        "  print(input_text)\n",
        "  print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "  print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "  print(\"-------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUyAGa1E3l3V",
        "outputId": "ac16421d-f9de-455f-8753-826f914c7158"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Summarize the following article about climate change: \n",
            "\n",
            "\"Climate change is one of the biggest challenges facing the world today. Recent studies suggest that the global temperature is rising faster than before, with an increase of around one degree Celsius within the last century. Climate change is causing floods, droughts, extreme weather conditions, and other natural disasters. This is putting our environment, ecosystems, and human health and safety at risk.\n",
            "\n",
            "Correct response:\n",
            ">> Climate change is causing global temperatures to rise faster than ever and has led to an increase of around 1 degree Celsius in the past century. This is resulting in floods, droughts, and extreme weather events, causing a risk to our environment, ecosystems, and human health and safety.\n",
            "\n",
            "Model response:\n",
            ">> Climate change is one of the biggest challenges facing the world today. Recent studies suggest that the global temperature is rising faster than before, with an increase of around one degree Celsius within the last century. This is putting our environment, ecosystems, and human health and safety at risk.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Analyze the given text and determine the primary argument being made.\n",
            "\n",
            "### Input:\n",
            "Water is vital to human health and access to it is a human right.\n",
            "\n",
            "Correct response:\n",
            ">> The primary argument being made is that water is essential to human health and everyone should have access to it.\n",
            "\n",
            "Model response:\n",
            ">> The argument is that water is essential for human health and access to it is a human right.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Create a document of 10 lines to introduce yourself.\n",
            "\n",
            "Correct response:\n",
            ">> My name is [name]. I am a [position] with [company] and have a passion for [industry]. I have been in the industry for [number] years, and have a deep understanding of [topics] and a strong knowledge of [subjects]. I am creative and think outside of the box, helping me to come up with new solutions to existing problems. I believe in working hard and can always be relied upon to deliver great results. I am a team player and enjoy collaborating with my colleagues to find solutions to difficult problems. In my free time, I enjoy [hobbies] and [activities], which help me to stay motivated and energised.\n",
            "\n",
            "Model response:\n",
            ">> 1. Introduction \n",
            "2. What are your goals? \n",
            "3. What are your goals in the future? \n",
            "4. What are your goals in the present? \n",
            "5. What are your goals in the future? \n",
            "6. What are your goals in the present? \n",
            "7. What are your goals in the future? \n",
            "8. What are your goals in the future? \n",
            "9. What are your goals in the present? \n",
            "10. What are your goals in the future?\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Describe three advantages and three disadvantages of living in an urban city.\n",
            "\n",
            "Correct response:\n",
            ">> Advantages of living in an urban city include access to a variety of job opportunities, an abundance of cultural activities and experiences, and a variety of transportation options. Disadvantages of living in an urban city include higher costs of living, increased pollution and noise levels, and a greater likelihood of crime.\n",
            "\n",
            "Model response:\n",
            ">> Pros:\n",
            "- Access to amenities such as public transportation, public parks, and public libraries.\n",
            "- Access to public parks and public transportation is often free and convenient.\n",
            "- Access to public parks and public parks is often free and convenient.\n",
            "- Access to public transportation is often free and convenient.\n",
            "- Access to public transportation is often reliable and reliable.\n",
            "- Access to public transportation is often accessible to those who live in urban cities.\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Generating test set responses and adding them to a dictionary"
      ],
      "metadata": {
        "id": "5vcSf5aK8uHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generating test set responses\n",
        "\n",
        "from tqdm import tqdm\n",
        "for i, entry in tqdm(enumerate(test_data), total= len(test_data)):\n",
        "  input_text = format_input(entry)\n",
        "\n",
        "  token_ids = generate(\n",
        "      model= model,\n",
        "      idx= text_to_token_ids(input_text, tokenizer).to(device),\n",
        "      max_new_tokens= 256,\n",
        "      context_size= BASE_CONFIG['context_length'],\n",
        "      eos_id = 50256\n",
        "  )\n",
        "\n",
        "  generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "  response_text = (generated_text[len(input_text):]\n",
        "                   .replace(\"### Response:\", \"\")\n",
        "                   .strip()\n",
        "  )\n",
        "  test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "with open('instruction-data-with-response.json', 'w') as file:\n",
        "  json.dump(test_data, file, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzvQr_9w4_wT",
        "outputId": "a71fc5e6-afc1-44d1-e277-fc7dcb4666d0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [22:56<00:00,  2.75s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check for a response\n",
        "print(test_data[6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm_yqek19oQm",
        "outputId": "9d3f372b-16dc-4498-f9c4-4a4885a15dec"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'Generate an example of a job offer letter.', 'input': '', 'output': 'Dear [Candidate Name], \\n\\nWe are delighted to offer you the position of [Position Name] with [Company Name]. You will be joining our team of dedicated and hardworking individuals who are committed to providing our customers with the highest quality of service. \\n\\nThis position offers competitive pay of [Salary], and you will receive benefits according to our company policy. We believe you will be an excellent addition to our team and we look forward to you joining us. \\n\\nPlease let us know if you accept the offer by [date]. If you have any questions, please do not hesitate to contact us. \\n\\nWe hope to hear from you soon! \\n\\nSincerely,\\n[Company Name]', 'model_response': 'Dear [Employer Name],\\n\\n\\nI am writing to apply for a position at [Company Name] and I am looking for a role that is flexible and rewarding.\\n\\nI am very motivated and I am ready to learn as much as possible about the position and apply my skills to make it a success.\\n\\nI am also motivated by the opportunity to learn more about the company and its culture.\\n\\nI am confident that I can make a successful contribution to the team’s success.\\n\\nThank you for your time and consideration.\\n\\nSincerely,\\n[Your Name]'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
        "torch.save(model.state_dict(), file_name)\n",
        "print(f\"Model saved as {file_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS02CXJ2BCN4",
        "outputId": "b0f84519-d25d-4c2c-84cf-b808e71479c9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as gpt2-medium355M-sft.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o61D5cTtI5IV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}